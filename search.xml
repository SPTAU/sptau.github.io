<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CNN 参数更新</title>
    <url>/posts/98f1e749.html</url>
    <content><![CDATA[<h2 id="lenet-51">LeNet-5<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2>
<h3 id="结构">结构</h3>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">Input Size</th>
<th style="text-align: center;">Kernel Size</th>
<th style="text-align: center;">padding</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">Kernel Numbers</th>
<th style="text-align: center;">Output Size</th>
<th style="text-align: center;">Activation function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">32*32*1</td>
<td style="text-align: center;">5*5*1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">28*28*6</td>
<td style="text-align: center;">Sigmoid</td>
</tr>
<tr class="even">
<td style="text-align: center;">Pool1</td>
<td style="text-align: center;">28*28*6</td>
<td style="text-align: center;">2*2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">14*14*6</td>
<td style="text-align: center;">Sigmoid</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv12</td>
<td style="text-align: center;">14*14*6</td>
<td style="text-align: center;">5*5*？</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">10*10*16</td>
<td style="text-align: center;">Sigmoid</td>
</tr>
<tr class="even">
<td style="text-align: center;">Pool2</td>
<td style="text-align: center;">10*10*16</td>
<td style="text-align: center;">2*2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">Sigmoid</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv13</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">1*1*120</td>
<td style="text-align: center;">Sigmoid</td>
</tr>
<tr class="even">
<td style="text-align: center;">FC1</td>
<td style="text-align: center;">1*1*120</td>
<td style="text-align: center;">1*1*120</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">1*1*84</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">FC2</td>
<td style="text-align: center;">1*1*84</td>
<td style="text-align: center;">1*1*84</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1*1*10</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Pool 层采用的是求和后乘以权值再加上偏置的方法，并不是常见的最大值池化</p>
<figure>
<img data-src="https://s2.loli.net/2023/03/27/pBOy4FCrKn1EqQU.png" alt=""><figcaption>Conv2 层卷积核与 Pool1 层特征图通道对应关系</figcaption>
</figure>
<p>由于 Conv12 层的不同的卷积核所需要对应的通道数量不同，因此卷积核的尺寸并不固定</p>
<figure>
<img data-src="https://s2.loli.net/2023/03/27/OmltDcWLSFayk8q.png" alt=""><figcaption>FC2 层计算公式</figcaption>
</figure>
<p>FC2 层是计算 FC2 层所有输出与特定权值之差的累计</p>
<h3 id="可训练参数">可训练参数</h3>
<p>Conv1 层的可训练参数为 <span class="math inline">\((5*5*1+1)*6 = 156\)</span>，其中 <span class="math inline">\(5*5\)</span> 是卷积核内的参数，<span class="math inline">\(1\)</span> 是偏置系数</p>
<p>Pool1 层的可训练参数为 <span class="math inline">\((1+1)*6 = 12\)</span>，其中的 <span class="math inline">\(1+1\)</span> 分别是权值和系数</p>
<p>Conv2 层的可训练参数为 <span class="math inline">\((5*5*3+1)*6 + (5*5*4+1)*6 + (5*5*4+1)*3 + (5*5*6+1)*1 = 1516\)</span></p>
<p>Pool2 层的可训练参数为 <span class="math inline">\((1+1)*16 = 32\)</span></p>
<p>Conv3 层的可训练参数为 <span class="math inline">\((5*5*16+1)*120 = 48120\)</span></p>
<p>FC1 层的可训练参数为 <span class="math inline">\((120+1)*84 = 10164\)</span></p>
<p>FC1 层的可训练参数为 <span class="math inline">\(84*10 = 840\)</span></p>
<p>总计需要训练 <span class="math inline">\(156+12+1516+32+48120+10164+840 = 60840\)</span> 个参数</p>
<h3 id="简化">简化</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">Input Size</th>
<th style="text-align: center;">Kernel Size</th>
<th style="text-align: center;">padding</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">Output Size</th>
<th style="text-align: center;">Activation function</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">32*32*1</td>
<td style="text-align: center;">5*5*1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">28*28*6</td>
<td style="text-align: center;">ReLu</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Pool1</td>
<td style="text-align: center;">28*28*6</td>
<td style="text-align: center;">2*2*1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">14*14*6</td>
<td style="text-align: center;">ReLu</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv12</td>
<td style="text-align: center;">14*14*6</td>
<td style="text-align: center;">5*5*6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10*10*16</td>
<td style="text-align: center;">ReLu</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Pool2</td>
<td style="text-align: center;">10*10*16</td>
<td style="text-align: center;">2*2*1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">ReLu</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv13</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">5*5*16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1*1*120</td>
<td style="text-align: center;">ReLu</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Flatten</td>
<td style="text-align: center;">1*1*120</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">FC1</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">FC2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<ul>
<li>卷积核其实是有深度的</li>
<li>输出的通道数就是卷积核的数量</li>
<li>同一个卷积核在不同深度的参数在很大程度上是不同的</li>
<li>同一个卷积核进行卷积时，卷积核的参数保持不变，这样每一处卷积的权重都是一致的，这就是 CNN 的权值共享，可以有效地减少参数量</li>
</ul>
<h3 id="参数更新与反向传播">参数更新与反向传播</h3>
<p>CNN 的参数更新依赖梯度下降算法和反向传播算法</p>
<p>反向传播在这里主要是根据 Loss 反求偏导</p>
<p><span class="math display">\[\frac{\partial L}{\partial w} = \frac{\partial y}{\partial w} * \frac{\partial L}{\partial y}\]</span></p>
<p>梯度下降则是将参数减去学习率乘偏导</p>
<p><span class="math display">\[w' = w - rate * \frac{\partial L}{\partial w}\]</span></p>
<p>在最大池化层中，未被保留的部分偏导为 0</p>
<h3 id="梯度消失">梯度消失</h3>
<p>当卷积神经网络的层数很多时，反传上来的梯度/偏导的的值会变得很小，趋近于0</p>
<p>这将会严重影响 CNN 参数的更新，最终影响到网络的性能</p>
<p>为避免这种情况发生，何凯明提出了 ResNet<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>，在 CNN 中进行 shortcut ，提高偏导的绝对值，很好地解决了梯度消失的问题</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 11 (November 1998), 2278–2324. DOI:<a href="https://doi.org/10.1109/5.726791" class="uri">https://doi.org/10.1109/5.726791</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. 770–778. Retrieved March 27, 2023 from <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" class="uri">https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>ICCV 2017 扫读总结</title>
    <url>/posts/c254b0.html</url>
    <content><![CDATA[<p>本次扫读从 ICCV 2017 论文集 600 篇中根据论文名称寻找了感兴趣的 50 篇，将根据 略读 - 细读 - 精读 的方式将这 50 篇论文快速过一遍</p>
<blockquote>
<p>因时间关系最后只读了其中的 18 篇</p>
</blockquote>
<h2 id="阅读方式">阅读方式</h2>
<h3 id="略读">略读</h3>
<p>所有论文都先过一遍 Abstract 、Introduction 、Conclusion ，简要了解该论文的主要贡献</p>
<p>若该论文的应用场景、研究方向不合预期，则立刻略过</p>
<h3 id="细读">细读</h3>
<p>将全文过一遍，但不细究其中的技术，主要把流程捋顺</p>
<h3 id="精读">精读</h3>
<p>对于十分契合研究方向论文，将会仔细学习其中的算法流程和思路</p>
<h2 id="篇论文">18 篇论文</h2>
<h3 id="d-graph-neural-networks-for-rgbd-semantic-segmentation1">3D Graph Neural Networks for RGBD Semantic Segmentation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h3>
<p>此前利用 RGB 图像和深度图像进行语义分割的方法是分别对这两个图像输入进 CNN 神经网络，这样做的计算成本和内存成本很大</p>
<p>该论文提出了一种 3D 图神经网络（ 3DGNN ），在点云上构建 k 最临近图，分别输入 RGB 图像、深度图像和点云，输出 RGB 图像的语义分类结果</p>
<p>考虑到适用场景不同以及 GNN 的复杂性，略过该论文</p>
<h3 id="d-driven-3d-object-detection-in-rgb-d-images2">2D-Driven 3D Object Detection in RGB-D Images<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h3>
<p>在 3D 目标检测中，一般使用 3D 边界框标注目标，但是 3D 标注框的计算成本较大且难以利用到点云中的局部特性</p>
<p>该论文提出了利用 RGB 图像和深度图像的 2D 目标检测驱动 3D 目标检测，先进行二维图像的目标检测，然后利用得到的 2D 边界框在 3D 点云上重新配准的到 3D 边界框</p>
<p>考虑到适用场景不同，略过该论文</p>
<h3 id="d-surface-detail-enhancement-from-a-single-normal-map3">3D Surface Detail Enhancement from A Single Normal Map<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h3>
<p>现有的三维重建算法重建的表面质量受到输入图像质量的影响，如何利用现有质量的图像重建更优质表面是需要解决的问题</p>
<p>该论文提出了一种不需要训练数据集、不依赖硬件的三维重建方式，提高了表面的细腻度</p>
<p>该论文不是基于深度学习，略过</p>
<h3 id="d-prnn-generating-shape-primitives-with-recurrent-neural-networks4">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h3>
<p>该论文提出了一种基于 RNN 输入深度图像、能够生成三维模型的 3D-PRNN 模型和一种基于高斯场和能量最小化的点云拟合基元的有效方法，能够有效应用于三维重建</p>
<p>考虑到应该不会使用深度图像，略过</p>
<h3 id="dcnn-dqn-rnn-a-deep-reinforcement-learning-framework-for-semantic-parsing-of-large-scale-3d-point-clouds5">3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></h3>
<p>该论文提出了一种自动解析大规模三维点云的 3DCNN-DQN-RNN 模型，利用 3DCNN 提取点云的特征，利用 RNN 融合 3DCNN 得到的各种特征，利用 DQN 进行类滑动窗口的操作，实现定位、检测、分类的集成</p>
<p>略感兴趣，可以后期再看看</p>
<p>DQN 是未见过的一类神经网络架构，后面也可以再留意</p>
<h3 id="a-read-write-memory-network-for-movie-story-understanding6">A Read-Write Memory Network for Movie Story Understanding*<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></h3>
<p>论文提出了一种基于 ResNet 和 Word2Vec 网络和 CNN 结构的电影故事理解模型，输入画面和台词，可以很好地回答针对故事的提问</p>
<p>只是对这方面略感兴趣，快速略过</p>
<h3 id="adversarial-examples-for-semantic-segmentation-and-object-detection7">Adversarial Examples for Semantic Segmentation and Object Detection*<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></h3>
<p>对对抗样本感兴趣，于是看到这篇论文的时候挑出来了</p>
<p>为了应对输入中的微小扰动带来的论文提出了一种密集对抗生成的新算法</p>
<p>只是看摘要、总结和引言看不出什么，快速略过</p>
<h3 id="an-empirical-study-of-language-cnn-for-image-captioning8">An Empirical Study of Language CNN for Image Captioning*<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></h3>
<p>论文提出的网络是非典型的神经网络，使用了图像和文字作为输入</p>
<p>论文提出的模型首先将图片输入进 CNN 里面得到特征，然后再将特征作为 RNN 的基础状态，和文字一起进行 RNN ，最终实现 Image To Text</p>
<p>快速略过</p>
<h3 id="bb8-a-scalable-accurate-robust-to-partial-occlusion-method-for-predicting-the-3d-poses-of-challenging-objects-without-using-depth9">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></h3>
<p>论文提出了一种基于二维图像进行物体的三维姿势检测的方法，实现了在图像上预测物体的三维边界框</p>
<p>但是论文中没有提出具体的模型，感觉不是很可靠，或者说因此觉得他不希望被复现</p>
<p>可以暂存</p>
<h3 id="channel-pruning-for-accelerating-very-deep-neural-networks10">Channel Pruning for Accelerating Very Deep Neural Networks<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></h3>
<p>论文提出了一种修剪卷积神经网络通道的方法，通过减少卷积神经网络中的通道数加速网络计算并将因此带来的误差减少到一定程度，且该方法具有很强的拓展性，可以在 VGG 、 ResNet 上使用该方法</p>
<p>可以暂存</p>
<h3 id="colored-point-cloud-registration-revisited11">Colored Point Cloud Registration Revisited<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></h3>
<p>论文提出了一种针对 RGB-D 图像和 XYZRGB 点云结合的点云配准方法，效果优于基于机器学习的 ICP 配准和 FGR 配准方法</p>
<p>但是看起来好像不是基于深度学习，且需要用到深度图像</p>
<p>可以留意有没有作者公布的代码实现</p>
<h3 id="deepcontext-context-encoding-neural-pathways-for-3d-holistic-scene-understanding12">DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></h3>
<p>论文提出了一种能够学习场景信息的三维卷积神经网络，通过对齐数据集学习到的基本场景模板和卷积神经网络，实现场景中物体的实例分割</p>
<p>可以暂存，但是如果应用在植物上的话，可能很难总结出不同的模板</p>
<h3 id="directionally-convolutional-networks-for-3d-shape-segmentation13">Directionally Convolutional Networks for 3D Shape Segmentation<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></h3>
<p>论文提出了一种基于 Mesh 网格数据集的定向卷积神经网络 DCN 和普通神经网络结合的双流分割网络</p>
<p>双流指的是，同一输入分别送入不同网络中学习到不同的特征后融合在一起，本质上还是一种将多种方法结合在一起的作弊行为，虽然很好用</p>
<p>由于该网络是针对 Mesh 网格的，快速略过</p>
<h3 id="editable-parametric-dense-foliage-from-3d-capture14">Editable Parametric Dense Foliage from 3D Capture<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></h3>
<p>论文提出了一种基于 RGB-D 图像的分割单个叶片的算法和利用 B ́ezier patches 模拟叶片表面进行重建叶片点云的方法</p>
<p>虽然很切合植物点云方向，但是由于该算法并不是基于深度学习且使用 C++ 实现，快速略过</p>
<h3 id="efficient-global-2d-3d-matching-for-camera-localization-in-a-large-scale-3d-map15">Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></h3>
<p>论文提出了一种基于 3D 点云的马尔可夫网络，实现了在大型 3D 点云中进行检索特定场景。</p>
<p>由于本文研究场景是在大型场景的 3D 点云与当前研究方向不太符合，略过</p>
<h3 id="egocentric-gesture-recognition-using-recurrent-3d-convolutional-neural-networks-with-spatiotemporal-transformer-modules16">Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></h3>
<p>论文提出了一种基于带有 RSTTM 的 3DCNN 模型，用于实现手势识别。此处的 3D 是指二维图像加上时间维度，即时序图像。</p>
<p>以为本文的 3D 是指三维立体物体，略过</p>
<h3 id="embedding-3d-geometric-features-for-rigid-object-part-segmentation17">Embedding 3D Geometric Features for Rigid Object Part Segmentation<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h3>
<p>论文提出了一种基于 FCN 框架的双流 CNN 网络模型，其中一个名为 AppNet 的流可以从输入图像中提取 2D 外观，另一个名为 GeoNet 的流能够提取 3D 集合特征，将双流输出的特征结合起来实现对图像中物体的零件分割。由于 GeoNet 流的输入为二维图像，输出为 3D 集合特征，因此需要利用名为 VolNet 的基于 2D-CNN 的网络从 3D 体素中提取 3D 集合特征，并以此训练 GeoNet</p>
<p>该论文主要是利用图像和体素，与研究方向不符，略过</p>
<h3 id="end-to-end-learning-of-geometry-and-context-for-deep-stereo-regression18">End-to-End Learning of Geometry and Context for Deep Stereo Regression<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></h3>
<p>论文提出了一种基于 2D-CNN 和 3D-CNN 的实现双目摄像机图像生成视差图像的卷积神经网络</p>
<p>该论文方向不属于三维重建，与研究方向不符，略过</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 2017. 3D Graph Neural Networks for RGBD Semantic Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5209–5218. DOI:<a href="https://doi.org/10.1109/ICCV.2017.556" class="uri">https://doi.org/10.1109/ICCV.2017.556</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Jean Lahoud and Bernard Ghanem. 2017. 2D-Driven 3D Object Detection in RGB-D Images. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 4632–4640. DOI:<a href="https://doi.org/10.1109/ICCV.2017.495" class="uri">https://doi.org/10.1109/ICCV.2017.495</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Wuyuan Xie, Miaohui Wang, Xianbiao Qi, and Lei Zhang. 2017. 3D Surface Detail Enhancement from a Single Normal Map. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2344–2352. DOI:<a href="https://doi.org/10.1109/ICCV.2017.255" class="uri">https://doi.org/10.1109/ICCV.2017.255</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, and Derek Hoiem. 2017. 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 900–909. DOI:<a href="https://doi.org/10.1109/ICCV.2017.103" class="uri">https://doi.org/10.1109/ICCV.2017.103</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, and Jiwen Lu. 2017. 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5679–5688. DOI:<a href="https://doi.org/10.1109/ICCV.2017.605" class="uri">https://doi.org/10.1109/ICCV.2017.605</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Seil Na, Sangho Lee, Jisung Kim, and Gunhee Kim. 2017. A Read-Write Memory Network for Movie Story Understanding. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 677–685. DOI:<a href="https://doi.org/10.1109/ICCV.2017.80" class="uri">https://doi.org/10.1109/ICCV.2017.80</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. 2017. Adversarial Examples for Semantic Segmentation and Object Detection. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1378–1387. DOI:<a href="https://doi.org/10.1109/ICCV.2017.153" class="uri">https://doi.org/10.1109/ICCV.2017.153</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Jiuxiang Gu, Gang Wang, Jianfei Cai, and Tsuhan Chen. 2017. An Empirical Study of Language CNN for Image Captioning. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1231–1240. DOI:<a href="https://doi.org/10.1109/ICCV.2017.138" class="uri">https://doi.org/10.1109/ICCV.2017.138</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Mahdi Rad and Vincent Lepetit. 2017. BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 3848–3856. DOI:<a href="https://doi.org/10.1109/ICCV.2017.413" class="uri">https://doi.org/10.1109/ICCV.2017.413</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel Pruning for Accelerating Very Deep Neural Networks. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1398–1406. DOI:<a href="https://doi.org/10.1109/ICCV.2017.155" class="uri">https://doi.org/10.1109/ICCV.2017.155</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Colored Point Cloud Registration Revisited. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 143–152. DOI:<a href="https://doi.org/10.1109/ICCV.2017.25" class="uri">https://doi.org/10.1109/ICCV.2017.25</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, and Jianxiong Xiao. 2017. DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1201–1210. DOI:<a href="https://doi.org/10.1109/ICCV.2017.135" class="uri">https://doi.org/10.1109/ICCV.2017.135</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>Haotian Xu, Ming Dong, and Zichun Zhong. 2017. Directionally Convolutional Networks for 3D Shape Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2717–2726. DOI:<a href="https://doi.org/10.1109/ICCV.2017.294" class="uri">https://doi.org/10.1109/ICCV.2017.294</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>Paul Beardsley and Gaurav Chaurasia. 2017. Editable Parametric Dense Foliage from 3D Capture. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5315–5324. DOI:<a href="https://doi.org/10.1109/ICCV.2017.567" class="uri">https://doi.org/10.1109/ICCV.2017.567</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>Liu Liu, Hongdong Li, and Yuchao Dai. 2017. Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2391–2400. DOI:<a href="https://doi.org/10.1109/ICCV.2017.260" class="uri">https://doi.org/10.1109/ICCV.2017.260</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Congqi Cao, Yifan Zhang, Yi Wu, Hanqing Lu, and Jian Cheng. 2017. Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 3783–3791. DOI:<a href="https://doi.org/10.1109/ICCV.2017.406" class="uri">https://doi.org/10.1109/ICCV.2017.406</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>Yafei Song, Xiaowu Chen, Jia Li, and Qinping Zhao. 2017. Embedding 3D Geometric Features for Rigid Object Part Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 580–588. DOI:<a href="https://doi.org/10.1109/ICCV.2017.70" class="uri">https://doi.org/10.1109/ICCV.2017.70</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. 2017. End-to-End Learning of Geometry and Context for Deep Stereo Regression. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 66–75. DOI:<a href="https://doi.org/10.1109/ICCV.2017.17" class="uri">https://doi.org/10.1109/ICCV.2017.17</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>PointNet++ 论文总结</title>
    <url>/posts/3e81cdbb.html</url>
    <content><![CDATA[<h2 id="第一遍阅读论文">第一遍阅读论文</h2>
<h3 id="感受">感受</h3>
<ol type="1">
<li><a href="https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html">PointNet++</a> 的网络结构和 FPN 结构很像，都使用“由上至下——下采样”、“横向连接——跨级链接”及“由下至上——上采样”的结构，从而对多尺度特征图进行融合，将高层的语义信息与低层的几何细节结合。</li>
<li>相比起 <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html">PointNet</a> 论文，这篇论文更难懂一点。或许是因为 PointNet 结构很简单。</li>
</ol>
<h3 id="疑惑">疑惑</h3>
<ol type="1">
<li><p>PointNet++ 对点云进行多次降采样后调用 PointNet</p>
<p>那么调用的是 PointNet 的哪一个模型？是分类模型还是语义分割模型？ PointNet 的输出结果是怎样的？</p></li>
<li><p>采样层使用最远点采样法进行分组</p>
<p>计算距离的坐标基点是哪里？是原点吗？计算的是什么距离？是欧氏距离吗？</p></li>
</ol>
<h2 id="第二遍阅读论文">第二遍阅读论文</h2>
<h3 id="abstract">Abstract</h3>
<ol type="1">
<li><p>PointNet 的缺点</p>
<p>PointNet 不能很好地捕捉到局部结构信息，其识别细腻度模式的能力较弱，且难以应用在复杂场景</p></li>
<li><p>PointNet++ 的优点</p>
<p>PointNet++ 引入分层神经网络，在嵌套分区上递归应用 PointNet ；通过距离学习不同尺度的局部信息；加入集合学习层，以自适应地结合来自多个尺度的特征</p></li>
</ol>
<h3 id="introduction">Introduction</h3>
<ol type="1">
<li><p>PointNet++ 的特征提取过程</p>
<ol type="1">
<li>PointNet++ 通过底层空间的距离度量将点集划分为若干重叠的局部单元</li>
<li>PointNet++ 从局部单元中提取捕捉精细几何结构的局部特征；</li>
<li>PointNet++ 将所有局部特征被进一步分组为更大的单元并被处理以产生更高层次的特征</li>
<li>PointNet++ 重复步骤 3 ，直到我们得到整个点集的特征。</li>
</ol></li>
<li><p>PointNet++ 面临的问题</p>
<ol type="1">
<li><p>如何产生局部单元</p>
<p>通过最远点采样法选取分区质心</p></li>
<li><p>如何通过局部特征学习器抽象出点集或局部特征</p></li>
<li><p>如何在密度不均匀的点集进行自适应分组</p>
<ul>
<li>在图像中，较小的卷积核可以提升 CNN 提取特征的能力</li>
<li>在点集中，在较小的邻域不足以让网络稳健地提取特征</li>
</ul></li>
</ol></li>
</ol>
<h3 id="problem-statement">Problem Statement</h3>
<p>设计一个输入为点集，输出为分级的语义信息的函数</p>
<h3 id="method">Method</h3>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/xhqnjzpP1SsQKrR.png" alt=""><figcaption>PointNet++ 网络结构图</figcaption>
</figure>
<ol type="1">
<li><p>回顾 PointNet</p>
<p><span class="math display">\[f(x_1,...,x_n) ≈ \gamma(\max_{i=1,...,n}\{h(x_i)\})\tag{1}\]</span></p>
<p>PointNet 简单高效，具有很强的鲁棒性，但是缺乏在不同尺度上捕捉局部结构的能力</p></li>
<li><p>分层特征学习</p>
<p>PointNet++ 的分层结构由多层 set abstraction 层级构成</p>
<p>每层 set abstraction 层级由采样层、分组层和 PointNet 层构成</p>
<ul>
<li><p>采样层从点集中选择一些点作为中心点</p></li>
<li><p>分组层寻找中心点周围的相邻点来构建局部单元</p></li>
<li><p>PointNet 层使用 mini-PointNet 从局部单元中提取特征</p></li>
</ul>
<p>set abstraction 层级的输入是 <span class="math inline">\(N \times (d + C)\)</span> 的矩阵，输出是 <span class="math inline">\(N' \times (d' + C')\)</span> 的矩阵</p>
<blockquote>
<p>输入矩阵来自 $N $ 个具有 $ d $ 维坐标和 $ C$ 维特征的点 输出矩阵由 $N' $ 个具有 $ d' $ 维坐标和 $ C'$ 维特征的点组成</p>
</blockquote>
<ol type="1">
<li><p>采样层</p>
<p>该层的输入是点集 <span class="math inline">\(\{x_1, x_2, ..., x_n \}\)</span> ，输出是子点集 <span class="math inline">\(\{x_{i_1}, x_{i_2}, ..., x_{i_n} \}\)</span></p>
<p>采样层使用最远采样法，从点集中选择几何距离较远的点作为子点集，子点集中的点即为中心点</p>
<p>常用的几何距离有欧氏距离和测地距离</p>
<p>选择初始点的方法有随机选取和选取距离点云重心的最远点</p>
<p>最远采样法相比于随机采样，能够更好覆盖整个点集；相比于 CNN ，最远采样法以一种依赖数据的方式生成感受野</p></li>
<li><p>分组层</p>
<p>该层的输入是整个点集构成的 <span class="math inline">\(N \times (d + C)\)</span> 矩阵和采样层输出的子点集的坐标构成的 <span class="math inline">\(N' \times d\)</span> 矩阵，输出是 <span class="math inline">\(N' \times K \times (d + C)\)</span> 的矩阵</p>
<blockquote>
<p>K 是中心点附近的点数，可以自适应调整</p>
</blockquote>
<p>分组层使用球状半径查询，从点集中寻找中心点半径范围内的 K 个点</p>
<p>球状半径查询相比于 kNN 搜索，能够保证固定的区域尺度</p></li>
<li><p>PointNet 层</p>
<p>该层的输入是 <span class="math inline">\(N' \times K \times (d + C)\)</span> 矩阵，输出是 <span class="math inline">\(N' \times (d + C')\)</span> 的矩阵</p></li>
</ol></li>
<li><p>非均匀采样密度下的特征学习</p>
<ol type="1">
<li><p>多尺度分组</p>
<p>对每个尺度的分组都送入 PointNet 层进行特征提取，然后将特征连接起来作为中心点的特征</p>
<p>计算成本很高</p></li>
<li><p>多分辨率分组</p>
<p>将大尺度的特征和送入 PointNet 层提取的小尺度特征进行拼接</p>
<p>在密度较低的区域，自适应提升大尺度特征的权重</p>
<p>在密度较高的区域，自适应提升送入 PointNet 层提取的小尺度特征的权重</p></li>
<li><p>随机丢弃输入数据</p>
<p>在将数据输入进 PointNet 层之前进行 95% 的重新采样</p></li>
</ol></li>
<li><p>分割任务中点特征的传播</p>
<p>在语义分割任务中，希望得到所有原始点的点特征</p>
<ol type="1">
<li><p>将所有的点都作为中心点进行采样分组</p>
<p>计算成本很高</p></li>
<li><p>将特征用内插特征值从子采样点传播到原始点</p>
<p>内插特征值使用基于 k 近邻的反距离加权平均</p>
<p>将内插的特征值与跳过的链接点特征相连接</p>
<p><span class="math display">\[{f^{(i)}} = {\frac {\sum_{i=1}^{k} {w_{i}(x)} {f}_{i}^{(j)}} {\sum_{i=1}^{k}{w_{i}(x)}}} \quad where \quad {w}_{i}(x) = {\frac {1} {d(x, x_i)^p}}, j = 1, ..., C \qquad \tag{2}\]</span></p></li>
</ol></li>
</ol>
<h3 id="experiments">Experiments</h3>
<ol type="1">
<li><p>数据集</p>
<ol type="1">
<li><p>MNIST</p>
<p>70k 张二维图片，按 6:1 比例划分训练集和测试集</p></li>
<li><p>ModelNet40</p>
<p>40 类 12311 个 CAD 模型，按 4:1 比例划分训练集和测试集</p></li>
<li><p>SHREC15</p>
<p>50 类 1200 个 CAD 模型，每个类别包括 24 种形状，使用五折交叉验证</p></li>
<li><p>ScanNet</p>
<p>1513 个室内扫描重建场景，按 4:1 比例划分训练集和测试集</p></li>
</ol></li>
<li><p>欧氏几何空间中的点集分类</p>
<p>在 MNIST 和 ModelNet40 数据集中的表现明显优于 PointNet</p>
<p>在模拟的不均匀数据集中，保持很好的性能</p></li>
<li><p>场景语义分割</p>
<p>对抽样密度变化具有稳健性</p></li>
<li><p>非欧几何空间中的点集分类</p>
<p>对每个模型构建由成对的测地线距离诱导的度量空间</p>
<p>提取 WKS 、 HKS 和多尺度高斯曲率等特征作为特征，而不是以三维坐标作为特征</p>
<p>因为以三维坐标作为特征不能揭示内在结构，而且受姿势变化的影响很大</p></li>
<li><p>特征可视化</p>
<p>通过特征可视化可以看出 PointNet++ 学习到了物体的基本结构</p></li>
</ol>
<h3 id="conclusion">conclusion</h3>
<p>改进方向：</p>
<p>如何通过在每个局部区域分享更多的计算来加快网络的推理速度</p>
<h2 id="复现结果">复现结果</h2>
<p>代码源自 yanx27 - <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch">PyTorch Implementation of PointNet and PointNet++</a></p>
<p>输入如下指令开始训练 PointNet++ 分类模型</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">python train_classification.py --model pointnet2_cls_ssg --log_dir pointnet2_cls_ssg</span><br></pre></td></tr></tbody></table></figure>
<p>训练最终结果</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">Epoch <span class="number">200</span> (<span class="number">200</span>/<span class="number">200</span>):</span><br><span class="line">Train Instance Accuracy: <span class="number">0.976626</span></span><br><span class="line">Test Instance Accuracy: <span class="number">0.920146</span>, Class Accuracy: <span class="number">0.890121</span></span><br><span class="line">Best Instance Accuracy: <span class="number">0.926133</span>, Class Accuracy: <span class="number">0.898150</span></span><br><span class="line">End of training...</span><br></pre></td></tr></tbody></table></figure>
<p>输入如下指令开始测试 PointNet++ 分类模型</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">python test_classification.py --log_dir pointnet2_cls_ssg</span><br></pre></td></tr></tbody></table></figure>
<p>测试最终结果</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">Test Instance Accuracy: <span class="number">0.927751</span>, Class Accuracy: <span class="number">0.896012</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考文章">参考文章</h2>
<p>刘昕宸 - <a href="https://zhuanlan.zhihu.com/p/266324173">搞懂 PointNet++ ，这篇文章就够了！</a></p>
<p>Guoguang Du - <a href="https://blog.csdn.net/dsoftware/article/details/107184116">最远点采样(Farthest Point Sampling)介绍</a></p>
<h2 id="论文总结">论文总结</h2>
<h3 id="pointnet-针对-pointnet-提出的改进">PointNet++ 针对 PointNet 提出的改进</h3>
<ol type="1">
<li><p>PointNet 只是提取点云中各个点的空间特征，然后聚合成全局信息。这样不能很好地提取局部结构信息</p>
<p>PointNet++ 通过采样和分组将点集划分为不同的重叠区域，即整合局部邻域</p></li>
<li><p>PointNet 在聚合成全局信息时只使用了最大值池化。这样会损失大量信息</p>
<p>PointNet++ 采用多层神经网络，对点集中的分组不断进行下采样，提取不同尺度下的 local-global feature</p></li>
<li><p>PointNet 中的语义分割模型提取的中间特征是全局特征和局部特征拼接而成，特征的差异度不够</p>
<p>PointNet++ 先进行下采样再进行上采样，使用跨级链接和内插特征值拼接对应层的 local-global feature</p></li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>PointNet++</tag>
        <tag>点云</tag>
      </tags>
  </entry>
  <entry>
    <title>PointNet 论文总结</title>
    <url>/posts/2ed13e41.html</url>
    <content><![CDATA[<p>作者提出了使用对称网络提取点云特征、局部特征与全局特征相结合的点云处理方法</p>
<p>Point Net 网络简单高效，具有很强的鲁棒性，能够抵抗异常点插入、部分点缺失和部分点抖动</p>
<p>Point Net 网络由 MLP 、全连接层和最大池化层组成</p>
<h2 id="abstract">Abstract</h2>
<p>PointNet 是直接使用点云进行三维目标分类、语义分割、场景语义解析的一种语义感知网络模型</p>
<p>PointNet 网络简单高效，面对微小扰动、点云缺失有很强的鲁棒性</p>
<h2 id="introduction">Introduction</h2>
<ol type="1">
<li><p>卷积不适用于点云数据</p>
<p>因为卷积结构要求输入数据高度规整，而点云是无序、不规整的</p></li>
<li><p>点云不适宜转换为其他形式进行语义感知</p>
<p>因为点云转换为其他形式进行处理时，会使得数据变得庞大</p></li>
<li><p>PointNet 网络的输入是三维坐标集合，输出是每点的标签/物体的标签</p></li>
<li><p>PointNet 网络能够学会利用稀疏的关键点来概括物体的形状，以判断物体的类别</p></li>
</ol>
<h2 id="related-work">Related Work</h2>
<h3 id="点云特征">点云特征</h3>
<p>现有的点云特征提取是针对特殊任务定制的，而找到最佳特征组合并非易事</p>
<h3 id="三维数据的深度学习">三维数据的深度学习</h3>
<ol type="1">
<li><p><em>Volumetric CNNs</em></p>
<p>在体素上应用卷积神经网络</p>
<p>受限于计算成本高和点云稀疏导致转换后的体素分辨率低</p></li>
<li><p><em>FPNN</em> 和 <em>Vote3D</em></p>
<p>提出了处理点云稀疏问题的办法</p>
<p>难以处理数量较大的点云</p></li>
<li><p><em>Multiview CNNs</em></p>
<p>将三维点云转换为二维图像，然后使用 CNN 进行分类</p>
<p>不能胜任语义分割、点云补全等任务</p></li>
<li><p><em>Feature-based DNNs</em></p>
<p>将三维点云转换为矢量，通过提取图形特征来进行分类</p>
<p>受限于特征提取能力，不能很好的完成任务</p></li>
</ol>
<h3 id="无序数据的深度学习">无序数据的深度学习</h3>
<p>人们尚未探索基于无序点集的深度学习领域</p>
<p>而无序数据处理领域少数的硕果，也是关注于 NLP ，不适用于无序点集</p>
<h2 id="problem-statement">Problem Statement</h2>
<ol type="1">
<li><p>点云是三维坐标几何</p></li>
<li><p>对于物体分类任务，网络输出的结果应该是对 k 类的 k 个分数</p></li>
<li><p>对于语义分割任务，网络输出的结果应该是 n 个点对 m 类的 n * m 个分数</p></li>
</ol>
<h2 id="deep-learning-on-point-sets">Deep Learning on Point Sets</h2>
<h3 id="点集的特性">点集的特性</h3>
<ol type="1">
<li><p>无序性</p>
<p>点集是无序且不规整的，因此网络模型需要使用对称函数来提取特征</p>
<blockquote>
<p>对称函数：无论输入顺序如何变化，输出结果保持不变</p>
</blockquote></li>
<li><p>联系性</p>
<p>点集中的每个点不适孤立存在的，都是与周围的点联系的</p>
<p>因此网络模型要能够从附近点捕捉到局部结构，也能将局部特征和全局特征结合起来</p></li>
<li><p>变换不变性</p>
<p>点集在刚性变换后不会改变其语义标签</p>
<p>因此输入刚性变换后的点集，网络模型的输出应该保持不变</p>
<p>因此需要设计对准网络来提取点集特征</p></li>
</ol>
<h3 id="point-net-的结构">Point Net 的结构</h3>
<figure>
<img data-src="https://s2.loli.net/2022/08/09/PdzYgLJWnkHE4K3.png" alt=""><figcaption>Point Net 网络结构图</figcaption>
</figure>
<p>Point Net 网络具有三大关键组成：</p>
<ul>
<li>使用最大池化层作为对称函数从点云中提取信息</li>
<li>局部特征与全局特征的结合机制</li>
<li>使用对准网络对齐输入点集及其特征</li>
</ul>
<ol type="1">
<li><p>针对无序输入的对称函数</p>
<p>对于无序序列，有以下三种解决方法：</p>
<ol type="1">
<li><p>按特定顺序对序列进行排序</p>
<p>但是在高维空间，很难找到一种稳定可靠的排列方式</p>
<p>而且实验证明排序对结果提升不大</p></li>
<li><p>进行数据增强后输入给 RNN</p>
<p>但是 RNN 难以处理大量数据</p></li>
<li><p>使用对称函数来提取点集中的特征</p>
<p>通过集合中的转换元素应用对称函数来近似定义在点集上的对称函数</p>
<p>Point Net 网络采用该种方法</p>
<p><span class="math display">\[f(\{x_1,...,x_n\}) ≈ g(h(x_1),...,h(x_n))\tag{1}\]</span></p>
<p><span class="math display">\[f:{ 2^{ \mathbb{R} } }^N \to \mathbb{R}, h:{ \mathbb{R} }^N \to { \mathbb{R} }^K, g:\underbrace{ {\mathbb{R} }^K \times \cdots \times {\mathbb{R} }^K }_{n} \to \mathbb{R}\]</span></p>
<p>在具体实现中，使用 MLP 来近似 <em>h</em> ，用单变量函数和最大池化函数来近似 <em>g</em></p></li>
</ol></li>
<li><p>局部特征和全局特征相结合</p>
<p>对称函数输出的是描述输入点集全局特征的一个向量</p>
<p>要实现点云分割需要结合局部特征和全局特征</p>
<p>Point Net 网络将全局特征和每个点的特征连接起来再提取每个点新的特征</p></li>
<li><p>对准网络</p>
<p>对点云进行处理需要保证输入点集进行刚性变换后其语义标签不变</p>
<p>一种解决方案是在特征提取前将所有输入点集对齐到一个典型空间</p>
<p>Point Net 网络通过一个微型网络（ T-Net ）预测一个矩阵，将该矩阵与输入坐标相乘实现变换</p>
<p>T-Net 也可以应用在特征空间的对齐，但是由于维度大，矩阵不好优化，需要加以约束，使得矩阵接近正交矩阵</p>
<p><span class="math display">\[L_{reg} = {||I - AA^T||}^{2}_{F}\tag{2}\]</span></p></li>
</ol>
<h3 id="理论分析">理论分析</h3>
<ol type="1">
<li><p>普遍近似</p>
<p>Point Net 网络具有对连续聚合函数普遍近似的能力，能够抵抗微小扰动</p>
<p>当最大池化层有足够多神经元，即式(1)中 K 足够大时， <em>f</em> 可以被任意近似</p>
<p>在最坏情况下， Point Net 网络 可以将空间划分为相同大小的体素，学习将点云转换为体积表示</p></li>
<li><p>瓶颈维度和稳定性</p>
<p>Point Net 网络的表现收到最大池化层维度的影响，即式(1)中 K</p>
<p>输入点集中的微小扰动不会影响 Point Net 网络的输出</p>
<p>Point Net 网络输出的结果实际上由点集中的部分点集决定，而该部分点集由式(1)中 K 决定</p>
<p>该部分点集 <span class="math inline">\(C_s\)</span> 被称为关键点集，式(1)中 K 被称为瓶颈维度，而最大可能的点集 <span class="math inline">\(N_s\)</span> 被称为上限点集</p>
<p>关键点集实际上形成了一个物体的骨架</p></li>
</ol>
<h2 id="experiment">Experiment</h2>
<h3 id="应用">应用</h3>
<ol type="1">
<li><p>物体分类</p>
<p>数据集：ModelNet40</p>
<p>数据构成：12311 个 CAD 模型，40 类物体，4:1 划分训练集和测试集</p>
<p>评价指标：准确率</p>
<p>数据预处理：对网格进行均匀采样后转换为单一球体，即转换为点云</p>
<blockquote>
<p>实际项目中的数据预处理使用 RealityCapture 软件完成</p>
</blockquote>
<p>数据增强：沿上轴随机旋转后通过高斯噪声对每个点位置进行抖动</p>
<p>结果：速度很快，分类准确率很高，但略逊于基于多视图的 MV CNN</p>
<p>分析：网络只有全连接层和最大池化层， CAD 模型转换为点云时损失了几何细节</p></li>
<li><p>物体语义分割</p>
<ul>
<li><p>完整点云实验</p>
<p>数据集： ShapeNet part data set</p>
<p>数据构成：16881 个 CAD 模型，16 类物体，每个物体含有 2 ~ 5 个部件，共有50 类部件</p>
<p>评价指标： mIoU</p>
<p>结果：比大多数现有方法 mIoU 更高</p></li>
<li><p>不完整点云实验</p>
<p>数据集： ShapeNet part data set</p>
<p>数据处理：用 Blensor Kinect Simulator 从六个随机视点生成不完整的点云</p>
<p>结果：相较完整的点云，平均 IoU 只损失了 5.3%</p></li>
</ul></li>
<li><p>场景语义分割</p>
<p>数据集： Stanford 3D semantic parsing data set</p>
<p>数据构成：271 个房间扫描的点云，包含 13 类物体</p>
<p>特殊处理：将房间划分为 1 m * 1 m 的区块，训练时在每个区块随机抽取4096个点</p>
<p>结果：明显优于使用定制的点云特征提取方法</p></li>
<li><p>场景中物体检测</p>
<p>结果：明显优于基于滑动形状法的 SVM</p></li>
</ol>
<h3 id="结构设计分析">结构设计分析</h3>
<ol type="1">
<li><p>对称函数间的比较</p>
<p>对比最大池化、平均池化和基于注意力的加权和</p>
<blockquote>
<p>基于注意力的加权和：从每个点特征预测权值，归一化后计算与特征的加权和</p>
</blockquote>
<p>最大池化的效果最好</p></li>
<li><p>输入转换和特征转换的有效性</p>
<p>使用 T-Net 进行转换使得性能提升 0.8%</p></li>
<li><p>Point Net 网络的鲁棒性</p>
<p>当 50% 的点缺失时，准确率只下降约 3%</p>
<p>当 20% 的点是离群的异常点时，准确率仍高于 80%</p>
<p>当高斯噪声使得点云抖动时，准确率依旧保持得很好</p></li>
</ol>
<h3 id="point-net-网络结果可视化">Point Net 网络结果可视化</h3>
<p>关键点集 <span class="math inline">\(C_s\)</span> 概括了物体形状的骨架</p>
<h3 id="时空复杂度分析">时空复杂度分析</h3>
<p>Point Net 网络比 MVCNN 和 3D CNN 计算成本要低</p>
<p>Point Net 网络在 TensorFlow 上使用 1080X GPU 每秒可以处理 1000K 个点，完成 1K 个物体的分类或 2 个房间内的场景语义分割</p>
<h2 id="conclusion">Conclusion</h2>
<p>Point Net 网络直接使用点云完成物体分类、语义分割和场景语义解析，且获得了很好的结果</p>
<h2 id="补充">补充</h2>
<p>Point Net 模型的激活函数均使用 ReLu 函数</p>
<p>在进行点云分类时，在最后的 MLP 中加入了 Dropout 层</p>
<h2 id="问题与解答">问题与解答</h2>
<ol type="1">
<li><p>如何将全局特征和局部特征连接在一起？</p>
<p></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">global_feat_expand = tf.tile(global_feat, [<span class="number">1</span>, num_point, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">concat_feat = tf.concat(<span class="number">3</span>, [point_feat, global_feat_expand])</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>使用 <code>tf.tile(input, multiples, name=None)</code> 将全局特征复制拓展，使用 <code>tf.concat([tensor1, tensor2], axis)</code> 与局部特征拼接起来</p>
<p><code>tf.tile(input, multiples, name=None)</code>是将 input 各维度对应 multiples 参数复制</p>
<blockquote>
<p>multiples 参数维度必须 input 维度一致，表示在第几维上复制几次 <code>multiples = [1, num_point, 1, 1]</code> 表示在第二维上复制 num_point 次</p>
</blockquote>
<p><code>tf.concat([tensor1, tensor2], axis)</code>是将 tensor1 和 tensor2 在 axis 维度上拼接起来</p>
<table>
<thead>
<tr class="header">
<th>变量</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>point_feat</td>
<td>[None, num_point, 1, 64]</td>
</tr>
<tr class="even">
<td>global_feat</td>
<td>[None, 1, 1, 1024]</td>
</tr>
<tr class="odd">
<td>global_feat_expand</td>
<td>[None, num_point, 1, 1024]</td>
</tr>
<tr class="even">
<td>concat_feat</td>
<td>[None, num_point, 1, 1088]</td>
</tr>
</tbody>
</table></li>
<li><p>T-Net 网络内部是怎样的？</p>
<table>
<thead>
<tr class="header">
<th>处理</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td>[None, num_point, 3, 1]</td>
</tr>
<tr class="even">
<td>1*3卷积核-64</td>
<td>[None, num_point, 1, 64]</td>
</tr>
<tr class="odd">
<td>conv1-128</td>
<td>[None, num_point, 1, 128]</td>
</tr>
<tr class="even">
<td>conv1-1024</td>
<td>[None, num_point, 1, 1024]</td>
</tr>
<tr class="odd">
<td>Max Pooling</td>
<td>[None, 1, 1, 1024]</td>
</tr>
<tr class="even">
<td>Reshape</td>
<td>[None, 1024]</td>
</tr>
<tr class="odd">
<td>FC-512</td>
<td>[None, 512]</td>
</tr>
<tr class="even">
<td>FC-256</td>
<td>[None, 256]</td>
</tr>
<tr class="odd">
<td>MLP</td>
<td>[None, 1, 9]</td>
</tr>
<tr class="even">
<td>Reshape</td>
<td>[None, 3, 3]</td>
</tr>
<tr class="odd">
<td>Output</td>
<td>[None, 3, 3]</td>
</tr>
</tbody>
</table>
<p>使用 T-Net 对数据进行转换可以理解为将数据进行矫正，对齐到到统一视角</p></li>
<li><p>为什么使用 Shared MLP ？</p>
<p>和普通 MLP 一样，都是实现特征提取</p>
<p>但是由于每个点的权重共享，可以节约大量参数</p></li>
<li><p>如何对点集应用 Shared MLP ？</p>
<p>实现时，使用 conv1，即使用 1 * 1 的卷积核进行卷积</p></li>
<li><p>模型的 Loss 如何计算？</p>
<p><code>classify_loss + mat_diff_loss * reg_weight</code></p>
<p>平均多分类稀疏 softmax 交叉熵 + T-Net 产生预测矩阵的约束（式(2)） * 权重（默认为0.001）</p></li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>点云</tag>
        <tag>PointNet</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 实现分类问题总结-续</title>
    <url>/posts/49b59d4e.html</url>
    <content><![CDATA[<h2 id="问题回顾">问题回顾</h2>
<h3 id="多线程加载-dataset">多线程加载 Dataset</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">dataset = TitanicDataset(<span class="string">"./dataset/titanic/processed_train.csv"</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>其中参数 num_works 表示载入数据时使用的进程数，此时如果参数的值不为0而使用多进程时会出现报错</p>
<blockquote>
<p>RuntimeError: An attempt has been made to start a new process before the current process has finished its bootstrapping phase. This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module: if <code>__name__ == '__main__':</code> freeze_support() ... The "<code>freeze_support()</code>" line can be omitted if the program is not going to be frozen to produce an executable.</p>
</blockquote>
<h4 id="解决方法">解决方法</h4>
<p>当参数 num_works 不为 0 时，需要在数据调用前之前加上 <code>if __name__ == '__main__':</code></p>
<h4 id="参考网页">参考网页</h4>
<p>diandianti - <a href="https://www.jianshu.com/p/4a1a92f0efd9">pytorch-Dataloader多进程使用出错</a></p>
<h3 id="k-折交叉校验法">K 折交叉校验法</h3>
<p>K 折交叉校验法是将训练集划分为 K 份子数据集，然后进行 K 轮训练，在第 K 轮训练时，将第 K 份子数据集作为验证集来评估训练结果，最终会得到 K 个结构相同，参数不同的网络，然后再从中选择评价最好模型</p>
<p>在实际训练中可以将下列代码嵌入训练过程中</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">kfold = KFold(n_splits=k_folds, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> fold, (train_ids, test_ids) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(train_dataset)):</span><br><span class="line">    train_subsampler = SubsetRandomSampler(train_ids)</span><br><span class="line">    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)</span><br><span class="line">    test_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=test_subsampler)</span><br><span class="line">    <span class="comment"># training epoch</span></span><br></pre></td></tr></tbody></table></figure>
<p>使得在每一折训练时，训练集和验证集能够变化</p>
<h4 id="参考网页-1">参考网页</h4>
<p>Jennie_J - <a href="https://blog.csdn.net/weixin_43685844/article/details/88635492">sklearn KFold()</a></p>
<p>外部逍遥 - <a href="https://zhuanlan.zhihu.com/p/150446294">python sklearn中KFold与StratifiedKFold</a></p>
<p>christianversloot - <a href="https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md">How to use K-fold Cross Validation with PyTorch?</a></p>
<h3 id="归一化问题">归一化问题</h3>
<h4 id="归一化与划分训练集和验证集顺序问题">归一化与划分训练集和验证集顺序问题</h4>
<p>应当先进行训练集划分再进行归一化</p>
<p>因为归一化实际上是从数据集中提取信息，而算法不应知道来自训练集以外信息，否则一定程度上会导致标签泄漏，造成训练结果出现偏差</p>
<h4 id="参考网页-2">参考网页</h4>
<p>PigOrz - <a href="https://blog.csdn.net/qq_33731081/article/details/103852478">【关于归一化与反归一化数据统一的问题】：训练集与测试集必须使用同一参数的归一化与反归一化</a></p>
<p>lizju - <a href="https://blog.csdn.net/weixin_42227482/article/details/105829627">DataFrame的归一化</a></p>
<p>Shian150629 - <a href="https://blog.csdn.net/weixin_43759518/article/details/113880715">标准化，归一化与训练-测试集数据处理</a></p>
<h3 id="模型保存">模型保存</h3>
<h4 id="保存加载模型参数">保存/加载模型参数</h4>
<ol type="1">
<li><p>保存</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>加载</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol>
<h4 id="保存加载整个模型">保存/加载整个模型</h4>
<ol type="1">
<li><p>保存</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>加载</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Model class must be defined somewhere</span></span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol>
<h4 id="参考网页-3">参考网页</h4>
<p>爱不持久 - <a href="https://blog.csdn.net/wacebb/article/details/108021921">Pytorch如何保存和加载模型参数</a></p>
<p>机器学习入坑者 - <a href="https://zhuanlan.zhihu.com/p/94971100">一文梳理pytorch保存和重载模型参数攻略</a></p>
<p>PyTorch - <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">SAVING AND LOADING MODELS</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 实现分类问题总结</title>
    <url>/posts/4fd12987.html</url>
    <content><![CDATA[<h2 id="数据集处理">数据集处理</h2>
<h3 id="实验记录">实验记录</h3>
<p>在 Kaggle 网站上下载 <a href="https://www.kaggle.com/competitions/titanic/data?select=train.csv">Titanic 数据集</a>，解压在 <code>./dataset/Titanic</code> 下</p>
<p>由于 Jupiter 可以实现数据的实时可视化，在此使用 Jupiter 进行数据集的观察与处理</p>
<ol type="1">
<li><p>新建 <code>.ipynb</code> 文件后导入 Python 库</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>设置数据集的相对地址</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">TRAIN_PATH = <span class="string">"./dataset/titanic/train.csv"</span></span><br><span class="line">PROCESSED_TRAIN_PATH = <span class="string">"./dataset/titanic/processed_train.csv"</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>读取并显示 train dataset 信息</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(TRAIN_PATH)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">12</span> columns):</span><br><span class="line"> <span class="comment">#   Column       Non-Null Count  Dtype</span></span><br><span class="line">---  ------       --------------  -----</span><br><span class="line"> <span class="number">0</span>   PassengerId  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Survived     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Pclass       <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">3</span>   Name         <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">4</span>   Sex          <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">5</span>   Age          <span class="number">714</span> non-null    float64</span><br><span class="line"> <span class="number">6</span>   SibSp        <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">7</span>   Parch        <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">8</span>   Ticket       <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">9</span>   Fare         <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">10</span>  Cabin        <span class="number">204</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">11</span>  Embarked     <span class="number">889</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">5</span>), <span class="built_in">object</span>(<span class="number">5</span>)</span><br><span class="line">memory usage: <span class="number">83.7</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>可以看到 PassengerId 、 Name 、 Ticket 三列数据与是否生存无关，将其丢弃</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data = train_data.drop([<span class="string">'PassengerId'</span>, <span class="string">'Name'</span>, <span class="string">'Ticket'</span>], axis=<span class="number">1</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">714</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Cabin     <span class="number">204</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">8</span>   Embarked  <span class="number">889</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), <span class="built_in">object</span>(<span class="number">3</span>)</span><br><span class="line">memory usage: <span class="number">62.8</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>Age 数据缺失，现在使用均值进行补充</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">avg_age = train_data[<span class="string">'Age'</span>].mean()</span><br><span class="line">train_data[<span class="string">'Age'</span>] = train_data[<span class="string">'Age'</span>].fillna(avg_age)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Cabin     <span class="number">204</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">8</span>   Embarked  <span class="number">889</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), <span class="built_in">object</span>(<span class="number">3</span>)</span><br><span class="line">memory usage: <span class="number">62.8</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>由于 Cabin 数据确实太多，将其丢弃</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data = train_data.drop([<span class="string">'Cabin'</span>], axis=<span class="number">1</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Embarked  <span class="number">889</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), <span class="built_in">object</span>(<span class="number">2</span>)</span><br><span class="line">memory usage: <span class="number">55.8</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>查看 Embarked 数据中的众数</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Embarked'</span>].value_counts()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">S    <span class="number">644</span></span><br><span class="line">C    <span class="number">168</span></span><br><span class="line">Q     <span class="number">77</span></span><br><span class="line">Name: Embarked, dtype: int64</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>使用众数进行补充缺失的数据</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Embarked'</span>] = train_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Embarked  <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), <span class="built_in">object</span>(<span class="number">2</span>)</span><br><span class="line">memory usage: <span class="number">55.8</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>查看 Sex 数据的情况</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Sex'</span>].head()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">0</span>      male</span><br><span class="line"><span class="number">1</span>    female</span><br><span class="line"><span class="number">2</span>    female</span><br><span class="line"><span class="number">3</span>    female</span><br><span class="line"><span class="number">4</span>      male</span><br><span class="line">Name: Sex, dtype: <span class="built_in">object</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>将 Sex 数据的值映射成数值</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">sex_2_dict = {<span class="string">"male"</span>: <span class="number">0</span>, <span class="string">"female"</span>:<span class="number">1</span>}</span><br><span class="line">train_data[<span class="string">'Sex'</span>] = train_data[<span class="string">'Sex'</span>].<span class="built_in">map</span>(sex_2_dict)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Embarked  <span class="number">891</span> non-null    <span class="built_in">object</span></span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), <span class="built_in">object</span>(<span class="number">1</span>)</span><br><span class="line">memory usage: <span class="number">55.8</span>+ KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>查看 Embarked 数据的情况</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data[<span class="string">'Embarked'</span>].head()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">0</span>    S</span><br><span class="line"><span class="number">1</span>    C</span><br><span class="line"><span class="number">2</span>    S</span><br><span class="line"><span class="number">3</span>    S</span><br><span class="line"><span class="number">4</span>    S</span><br><span class="line">Name: Embarked, dtype: <span class="built_in">object</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>将 Embarked 数据的值映射成数值</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">embarked_2_dict = {<span class="string">"C"</span>: <span class="number">0</span>, <span class="string">"Q"</span>: <span class="number">1</span>, <span class="string">"S"</span>: <span class="number">2</span>}</span><br><span class="line">train_data[<span class="string">'Embarked'</span>] = train_data[<span class="string">'Embarked'</span>].<span class="built_in">map</span>(embarked_2_dict)</span><br><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line"> <span class="comment">#   Column    Non-Null Count  Dtype</span></span><br><span class="line">---  ------    --------------  -----</span><br><span class="line"> <span class="number">0</span>   Survived  <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">1</span>   Pclass    <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">2</span>   Sex       <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">3</span>   Age       <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">4</span>   SibSp     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">5</span>   Parch     <span class="number">891</span> non-null    int64</span><br><span class="line"> <span class="number">6</span>   Fare      <span class="number">891</span> non-null    float64</span><br><span class="line"> <span class="number">7</span>   Embarked  <span class="number">891</span> non-null    int64</span><br><span class="line">dtypes: float64(<span class="number">2</span>), int64(<span class="number">6</span>)</span><br><span class="line">memory usage: <span class="number">55.8</span> KB</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>将处理好的数据集导出到文件夹中</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.to_csv(PROCESSED_TRAIN_PATH, index=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h3 id="回顾">回顾</h3>
<h4 id="conda-安装-jupiter-报错">conda 安装 Jupiter 报错</h4>
<p>打开 clash 后，用 conda 安装 Jupiter 报错</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">CondaHTTPError: HTTP 000 CONNECTION FAILED <span class="keyword">for</span> url &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64/current_repodata.json&gt;</span><br></pre></td></tr></tbody></table></figure>
<h5 id="错误出现原因">错误出现原因</h5>
<p>clash 代理影响 conda 与 conda 源的连接</p>
<h5 id="解决方法1">解决方法1</h5>
<p>在用户文件夹下的 <code>.condarc</code> 文件中添加代理端口</p>
<p>clash 的默认代理端口为 7890</p>
<figure class="highlight txt"><table><tbody><tr><td class="code"><pre><span class="line">proxy_servers:</span><br><span class="line">http: http://127.0.0.1:7890</span><br><span class="line">https: https://127.0.0.1:7890</span><br></pre></td></tr></tbody></table></figure>
<p>但在后续的安装中依旧报错</p>
<figure class="highlight txt"><table><tbody><tr><td class="code"><pre><span class="line">Retrieving notices: ...working... failed</span><br><span class="line">ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1129)</span><br></pre></td></tr></tbody></table></figure>
<p>此解决方法<strong>无效</strong></p>
<h5 id="解决方法2">解决方法2</h5>
<p>关闭 clash</p>
<h5 id="参考网页">参考网页</h5>
<p>wielice - <a href="https://blog.csdn.net/weixin_38789153/article/details/121598812">conda 配置本机代理</a></p>
<p>好事要发生 - <a href="https://blog.csdn.net/weixin_43505418/article/details/123711162">An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent......</a></p>
<h4 id="pandas.read_csv-函数"><code>pandas.read_csv()</code> 函数</h4>
<p>从 CSV 文件中读取数据到 DataFrame</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">pandas.read_csv(filepath_or_buffer, sep=NoDefault.no_default,</span><br><span class="line">delimiter=<span class="literal">None</span>, header=<span class="string">'infer'</span>, names=NoDefault.no_default,</span><br><span class="line">index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, squeeze=<span class="literal">None</span>, prefix=NoDefault.no_default,</span><br><span class="line">mangle_dupe_cols=<span class="literal">True</span>, dtype=<span class="literal">None</span>, engine=<span class="literal">None</span>, converters=<span class="literal">None</span>,</span><br><span class="line">true_values=<span class="literal">None</span>, false_values=<span class="literal">None</span>, skipinitialspace=<span class="literal">False</span>,</span><br><span class="line">skiprows=<span class="literal">None</span>, skipfooter=<span class="number">0</span>, nrows=<span class="literal">None</span>, na_values=<span class="literal">None</span>,</span><br><span class="line">keep_default_na=<span class="literal">True</span>, na_filter=<span class="literal">True</span>, verbose=<span class="literal">False</span>,</span><br><span class="line">skip_blank_lines=<span class="literal">True</span>, parse_dates=<span class="literal">None</span>, infer_datetime_format=<span class="literal">False</span>,</span><br><span class="line">keep_date_col=<span class="literal">False</span>, date_parser=<span class="literal">None</span>, dayfirst=<span class="literal">False</span>,</span><br><span class="line">cache_dates=<span class="literal">True</span>, iterator=<span class="literal">False</span>, chunksize=<span class="literal">None</span>, compression=<span class="string">'infer'</span>,</span><br><span class="line">thousands=<span class="literal">None</span>, decimal=<span class="string">'.'</span>, lineterminator=<span class="literal">None</span>, quotechar=<span class="string">'"'</span>,</span><br><span class="line">quoting=<span class="number">0</span>, doublequote=<span class="literal">True</span>, escapechar=<span class="literal">None</span>, comment=<span class="literal">None</span>,</span><br><span class="line">encoding=<span class="literal">None</span>, encoding_errors=<span class="string">'strict'</span>, dialect=<span class="literal">None</span>,</span><br><span class="line">error_bad_lines=<span class="literal">None</span>, warn_bad_lines=<span class="literal">None</span>, on_bad_lines=<span class="literal">None</span>,</span><br><span class="line">delim_whitespace=<span class="literal">False</span>, low_memory=<span class="literal">True</span>, memory_map=<span class="literal">False</span>,</span><br><span class="line">float_precision=<span class="literal">None</span>, storage_options=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="部分重要参数解析">部分重要参数解析</h5>
<ol type="1">
<li><p>filepath_or_buffer</p>
<p>str ，路径对象或类文件对象</p>
<p>任何有效的字符串路径都可以接受</p>
<p>该字符串可以是一个 URL</p>
<p>有效的 URL 方案包括 http 、 ftp 、 s3 、 gs 和 file</p></li>
<li><p>sep</p>
<p>str ，默认为','</p>
<p>长于 1 个字符且与' + '不同的分隔符将被解释为正则表达式</p></li>
<li><p>delimiter</p>
<p>str ，与 sep 参数功能相同，但默认为 'None'</p>
<p>当 sep 参数与 delimiter 参数均不为 None 时，弹出错误信息</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">ValueError: Specified a sep <span class="keyword">and</span> a delimiter; you can only specify one.</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>header</p>
<p>int 或 int 列表 或 None ，但默认为 'Infer'</p>
<p>指定表头在数据中的行数</p>
<ul>
<li>当 header 参数为默认的 'Infer' 且 names 参数为 None 时，等价于 header = 0 ，此时将读取数据的第一行作为表头/列名</li>
<li>当 header 参数为默认的 'Infer' 且 names 参数不为 None 时，等价于 header = None ，此时将读取 names 参数作为表头/列名</li>
<li>当 header 参数为 int 时，将从整数对应的行号读取列名</li>
<li>当 header 参数为 int 列表时，将从列表对应的行号读取列名</li>
<li>当 skip_blank_lines 参数为 True 且 names 参数不为 None 时，这个参数会忽略注释行和空行，会从<strong>数据的第一行</strong>而不是文件的第一行开始计算行数并读取表头</li>
</ul></li>
<li><p>names</p>
<p>array-like ，可选</p>
<p>以传入的参数作为表头</p>
<ul>
<li>参数中不允许有重复的内容</li>
<li>当 header 参数为 int 或 int 列表且 names 参数不为 None 时，将从 header 参数对应的行号读取列名，然后根据 names 参数对表头进行替换</li>
</ul></li>
<li><p>dtype</p>
<p>Type name 或 column → type 字典，可选，默认为 None</p>
<p>将数据以参数数据类型读取</p></li>
<li><p>skiprows</p>
<p>list-like 或 int 或 可调用对象 ，可选，默认为 None</p>
<ul>
<li><p>当 skiprows 的参数为 int 或 列表 时，将跳过对应行号的数据，对余下数据进行读取</p></li>
<li><p>当 skiprows 的参数为 可调用对象（如函数） 时，函数会先遍历行索引，进行条件判断，然后跳过函数返回值为 True 的行号的数据，对余下数据进行读取</p></li>
</ul></li>
<li><p>index_col</p>
<p>int 或 str 或 int / str 序列，可选，默认为 None</p>
<ul>
<li><p>当 index_col 的参数为 int 或 str 时，将对应列号/列名用作数据的索引</p></li>
<li><p>当 index_col 的参数为 int / str 序列时，将使用 MultiIndex，将对应列号/列名用作数据的索引</p></li>
</ul></li>
</ol>
<h5 id="参考网页-1">参考网页</h5>
<p>pandas - <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html">pandas.read_csv</a></p>
<p>用 Python 学机器学习 - <a href="https://baijiahao.baidu.com/s?id=1696564586951432790&amp;wfr=spider&amp;for=pc">一文掌握 read_csv 函数</a></p>
<h4 id="pandas.dataframe.to_csv-函数"><code>pandas.DataFrame.to_csv()</code> 函数</h4>
<p>将 DataFrame 写入 CSV 文件中</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">DataFrame.to_csv(path_or_buf=<span class="literal">None</span>, sep=<span class="string">','</span>, na_rep=<span class="string">''</span>,</span><br><span class="line">float_format=<span class="literal">None</span>, columns=<span class="literal">None</span>, header=<span class="literal">True</span>, index=<span class="literal">True</span>,</span><br><span class="line">index_label=<span class="literal">None</span>, mode=<span class="string">'w'</span>, encoding=<span class="literal">None</span>, compression=<span class="string">'infer'</span>,</span><br><span class="line">quoting=<span class="literal">None</span>, quotechar=<span class="string">'"'</span>, line_terminator=<span class="literal">None</span>, chunksize=<span class="literal">None</span>,</span><br><span class="line">date_format=<span class="literal">None</span>, doublequote=<span class="literal">True</span>, escapechar=<span class="literal">None</span>, decimal=<span class="string">'.'</span>,</span><br><span class="line">errors=<span class="string">'strict'</span>, storage_options=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="部分重要参数">部分重要参数</h5>
<ol type="1">
<li><p>path_or_buf</p>
<p>str 或 path object 或 file-like object 或 None ，默认为 None</p>
<ul>
<li>当 path_or_buf 的参数不为 None 时，将把 DataFrame 写入对应路径/文件名中</li>
<li>当 path_or_buf 的参数为 None 时，将把 DataFrame 以字符串形式打印出来 如果传递的是一个非二进制文件对象，应该用newline=''打开，禁用通用换行。 如果传递的是二进制文件对象，模式可能需要包含一个'b'。</li>
</ul></li>
<li><p>sep</p>
<p>str ，默认为 ‘,’</p>
<p>用于输出文件的字段分隔符。</p>
<p>sep 的参数要求长度为1的字符串</p></li>
<li><p>na_rep</p>
<p>str ，默认为 ‘ ’</p>
<p>用于转换 DataFrame 中的 NaN</p></li>
<li><p>header</p>
<p>bool or str 列表，默认为 True</p>
<ul>
<li><p>当 header 的参数为 bool 时，将根据 header 的参数决定是否将数据的表头写入文件中</p></li>
<li><p>当 header 的参数为 str 列表 时，将把 header 的参数作为表头写入文件中。</p></li>
</ul></li>
<li><p>index</p>
<p>bool ，默认为 True</p>
<p>用于决定是否将索引写入文件中</p></li>
<li><p>mode</p>
<p>str ，默认为 'w'</p>
<p>用于设置 Python 的写入模式</p></li>
</ol>
<h5 id="参考网页-2">参考网页</h5>
<p>pandas - <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html?highlight=to_csv">pandas.DataFrame.to_csv</a></p>
<p>AI阿聪 - <a href="https://blog.csdn.net/weixin_40431584/article/details/105065464">pd.read_csv() 和 pd.to_csv() 常用参数</a></p>
<p>quantLearner - <a href="https://blog.csdn.net/The_Time_Runner/article/details/88353161">pd.read_csv()||pd.to_csv() 索引问题 index</a></p>
<p>暴躁的猴子 - <a href="https://blog.csdn.net/orangefly0214/article/details/80764569">pd.to_csv详解</a></p>
<h2 id="模型训练">模型训练</h2>
<p>按照<a href="https://www.bilibili.com/video/BV1Y7411d7Ys">课程</a>将代码分为四部分</p>
<p>在此之前导入库</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br></pre></td></tr></tbody></table></figure>
<h3 id="prepare-dataset">Prepare dataset</h3>
<p>构建 Dataset ，完成 Dataloader</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        xy = pd.read_csv(filepath, sep=<span class="string">","</span>, dtype=<span class="string">"float32"</span>)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.tensor(xy.iloc[:, <span class="number">1</span>:].values)</span><br><span class="line">        self.y_data = torch.tensor(xy.iloc[:, <span class="number">0</span>].values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = TitanicDataset(<span class="string">"./dataset/titanic/processed_train.csv"</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="design-model-using-class">Design model using Class</h3>
<p>从 torch.nn.Module 继承构建模型</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.Linear1 = nn.Linear(<span class="number">7</span>, <span class="number">22</span>)</span><br><span class="line">        self.Linear2 = nn.Linear(<span class="number">22</span>, <span class="number">11</span>)</span><br><span class="line">        self.Linear3 = nn.Linear(<span class="number">11</span>, <span class="number">1</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu(self.Linear1(x))</span><br><span class="line">        x = self.relu(self.Linear2(x))</span><br><span class="line">        x = self.sigmoid(self.Linear3(x))</span><br><span class="line">        x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="construct-loss-and-optimizer">Construct loss and optimizer</h3>
<p>使用 PyTorch API 指定 Loss 函数和优化器</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">criterion = nn.BCELoss(reduction=<span class="string">"mean"</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="training-cycle">Training cycle</h3>
<p>完成前向传播，反向传播，更新权值</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">acc_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line"></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        inputs, labels = data                   <span class="comment"># 载入数据</span></span><br><span class="line"></span><br><span class="line">        label_pred = model(inputs)              <span class="comment"># 前向传播，计算梯度</span></span><br><span class="line">        loss = criterion(label_pred, labels)    <span class="comment"># 计算 Loss</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(label_pred)):        <span class="comment"># 计算 acc</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">round</span>(label_pred[i - <span class="number">1</span>].item()) == labels[i - <span class="number">1</span>].item():</span><br><span class="line">                acc += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()                   <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward()                         <span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line">        optimizer.step()                        <span class="comment"># 更新权值</span></span><br><span class="line"></span><br><span class="line">    acc /= <span class="built_in">len</span>(dataset)</span><br><span class="line">    acc_list.append(acc)                        <span class="comment"># 存储 acc</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"epoch:"</span>, epoch, <span class="string">" acc:"</span>, acc)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()                        <span class="comment"># 绘制 acc 变化曲线</span></span><br><span class="line">ax.plot(np.arange(<span class="number">300</span>), acc_list)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="回顾-1">回顾</h3>
<h4 id="继承-dataset-类初始化报错">继承 Dataset 类初始化报错</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">TypeError: <span class="built_in">object</span>.__new__() takes exactly one argument (the <span class="built_in">type</span> to instantiate)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="错误出现原因-1">错误出现原因</h5>
<p>发现是将子类的初始化函数名称写成了 <code>_init_</code> 而非 <code>__init__</code></p>
<p>子类继承时未重构初始化函数，在创建实例时不能进行初始化</p>
<h5 id="解决方法">解决方法</h5>
<p>修改为 <code>__init__</code> 后错误信息消失</p>
<h5 id="参考网页-3">参考网页</h5>
<p>摩天仑 - <a href="https://blog.csdn.net/weixin_57064740/article/details/121589595">Python 学习坑 —— init</a></p>
<h4 id="读取数据时将索引作为数据读入">读取数据时将索引作为数据读入</h4>
<p>使用 <code>pandas.read_csv()</code> 函数读取数据时，将索引作为数据读入数据中的第一列</p>
<h5 id="错误出现原因-2">错误出现原因</h5>
<p>在将处理后的数据写入 CSV 文件时将索引也一并写入，但从 CSV 文件中读取数据时未声明数据中包含有索引</p>
<h5 id="解决方法1-1">解决方法1</h5>
<p>将 DataFrame 数据写入 CSV 文件时使用 <code>pandas.DataFrame.to_csv(PATH, index=False)</code></p>
<h5 id="解决方法2-1">解决方法2</h5>
<p>从 CSV 文件读取数据时使用 <code>pandas.read_csv(PATH, index_col=0)</code></p>
<h5 id="参考网页-4">参考网页</h5>
<p>hellocsz - <a href="https://blog.csdn.net/hellocsz/article/details/79623142">read_csv 文件读写参数详解</a></p>
<h4 id="从-dataframe-中提取数据报错">从 DataFrame 中提取数据报错</h4>
<p>代码</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">self.x_data = torch.from_numpy(xy[:, <span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">self.y_data = torch.from_numpy(xy[:, [<span class="number">0</span>]])</span><br></pre></td></tr></tbody></table></figure>
<p>错误信息</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">TypeError: <span class="string">'(slice(None, None, None), slice(1, -1, None))'</span> <span class="keyword">is</span> an invalid key</span><br></pre></td></tr></tbody></table></figure>
<h5 id="错误出现原因-3">错误出现原因</h5>
<p>DataFrame 的切片操作仅支持 <code>.loc</code> 和 <code>.iloc</code> 函数</p>
<h5 id="解决方法-1">解决方法</h5>
<p>使用 <code>pandas.DataFrame.iloc()</code> 函数</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">self.x_data = torch.tensor(xy.iloc[:, <span class="number">1</span>:])</span><br><span class="line">self.y_data = torch.tensor(xy.iloc[:, <span class="number">0</span>])</span><br></pre></td></tr></tbody></table></figure>
<p><code>pandas.DataFrame.iloc()</code> 中的范围是<strong>左开右闭</strong>的</p>
<h5 id="参考网页-5">参考网页</h5>
<p>pandas - <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html">pandas.DataFrame.iloc</a></p>
<p>方如一 - <a href="https://blog.csdn.net/Fwuyi/article/details/123127754">iloc[ ]函数（Pandas库）</a></p>
<h4 id="从-dataset-读取数据时报错">从 dataset 读取数据时报错</h4>
<p>代码</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">dataset = TitanicDataset(<span class="string">"./dataset/titanic/processed_train.csv"</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>错误信息</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">RuntimeError: DataLoader worker (pid(s) <span class="number">32708</span>, <span class="number">31720</span>) exited unexpectedly</span><br></pre></td></tr></tbody></table></figure>
<h5 id="错误出现原因-4">错误出现原因</h5>
<p>未知，据推测是多线程工作的支持不好，或者是多线程被某些特定程序杀掉了</p>
<h5 id="解决方法-2">解决方法</h5>
<p>将 num_workers 参数设置为 0 （默认为 0 ）</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="参考网页-6">参考网页</h5>
<p>xiuxiuxiuxiul - <a href="https://blog.csdn.net/xiuxiuxiuxiul/article/details/86233500">Pytorch设置多线程进行dataloader时影响GPU运行</a></p>
<h4 id="计算-loss-时报错">计算 Loss 时报错</h4>
<p>模型代码</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu(self.Linear1(x))</span><br><span class="line">        x = self.relu(self.Linear2(x))</span><br><span class="line">        x = self.sigmoid(self.Linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<p>计算 Loss 代码</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        label_pred = model(inputs)</span><br><span class="line">        loss = criterion(label_pred, labels)</span><br><span class="line">        <span class="comment"># ......</span></span><br></pre></td></tr></tbody></table></figure>
<p>错误信息</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">ValueError: Using a target size (torch.Size([<span class="number">32</span>])) that <span class="keyword">is</span> different to the <span class="built_in">input</span> size (torch.Size([<span class="number">32</span>, <span class="number">1</span>])) <span class="keyword">is</span> deprecated. Please ensure they have the same size.</span><br></pre></td></tr></tbody></table></figure>
<h5 id="错误出现原因-5">错误出现原因</h5>
<p>模型输出结果 <code>label_pred</code> 为 [32, 1] ，而 <code>labels</code> 为 [32] ，二者的维度不匹配</p>
<h5 id="解决方法-3">解决方法</h5>
<p>在前馈计算中加入 <code>pandas.DataFrame.squeeze(-1)</code> 将输出结果的最后一维压缩</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu(self.Linear1(x))</span><br><span class="line">        x = self.relu(self.Linear2(x))</span><br><span class="line">        x = self.sigmoid(self.Linear3(x))</span><br><span class="line">+       x = x.squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h5 id="参考网页-7">参考网页</h5>
<p>发梦公主 - <a href="https://blog.csdn.net/lyh2240465046/article/details/123695335">深度学习过程的Python报错收集</a></p>
<p>pandas - <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.squeeze.html">pandas.DataFrame.squeeze</a></p>
<h3 id="实验结果">实验结果</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># ......</span></span><br><span class="line">epoch: <span class="number">290</span>  acc: <span class="number">0.6767676767676768</span></span><br><span class="line">epoch: <span class="number">291</span>  acc: <span class="number">0.712682379349046</span></span><br><span class="line">epoch: <span class="number">292</span>  acc: <span class="number">0.7485970819304153</span></span><br><span class="line">epoch: <span class="number">293</span>  acc: <span class="number">0.7542087542087542</span></span><br><span class="line">epoch: <span class="number">294</span>  acc: <span class="number">0.7845117845117845</span></span><br><span class="line">epoch: <span class="number">295</span>  acc: <span class="number">0.6767676767676768</span></span><br><span class="line">epoch: <span class="number">296</span>  acc: <span class="number">0.7485970819304153</span></span><br><span class="line">epoch: <span class="number">297</span>  acc: <span class="number">0.8204264870931538</span></span><br><span class="line">epoch: <span class="number">298</span>  acc: <span class="number">0.8204264870931538</span></span><br><span class="line">epoch: <span class="number">299</span>  acc: <span class="number">0.7901234567901234</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="结果评价">结果评价</h4>
<figure>
<img data-src="https://s2.loli.net/2022/08/28/XserVa7zuvLwYUQ.png" alt=""><figcaption>image.png</figcaption>
</figure>
<p>可以看到训练存在一定的效果，但并不明显，抖动明显</p>
<h4 id="存在的问题">存在的问题</h4>
<ol type="1">
<li><p>由于 loss 设置问题，不能直观显示 loss 随训练轮数变化</p></li>
<li><p>设计模型设计不够完善</p></li>
<li><p>本次实验中选取的特征只针对训练集，没有考虑测试集中数据的情况</p>
<p>在查看测试集中的数据后发现其中的 Fare 和 Age 数据缺失</p>
<p>可以将训练集和测试集中存在缺失数据的特征都去除</p>
<p>可以用训练集中的均值、众数填补测试集中的缺失项，但是这样将影响测试集，导致分类结果产生偏移</p></li>
<li><p>数据集中特征较少</p></li>
</ol>
<h4 id="后续改进">后续改进</h4>
<p>使用 K 折交叉校验法</p>
<p>重新设计训练模块</p>
<h3 id="参考网页-8">参考网页</h3>
<p>刘二大人 - <a href="https://www.bilibili.com/video/BV1Y7411d7Ys?p=8">《 PyTorch 深度学习实践》完结合集</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 实现基于 CNN 的图像分类总结</title>
    <url>/posts/605ef827.html</url>
    <content><![CDATA[<h2 id="数据集">数据集</h2>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">"./dataset/mnist/"</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">"./dataset/mnist/"</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset, shuffle=<span class="literal">False</span>, atch_size=batch_size)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="torchvision.datasets">torchvision.datasets</h3>
<p>torchvision.datasets 中包含了 <code>MNIST</code> 数据集</p>
<p>通过 torchvision.datasets.MNIST 提供的 API 可以下载、转换、加载数据集</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">datasets.MNIST(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="参数解析">参数解析</h4>
<ol type="1">
<li><p>root</p>
<p>str ，数据集路径对象</p>
<p>用于储存 <code>processed/training.pt</code> 和 <code>processed/test.pt</code></p></li>
<li><p>train</p>
<p>bool ，可选</p>
<ul>
<li>当 train 参数为 True 时，载入的数据集为训练集</li>
<li>当 train 参数为 False 时，载入的数据集为测试集</li>
</ul></li>
<li><p>download</p>
<p>bool ，可选</p>
<ul>
<li><p>当 download 参数为 True 且未下载过该数据时，将从 <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a> 下载数据集并将其放在 root 参数指定路径下</p></li>
<li><p>如果 download 参数为 True 且下载过该数据，则不会再次下载</p></li>
</ul></li>
<li><p>transform</p>
<p>可调用对象，可选</p>
<p>接收 PIL 映像并返回转换版本的函数/变换。例如:transforms.RandomCrop</p></li>
<li><p>target_transform</p>
<p>可调用对象，可选</p>
<p>一个接收目标并转换它的函数/变换。</p>
<p>transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional): A function/transform that takes in the target and transforms it.</p></li>
</ol>
<h3 id="torchvision.transforms">torchvision.transforms</h3>
<p>torchvision.transforms 可以实现对图像的变化</p>
<h4 id="torchvision.transforms.composetransforms">torchvision.transforms.Compose(transforms)</h4>
<p>Compose 函数可以实现不同变化的组合</p>
<h4 id="torchvision.transforms.totensor">torchvision.transforms.ToTensor()</h4>
<p>ToTensor 函数可以将图像的多通道矩阵转换为张量形式，方便后续计算</p>
<h4 id="torchvision.transforms.nomalizemean-std-inplace">torchvision.transforms.Nomalize(mean, std[, inplace])</h4>
<p>Normalize 函数可以用均值和标准差对张量图像进行归一化</p>
<h5 id="mnist-数据集的均值和标准差">MNIST 数据集的均值和标准差</h5>
<p>MNIST 数据集的均值为 0.1307 ， 标准差为 0.3081</p>
<h2 id="网络设计">网络设计</h2>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Flatten data from (n, 1, 28, 28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># flatten</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="torch.nn.conv2d-函数"><code>torch.nn.Conv2d()</code> 函数</h3>
<p>Conv2d 函数可以实现卷积层</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">'zeros'</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="部分参数解析">部分参数解析</h4>
<ol type="1">
<li><p>in_channels</p>
<p>int ，必选</p>
<p>输入的通道数</p></li>
<li><p>out_channels</p>
<p>int ，必选</p>
<p>输出的通道数</p></li>
<li><p>kernel_size</p>
<p>int 或 tuple ，必选</p>
<p>卷积核的尺寸</p></li>
<li><p>stride</p>
<p>int 或 tuple ，可选，默认为 1</p>
<p>卷积时的步长</p></li>
<li><p>padding</p>
<p>int 或 tuple ，可选，默认为 0</p>
<p>填充边缘的宽度</p></li>
<li><p>dilation</p>
<p>int 或 tuple ，可选，默认为 1</p>
<p>卷积核中各元素的间距</p></li>
<li><p>padding_mode</p>
<p>str ，可选，默认为 'zeros'</p>
<p>选择填充模式</p>
<p>可选 'zeros' ， 'reflect' ， 'replicate' ， 'circular'</p></li>
</ol>
<h4 id="参考网页">参考网页</h4>
<p>Pytorch - <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html">CONV2D</a></p>
<p>夏普通 - <a href="https://blog.csdn.net/qq_34243930/article/details/107231539">pytorch之torch.nn.Conv2d()函数详解</a></p>
<h3 id="torch.nn.maxpool2d-函数"><code>torch.nn.MaxPool2d()</code> 函数</h3>
<p>MaxPool2d 函数可以实现最大池化层</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="部分参数解析-1">部分参数解析</h4>
<ol type="1">
<li><p>kernel_size</p>
<p>int 或 tuple ，必选</p>
<p>进行最大值池化的窗口大小</p></li>
<li><p>stride</p>
<p>int 或 tuple ，可选，默认与 kernel_size 参数相同</p>
<p>最大值池化窗口的步长</p></li>
<li><p>padding</p>
<p>int 或 tuple ，可选，默认为 0</p>
<p>填充边缘的宽度</p></li>
</ol>
<h4 id="参考网页-1">参考网页</h4>
<p>PyTorch - <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">MAXPOOL2D</a></p>
<h3 id="torch.nn.linear-函数"><code>torch.nn.Linear()</code> 函数</h3>
<p>Linear 函数可以实现全连接层，其本质上是线性层</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">torch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="部分参数解析-2">部分参数解析</h4>
<ol type="1">
<li><p>in_features</p>
<p>输入数据的维度</p></li>
<li><p>out_features</p>
<p>输出数据的维度</p></li>
<li><p>bias</p>
<p>bool ，默认为 True</p>
<p>决定是否开启加法偏置</p></li>
</ol>
<h4 id="参考网页-2">参考网页</h4>
<p>PyTorch - <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear">LINEAR</a></p>
<h3 id="pytorch-使用-gpu-训练">PyTorch 使用 GPU 训练</h3>
<p>PyTorch 使用 GPU 训练时先需要</p>
<ul>
<li><p>单 GPU 训练时</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>多 GPU 训练时</p>
<p>需要根据所需使用的 GPU ，在文件头部添加</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"1, 2, 3"</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>使得当前代码仅对指定 GPU 可见，系统将会对指定 GPU 从零开始重新编号</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>训练过程中</p>
<p>需要将模型和张量计算都搬运到 GPU 上</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">model.to(device)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">inputs, targets = inputs.to(device), targets.to(device)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ul>
<h4 id="参考网页-3">参考网页</h4>
<p>hello_dear_you - <a href="https://blog.csdn.net/hello_dear_you/article/details/120190567">pytorch 之多 GPU 训练</a></p>
<h2 id="设计-loss-和优化器">设计 Loss 和优化器</h2>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>技术</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 搭建 AlexNet 模型对 CIFAR10 数据集分类总结</title>
    <url>/posts/3cbaf5b4.html</url>
    <content><![CDATA[<h2 id="alexnet-网络模型搭建">AlexNet 网络模型搭建</h2>
<p>根据 <a href="https://dl.acm.org/doi/abs/10.1145/3065386">ImageNet Classification with Deep Convolutional Neural Networks</a> 论文中的文字描述和图标信息，可以得到 AlexNet 模型（忽略 batch_size ）如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">padding</th>
<th style="text-align: center;">stride</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Input</td>
<td style="text-align: center;">224*224</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">55*55</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Maxpool1</td>
<td style="text-align: center;">27*27</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2</td>
<td style="text-align: center;">27*27</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Maxpool2</td>
<td style="text-align: center;">13*13</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv3</td>
<td style="text-align: center;">13*13</td>
<td style="text-align: center;">384</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv4</td>
<td style="text-align: center;">13*13</td>
<td style="text-align: center;">384</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv5</td>
<td style="text-align: center;">13*13</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Maxpool3</td>
<td style="text-align: center;">6*6</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">FC1</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="odd">
<td style="text-align: center;">FC2</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">FC3</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<p>据此可搭建网络模型</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_Paper</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channel=<span class="number">3</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet_Paper, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=input_channel, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv5 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">6</span> * <span class="number">6</span> * <span class="number">256</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc3 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h3 id="各层输出-size-计算">各层输出 size 计算</h3>
<ol type="1">
<li><p>Conv 层的输出 size 计算公式</p>
<p>设输入 size 为 i * i ，卷积核 size 为 k * k ，步长为 s ，边缘填充为 p ，输出 size 为 o * o</p>
<p><span class="math display">\[o = \frac{i-k+2*p}{s}+1\]</span></p></li>
<li><p>Maxpool 层的 size 计算公式</p>
<p>设输入 size 为 i * i ，池化核 size 为 k * k ，步长为 s ，输出 size 为 o * o</p>
<p><span class="math display">\[o = \frac{i-k}{s}+1\]</span></p></li>
</ol>
<h4 id="alexnet-输入-size-问题">AlexNet 输入 size 问题</h4>
<p>AlexNet 输入的图像 size 在论文中多处均描述为 224*224 ，Maxpool1 层输入的 size 在论文 Figure2 中描述为 55*55</p>
<figure>
<img data-src="https://s2.loli.net/2022/09/13/9AwGtkcOUJd8X2p.png" alt=""><figcaption>image.png</figcaption>
</figure>
<blockquote>
<p>The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map).</p>
</blockquote>
<p>当将论文中的数据带入计算后发现经过 Conv1 层输出的 size 与 Maxpool1 层输入的 size 有冲突</p>
<p><span class="math display">\[o = \frac{i-k+2*p}{s} +1 = \frac{224-11}{4} +1 = 54.25 \approx 54\]</span></p>
<p>即 Conv1 层输出的 size为 54*54 ，而 Maxpool1 层输入的 size 为 55*55</p>
<p>若 AlexNet 输入的图像 size 改为 227*227 ，则可得到 Maxpool1 层所需要的输入 size</p>
<p><span class="math display">\[o = \frac{i-k+2*p}{s} +1 = \frac{227-11}{4} +1 = 55\]</span></p>
<p>然后在 Pytorch 的 model 库中发现 Conv1 层还应用了 padding</p>
<p><span class="math display">\[o = \frac{i-k+2*p}{s} +1 = \frac{224-11+2*2}{4} +1 = 55.25 \approx 55\]</span></p>
<p>最终搭建模型时输入 size 采用 224*224</p>
<h4 id="参考网页">参考网页</h4>
<p>一抹青竹 - <a href="https://www.bilibili.com/read/cv7181322/">较真AlexNet：到底是224还是227？</a></p>
<p>ZJE_ANDY - <a href="https://blog.csdn.net/u014453898/article/details/85126733">图像卷积和池化操作后的特征图大小计算方法</a></p>
<p>GitHub - <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py">torchvision/models/AlexNet.py</a></p>
<h3 id="flatten-层">Flatten 层</h3>
<p>Conv5 层输出的 size 为 6*6*256 ，而 FC1 层所需输入要求是一维的，因此需要使用 Flatten 将三维数据压缩成一维</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">nn.Flatten()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="针对-cifar10-数据集的修改">针对 CIFAR10 数据集的修改</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet_CIFAR10</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet_CIFAR10, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        self.conv5 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">4</span> * <span class="number">4</span> * <span class="number">256</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">        )</span><br><span class="line">        self.fc3 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h2 id="模型训练">模型训练</h2>
<h3 id="准备数据集">准备数据集</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">ROOT_DIR = os.getcwd()</span><br><span class="line">DATASET_DIR = os.path.join(ROOT_DIR, <span class="string">"dataset"</span>, <span class="string">"CIFAR10"</span>)</span><br><span class="line"></span><br><span class="line">transform = T.Compose(</span><br><span class="line">  [</span><br><span class="line">      T.ToTensor(),</span><br><span class="line">      T.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.CIFAR10(root=DATASET_DIR, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.CIFAR10(root=DATASET_DIR, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(train_dataset))</span><br><span class="line">eval_size = <span class="built_in">len</span>(train_dataset) - train_size</span><br><span class="line">test_size = <span class="built_in">len</span>(test_dataset)</span><br><span class="line"></span><br><span class="line">train_dataset, eval_dataset = random_split(train_dataset, [train_size, eval_size])</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="计算数据集均值和方差">计算数据集均值和方差</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_CIFAR10_mean_std</span>(<span class="params">dataset_dir</span>):</span><br><span class="line">    train_dataset = datasets.CIFAR10(root=dataset_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=T.ToTensor())</span><br><span class="line">    train_loader = DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    channels_sum, channels_squared_sum, num_batches = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> data, _ <span class="keyword">in</span> train_loader:  <span class="comment"># 批量*通道*高*宽</span></span><br><span class="line">        channels_sum += torch.mean(data, dim=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 剩下通道这个维度</span></span><br><span class="line">        channels_squared_sum += torch.mean(data**<span class="number">2</span>, dim=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">        num_batches += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    mean = channels_sum / num_batches</span><br><span class="line">    std = (channels_squared_sum / num_batches - mean**<span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mean, std</span><br></pre></td></tr></tbody></table></figure>
<p>通过自定义函数来计算数据集均值和方差，可以自适应的对数据集进行归一化</p>
<h4 id="划分验证集">划分验证集</h4>
<p>在本次实验中为了观察训练过程中的模型训练效果，从训练集中划分出了验证集</p>
<p><code>random_split()</code> 函数可以将输入的 Dataset 划分为输入 list 中各元素长度的子 Dataset</p>
<h3 id="网络模型实例化">网络模型实例化</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">model = AlexNet_CIFAR10()</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="设计-loss-和优化器">设计 Loss 和优化器</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="训练过程">训练过程</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_loss, train_acc, eval_loss, eval_acc = [], [], [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(args.num_epochs)):</span><br><span class="line"></span><br><span class="line">    training_epoch_loss, training_epoch_acc = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">    training_temp_loss, training_temp_correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, start=<span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        inputs, targets = data</span><br><span class="line">        <span class="keyword">if</span> args.gpu:</span><br><span class="line">            inputs, targets = inputs.to(device), targets.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        training_temp_loss += loss.item()</span><br><span class="line">        predicted = torch.argmax(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">        training_temp_correct += (predicted == targets).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    training_epoch_loss = training_temp_loss / batch_idx</span><br><span class="line">    training_epoch_acc = <span class="number">100</span> * training_temp_correct / train_size</span><br><span class="line"></span><br><span class="line">    train_loss.append(training_epoch_loss)</span><br><span class="line">    train_acc.append(training_epoch_acc)</span><br><span class="line"></span><br><span class="line">    evaling_epoch_loss, evaling_epoch_acc = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">    evaling_temp_loss, evaling_temp_correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(eval_loader, start=<span class="number">0</span>):</span><br><span class="line">            inputs, targets = data</span><br><span class="line">            <span class="keyword">if</span> args.gpu:</span><br><span class="line">                inputs, targets = inputs.to(device), targets.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">            evaling_temp_loss += loss.item()</span><br><span class="line">            predicted = torch.argmax(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            evaling_temp_correct += (predicted == targets).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    evaling_epoch_loss = evaling_temp_loss / batch_idx</span><br><span class="line">    evaling_epoch_acc = <span class="number">100</span> * evaling_temp_correct / eval_size</span><br><span class="line"></span><br><span class="line">    eval_loss.append(evaling_epoch_loss)</span><br><span class="line">    eval_acc.append(evaling_epoch_acc)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">"[Epoch {:3d}] train_loss: {:.3f} train_acc: {:.3f}% eval_loss: {:.3f} eval_acc: {:.3f}%"</span>.<span class="built_in">format</span>(</span><br><span class="line">            epoch + <span class="number">1</span>, training_epoch_loss, training_epoch_acc, evaling_epoch_loss, evaling_epoch_acc</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Training process has finished. Saving trained model."</span>)</span><br><span class="line">SAVE_DIR = <span class="string">"./AlexNet_CIFAR10.pth"</span></span><br><span class="line">torch.save(model.state_dict(), SAVE_DIR)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="验证过程">验证过程</h4>
<ol type="1">
<li><p>模型模式切换</p>
<p>在对验证集进行验证时，需要将网络模型 Dropout 层禁用，这时候就需要如下代码将模型切换到分析模式</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>而再次进行训练时，需要启用 Dropout 层，这时需要如下代码将模型切换到训练模式</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">model.train()</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>停止计算图的构建</p>
<p>在验证过程和测试过程中，由于只需要网络模型的结果，而不需要生成计算图，可以使用 <code>with torch.no_grad()</code> 节约运算资源</p></li>
</ol>
<h4 id="参考网页-1">参考网页</h4>
<p>未来达摩大师 - <a href="https://zhuanlan.zhihu.com/p/494060986">【PyTorch】搞定网络训练中的model.train()和model.eval()模式</a></p>
<p>失之毫厘，差之千里 - <a href="https://blog.csdn.net/qq_42251157/article/details/124101436">with torch.no_grad() 详解</a></p>
<h3 id="保存模型">保存模型</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Training process has finished. Saving trained model."</span>)</span><br><span class="line">SAVE_DIR = <span class="string">"./AlexNet_CIFAR10.pth"</span></span><br><span class="line">torch.save(model.state_dict(), SAVE_DIR)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="测试过程">测试过程</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"------Starting testing------"</span>)</span><br><span class="line">testing_temp_loss, testing_temp_correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(test_loader, start=<span class="number">0</span>)):</span><br><span class="line">      inputs, targets = data</span><br><span class="line">      <span class="keyword">if</span> args.gpu:</span><br><span class="line">          inputs, targets = inputs.to(device), targets.to(device)</span><br><span class="line"></span><br><span class="line">      outputs = model(inputs)</span><br><span class="line">      loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">      testing_temp_loss += loss.item()</span><br><span class="line">      predicted = torch.argmax(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">      testing_temp_correct += (predicted == targets).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">testing_loss = testing_temp_loss / batch_idx</span><br><span class="line">testing_acc = <span class="number">100</span> * testing_temp_correct / test_size</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"[Test     ] loss: {:.3f} acc: {:.3f}%%"</span>.<span class="built_in">format</span>(testing_loss, testing_acc))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="绘制图表">绘制图表</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(np.arange(args.num_epochs), train_loss, np.arange(args.num_epochs), eval_loss)</span><br><span class="line">ax.set_xlabel(<span class="string">"Epoch"</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">ax.legend([<span class="string">"train_loss"</span>, <span class="string">"eval_loss"</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.plot(np.arange(args.num_epochs), train_acc, np.arange(args.num_epochs), eval_acc)</span><br><span class="line">ax.set_xlabel(<span class="string">"Epoch"</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">"acc"</span>)</span><br><span class="line">ax.legend([<span class="string">"train_acc"</span>, <span class="string">"eval_acc"</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="softmax">softmax</h3>
<p>在构建网络的时候，由于没有考虑到交叉熵损失函数中已经包含了 Softmax 函数，而在模型的末尾加入了 Softmax 层，导致网络模型不能正常收敛</p>
<figure>
<img data-src="https://s2.loli.net/2022/09/14/uCjMp1lHcwJ5PsB.png" alt=""><figcaption>Softmax acc-Epoch</figcaption>
</figure>
<figure>
<img data-src="https://s2.loli.net/2022/09/14/on4mcODhAWF8Ygu.png" alt=""><figcaption>Softmax Loss-Epoch</figcaption>
</figure>
<blockquote>
<p><code>Torch.nn.CrossEntropyLoss()</code> 函数是先将输出结果输入到 Softmax 层后取对数，再应用 NLLLoss 即 <code>Torch.nn.CrossEntropyLoss()</code> = LogSoftmax + NLLLoss</p>
</blockquote>
<p>将模型最后的 Softmax 层去掉网络就可正常收敛了</p>
<p>在此处把警钟敲烂，要和 GPU 一起检查</p>
<h3 id="参考网页-2">参考网页</h3>
<p>爱学英语的程序媛 - <a href="https://www.cnblogs.com/Bella2017/p/11791216.html">Pytorch划分数据集的方法</a></p>
<p>Wabi―sabi - <a href="https://blog.csdn.net/qq_53640005/article/details/115434295">AlexNet网络对CIFAR10分类——torch实现</a></p>
<p>故你， - <a href="https://blog.csdn.net/Myshrry/article/details/123853892">[pytorch] 利用Alexnet训练cifar10</a></p>
<h2 id="argparse-库">argparse 库</h2>
<p>argparse 可以给 Python 脚本传入参数</p>
<p>使用时需要先导入库</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br></pre></td></tr></tbody></table></figure>
<p>然后实例化参数容器</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br></pre></td></tr></tbody></table></figure>
<p>最后逐一添加所需参数（及其默认值）</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">parser.add_argument(<span class="string">"--gpu"</span>, action=<span class="string">"store_true"</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">"use gpu mode"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--batch_size"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">32</span>, <span class="built_in">help</span>=<span class="string">"batch size in training"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--num_epochs"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>, <span class="built_in">help</span>=<span class="string">"epochs in training"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>后续代码只需要调用 parser 的对应成员即可</p>
<h3 id="参考网页-3">参考网页</h3>
<p>Fan19zju - <a href="https://blog.csdn.net/Fan19zju/article/details/118570720">argparse库教程（超易懂）</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 实现多分类任务总结</title>
    <url>/posts/7efb9c78.html</url>
    <content><![CDATA[<h2 id="数据集处理">数据集处理</h2>
<h3 id="实验记录">实验记录</h3>
<p>在 Kaggle 网站上下载 <a href="https://www.kaggle.com/competitions/otto-group-product-classification-challenge/data">Otto Group Product Classification Challenge 数据集</a>，解压在 <code>./dataset/otto-group-product-classification-challenge</code> 下</p>
<ol type="1">
<li><p>导入数据集并查看基本信息</p>
<p>导入库</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>设置 dataset 地址</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">TRAIN_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/train.csv"</span></span><br><span class="line">TEST_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/test.csv"</span></span><br><span class="line">SAMPLE_SUBMISSION_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/sampleSubmission.csv"</span></span><br><span class="line">PROCESSED_TRAIN_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/processed_train.csv"</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>读取 training dataset</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data = pd.read_csv(TRAIN_PATH, index_col=<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>显示 training dataset 信息</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">Output exceeds the size limit. Open the full output data <span class="keyword">in</span> a text editor</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">Int64Index: <span class="number">61878</span> entries, <span class="number">1</span> to <span class="number">61878</span></span><br><span class="line">Data columns (total <span class="number">94</span> columns):</span><br><span class="line"> <span class="comment">#   Column   Non-Null Count  Dtype</span></span><br><span class="line">---  ------   --------------  -----</span><br><span class="line"> <span class="number">0</span>   feat_1   <span class="number">61878</span> non-null  int64</span><br><span class="line"> <span class="number">1</span>   feat_2   <span class="number">61878</span> non-null  int64</span><br><span class="line"> <span class="number">2</span>   feat_3   <span class="number">61878</span> non-null  int64</span><br><span class="line">...</span><br><span class="line"> <span class="number">92</span>  feat_93  <span class="number">61878</span> non-null  int64</span><br><span class="line"> <span class="number">93</span>  target   <span class="number">61878</span> non-null  <span class="built_in">object</span></span><br><span class="line">dtypes: int64(<span class="number">93</span>), <span class="built_in">object</span>(<span class="number">1</span>)</span><br><span class="line">memory usage: <span class="number">44.8</span>+ MB</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>查看 training dataset 前几行</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></tbody></table></figure><p></p>
<figure>
<img data-src="https://s2.loli.net/2022/09/08/3YcZkMEpTrs5eQI.png" alt=""><figcaption>training dataset(before)</figcaption>
</figure>
<p>查找是否存在缺失值</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">feat_1     <span class="number">0</span></span><br><span class="line">feat_2     <span class="number">0</span></span><br><span class="line">feat_3     <span class="number">0</span></span><br><span class="line">feat_4     <span class="number">0</span></span><br><span class="line">feat_5     <span class="number">0</span></span><br><span class="line">          ..</span><br><span class="line">feat_90    <span class="number">0</span></span><br><span class="line">feat_91    <span class="number">0</span></span><br><span class="line">feat_92    <span class="number">0</span></span><br><span class="line">feat_93    <span class="number">0</span></span><br><span class="line">target     <span class="number">0</span></span><br><span class="line">Length: <span class="number">94</span>, dtype: int64</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>读取 testing dataset</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">test_data = pd.read_csv(TEST_PATH, index_col=<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>显示 testing dataset 信息</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">test_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">Output exceeds the size limit. Open the full output data <span class="keyword">in</span> a text editor</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">Int64Index: <span class="number">144368</span> entries, <span class="number">1</span> to <span class="number">144368</span></span><br><span class="line">Data columns (total <span class="number">93</span> columns):</span><br><span class="line"> <span class="comment">#   Column   Non-Null Count   Dtype</span></span><br><span class="line">---  ------   --------------   -----</span><br><span class="line"> <span class="number">0</span>   feat_1   <span class="number">144368</span> non-null  int64</span><br><span class="line"> <span class="number">1</span>   feat_2   <span class="number">144368</span> non-null  int64</span><br><span class="line"> <span class="number">2</span>   feat_3   <span class="number">144368</span> non-null  int64</span><br><span class="line">...</span><br><span class="line"> <span class="number">91</span>  feat_92  <span class="number">144368</span> non-null  int64</span><br><span class="line"> <span class="number">92</span>  feat_93  <span class="number">144368</span> non-null  int64</span><br><span class="line">dtypes: int64(<span class="number">93</span>)</span><br><span class="line">memory usage: <span class="number">103.5</span> MB</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>查看 testing dataset 前几行</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">test_data.head()</span><br></pre></td></tr></tbody></table></figure><p></p>
<figure>
<img data-src="https://s2.loli.net/2022/09/08/JCoyXLOQSTUMe3w.png" alt=""><figcaption>testing dataset(before)</figcaption>
</figure>
<p>查找是否存在缺失值</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">test_data.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">feat_1     <span class="number">0</span></span><br><span class="line">feat_2     <span class="number">0</span></span><br><span class="line">feat_3     <span class="number">0</span></span><br><span class="line">feat_4     <span class="number">0</span></span><br><span class="line">feat_5     <span class="number">0</span></span><br><span class="line">          ..</span><br><span class="line">feat_89    <span class="number">0</span></span><br><span class="line">feat_90    <span class="number">0</span></span><br><span class="line">feat_91    <span class="number">0</span></span><br><span class="line">feat_92    <span class="number">0</span></span><br><span class="line">feat_93    <span class="number">0</span></span><br><span class="line">Length: <span class="number">93</span>, dtype: int64</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>统计 target 列中的类别和数量</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data[<span class="string">'target'</span>].value_counts()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">Class_2    <span class="number">16122</span></span><br><span class="line">Class_6    <span class="number">14135</span></span><br><span class="line">Class_8     <span class="number">8464</span></span><br><span class="line">Class_3     <span class="number">8004</span></span><br><span class="line">Class_9     <span class="number">4955</span></span><br><span class="line">Class_7     <span class="number">2839</span></span><br><span class="line">Class_5     <span class="number">2739</span></span><br><span class="line">Class_4     <span class="number">2691</span></span><br><span class="line">Class_1     <span class="number">1929</span></span><br><span class="line">Name: target, dtype: int64</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>数据处理</p>
<p>根据上面的信息，可以看到 training dataset 的特征为 int ，标签为 Classs_1 ~ Class_9 的字符串</p>
<p>现在需要将标签转换为 one-hot 格式</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data = pd.get_dummies(train_data)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>确认处理结果</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></tbody></table></figure><p></p>
<figure>
<img data-src="https://s2.loli.net/2022/09/08/yTz1bLJh6EuU2WV.png" alt=""><figcaption>training dataset(after)</figcaption>
</figure>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">Int64Index: <span class="number">61878</span> entries, <span class="number">1</span> to <span class="number">61878</span></span><br><span class="line">Columns: <span class="number">102</span> entries, feat_1 to target_Class_9</span><br><span class="line">dtypes: int64(<span class="number">93</span>), uint8(<span class="number">9</span>)</span><br><span class="line">memory usage: <span class="number">44.9</span> MB</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.notnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure><p></p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">feat_1            <span class="number">61878</span></span><br><span class="line">feat_2            <span class="number">61878</span></span><br><span class="line">feat_3            <span class="number">61878</span></span><br><span class="line">feat_4            <span class="number">61878</span></span><br><span class="line">feat_5            <span class="number">61878</span></span><br><span class="line">                  ...</span><br><span class="line">target_Class_5    <span class="number">61878</span></span><br><span class="line">target_Class_6    <span class="number">61878</span></span><br><span class="line">target_Class_7    <span class="number">61878</span></span><br><span class="line">target_Class_8    <span class="number">61878</span></span><br><span class="line">target_Class_9    <span class="number">61878</span></span><br><span class="line">Length: <span class="number">102</span>, dtype: int64</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>写入到 CSV 文件中</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">train_data.to_csv(PROCESSED_TRAIN_PATH, index=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol>
<h3 id="回顾">回顾</h3>
<h4 id="pandas.isnull-函数"><code>pandas.isnull()</code> 函数</h4>
<p>可以以布尔类型返回各行各列是否存在缺失 加上 sum() 函数可以统计各列的缺失情况</p>
<h5 id="参考网站">参考网站</h5>
<p>若尘公子 - <a href="https://zhuanlan.zhihu.com/p/158684561">#有空学04# pandas缺失数据查询</a></p>
<p>pandas - <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html">pandas.isnull</a></p>
<h4 id="pandas.value_count-函数"><code>pandas.value_count()</code> 函数</h4>
<p>可以返回该列中数据种类及其数量</p>
<p>方便后续进行格式转换</p>
<h5 id="参考网站-1">参考网站</h5>
<p>快乐的皮卡丘呦呦 - <a href="https://www.bbsmax.com/A/mo5k0wkndw/">Pandas中查看列中数据的种类及个数</a> pandas - <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html">pandas.DataFrame.value_counts</a></p>
<h4 id="pandas.get_dummies-函数"><code>pandas.get_dummies()</code> 函数</h4>
<p><code>pandas.get_dummies()</code> 函数会将非数值型数据转换为 One-Hot 格式</p>
<p>在该数据集中即使不指定 columns 参数，也只会转换 target 一列</p>
<h5 id="参考网页">参考网页</h5>
<p>ChaoFeiLi - <a href="https://blog.csdn.net/ChaoFeiLi/article/details/115345237">操作pandas某一列实现one-hot</a></p>
<p>pandas - <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html">pandas.get_dummies</a></p>
<p>pandas - <a href="https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html">pandas.get_dummies</a></p>
<h2 id="模型训练">模型训练</h2>
<p>导入库</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br></pre></td></tr></tbody></table></figure>
<h3 id="准备数据集">准备数据集</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 派生 CSV_Dataset 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OGPCCDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, filepath</span>):</span><br><span class="line">        xy = pd.read_csv(filepath, sep=<span class="string">","</span>, dtype=<span class="string">"float32"</span>)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.tensor(xy.iloc[:, :<span class="number">93</span>].values)</span><br><span class="line">        self.y_data = torch.tensor(xy.iloc[:, <span class="number">93</span>:].values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据集位置</span></span><br><span class="line">TRAIN_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/processed_train.csv"</span></span><br><span class="line">TEST_PATH = <span class="string">"./dataset/otto-group-product-classification-challenge/test.csv"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 装载数据集</span></span><br><span class="line">train_dataset = OGPCCDataset(TRAIN_PATH)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="设计网络模型">设计网络模型</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(<span class="number">93</span>, <span class="number">64</span>)</span><br><span class="line">        self.l2 = nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.l3 = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.l4 = nn.Linear(<span class="number">16</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">93</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        <span class="keyword">return</span> self.l4(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="设计-loss-和优化器">设计 Loss 和优化器</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="交叉熵损失函数">交叉熵损失函数</h4>
<p><span class="math display">\[Loss = -\sum^n_{i=1} y_i \log y'_i\]</span></p>
<p>交叉熵损失函数通常用于多分类任务的损失函数</p>
<h5 id="nllloss">NLLLoss</h5>
<p>NLLLoss 是将 Label 转换为 One-Hot 形式后与输出结果进行交叉熵计算</p>
<h5 id="torch.nn.crossentropyloss-函数"><code>Torch.nn.CrossEntropyLoss()</code> 函数</h5>
<p><code>Torch.nn.CrossEntropyLoss()</code> 函数是先将输出结果输入到 Softmax 层后取对数，再应用 NLLLoss</p>
<p>即 <code>Torch.nn.CrossEntropyLoss()</code> = LogSoftmax + NLLLoss</p>
<h3 id="训练过程">训练过程</h3>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    epoch_loss_list = []</span><br><span class="line">    epoch_acc_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            inputs, targets = data</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs, targets)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            _, labels = torch.<span class="built_in">max</span>(targets.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += targets.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        epoch_loss = running_loss / batch_idx</span><br><span class="line">        epoch_acc = <span class="number">100</span> * correct / total</span><br><span class="line">        epoch_loss_list.append(epoch_loss)</span><br><span class="line">        epoch_acc_list.append(epoch_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"[Epoch %3d] loss: %.3f acc: %.3f"</span> % (i + <span class="number">1</span>, epoch_loss, epoch_acc))</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    ax.plot(np.arange(epoch), epoch_loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    ax.plot(np.arange(epoch), epoch_acc_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    train(epoch)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="训练结果">训练结果</h2>
<p>Loss 随训练轮次变化</p>
<figure>
<img data-src="https://s2.loli.net/2022/09/08/crteKZJRNg1IHMy.png" alt=""><figcaption>Loss-Epoch</figcaption>
</figure>
<p>acc 随训练轮次变化</p>
<figure>
<img data-src="https://s2.loli.net/2022/09/08/MVjkvqKGX9Aswy4.png" alt=""><figcaption>acc-Epoch</figcaption>
</figure>
]]></content>
      <categories>
        <category>技术</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 搭建 DenseNet 模型对 CIFAR10 数据集分类总结</title>
    <url>/posts/25308b42.html</url>
    <content><![CDATA[<h2 id="densenet-模型搭建">DenseNet 模型搭建</h2>
<h3 id="结构分析">结构分析</h3>
<h4 id="通用框架">通用框架</h4>
<p>根据论文中的信息，可以得到常规 DenseNet 模型（忽略 batch_size ）的通用框架如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Input</td>
<td style="text-align: center;">224 * 224</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv</td>
<td style="text-align: center;">112 * 112</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Maxpool</td>
<td style="text-align: center;">56 * 56</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_1</td>
<td style="text-align: center;">56 * 56</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Transition Layer_1</td>
<td style="text-align: center;">28 * 28</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_2</td>
<td style="text-align: center;">28 * 28</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Transition Layer_2</td>
<td style="text-align: center;">14 * 14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_3</td>
<td style="text-align: center;">14 * 14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Transition Layer_3</td>
<td style="text-align: center;">7 * 7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_4</td>
<td style="text-align: center;">7 * 7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Global Avgpool</td>
<td style="text-align: center;">1 * 1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">FC</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<p>其中 Dense Layer 由多个 Dense Block 组成</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">ResNet18</th>
<th style="text-align: center;">ResNet34</th>
<th style="text-align: center;">ResNet50</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Dense Layer_1</td>
<td style="text-align: center;">Dense Block * 6</td>
<td style="text-align: center;">Dense Block * 6</td>
<td style="text-align: center;">Dense Block * 6</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_2</td>
<td style="text-align: center;">Dense Block * 12</td>
<td style="text-align: center;">Dense Block * 12</td>
<td style="text-align: center;">Dense Block * 12</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Dense Layer_3</td>
<td style="text-align: center;">Dense Block * 24</td>
<td style="text-align: center;">Dense Block * 32</td>
<td style="text-align: center;">Dense Block * 48</td>
</tr>
<tr class="even">
<td style="text-align: center;">Dense Layer_4</td>
<td style="text-align: center;">Dense Block * 16</td>
<td style="text-align: center;">Dense Block * 32</td>
<td style="text-align: center;">Dense Block * 32</td>
</tr>
</tbody>
</table>
<h4 id="基础结构">基础结构</h4>
<p>DenseNet 使用的 Dense Block 结构如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">in_size</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">4 * growth_rate</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">growth_rate</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Concatence</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">in_channel + growth_rate</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<p>DenseNet 使用的 Dense Block 结构如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">in_size</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">in_channel // 2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">Avgpool</td>
<td style="text-align: center;">x * x</td>
<td style="text-align: center;">(x/2) * (x/2)</td>
<td style="text-align: center;">in_channel // 2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<h4 id="conv">Conv</h4>
<p>DenseNet 的 Conv 层和 ResNet 的 Conv 层不同</p>
<p>ResNet 的 Conv 层实际上是 Conv - BatchNorm - ReLU</p>
<p>而 DenseNet 中除了第一个 Conv 层以外的其他 Conv 层实际上是 ReLU - BatchNorm - Conv</p>
<h3 id="网络实现">网络实现</h3>
<h4 id="dense-block">Dense Block</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features: <span class="built_in">int</span>, growth_rate: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_input_features)</span><br><span class="line">        self.relu1 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv1 = nn.Conv2d(num_input_features, <span class="number">4</span> * growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">4</span> * growth_rate)</span><br><span class="line">        self.relu2 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">4</span> * growth_rate, growth_rate, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_features: Tensor</span>) -&gt; Tensor:</span><br><span class="line"></span><br><span class="line">        new_features = self.conv1(self.relu1(self.bn1(input_features)))</span><br><span class="line">        new_features = self.conv2(self.relu2(self.bn2(new_features)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</span><br><span class="line">            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)</span><br><span class="line"></span><br><span class="line">        new_features = torch.cat((input_features, new_features), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> new_features</span><br></pre></td></tr></tbody></table></figure>
<h4 id="transition">Transition</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_Transition</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input_features: <span class="built_in">int</span>, num_output_features: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.Conv = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_input_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(num_input_features, num_output_features, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line">        self.AvgPool = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>):</span><br><span class="line">        output = self.Conv(x)</span><br><span class="line">        output = self.AvgPool(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></tbody></table></figure>
<h4 id="通用框架-1">通用框架</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DenseNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        growth_rate: <span class="built_in">int</span> = <span class="number">32</span>,</span></span><br><span class="line"><span class="params">        num_layers: <span class="type">List</span>[<span class="built_in">int</span>] = [<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>],</span></span><br><span class="line"><span class="params">        num_init_features: <span class="built_in">int</span> = <span class="number">64</span>,</span></span><br><span class="line"><span class="params">        drop_rate: <span class="built_in">float</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">        num_classes: <span class="built_in">int</span> = <span class="number">1000</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># transforming (batch_size * 224 * 224 * input_channel) to (batch_size * 112 * 112 * 64)</span></span><br><span class="line">        <span class="comment"># floor(((224 - 7 + 2 * 3) / 2) + 1) =&gt; floor(112.5) =&gt; floor(112)</span></span><br><span class="line">        self.Conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, num_init_features, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_init_features),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># transforming (batch_size * 112 * 112 * 64) to (batch_size * 56 * 56 * 64)</span></span><br><span class="line">        <span class="comment"># floor(((112 - 3 + 2 * 1) / 2) + 1) =&gt; floor(56.5) =&gt; floor(56)</span></span><br><span class="line">        self.MaxPool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        num_input_features = num_init_features</span><br><span class="line">        <span class="comment"># transforming (batch_size * 56 * 56 * in_channel) to (batch_size * 56 * 56 * (in_channel + num_layers[0] * growth_rate))</span></span><br><span class="line">        self.DenseBlock1 = self._make_DenseBlock(growth_rate, num_layers[<span class="number">0</span>], num_input_features, drop_rate)</span><br><span class="line">        num_input_features += num_layers[<span class="number">0</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 56 * 56 * in_channel) to (batch_size * 28 * 28 * (in_channel // 2))</span></span><br><span class="line">        self.Transition1 = _Transition(num_input_features, num_input_features // <span class="number">2</span>)</span><br><span class="line">        num_input_features = num_input_features // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 28 * 28 * in_channel) to (batch_size * 28 * 28 * (in_channel + num_layers[1] * growth_rate))</span></span><br><span class="line">        self.DenseBlock2 = self._make_DenseBlock(growth_rate, num_layers[<span class="number">1</span>], num_input_features, drop_rate)</span><br><span class="line">        num_input_features += num_layers[<span class="number">1</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 28 * 28 * in_channel) to (batch_size * 14 * 14 * (in_channel // 2))</span></span><br><span class="line">        self.Transition2 = _Transition(num_input_features, num_input_features // <span class="number">2</span>)</span><br><span class="line">        num_input_features = num_input_features // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 14 * 14 * in_channel) to (batch_size * 14 * 14 * (in_channel + num_layers[2] * growth_rate))</span></span><br><span class="line">        self.DenseBlock3 = self._make_DenseBlock(growth_rate, num_layers[<span class="number">2</span>], num_input_features, drop_rate)</span><br><span class="line">        num_input_features += num_layers[<span class="number">2</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 14 * 14 * in_channel) to (batch_size * 7 * 7 * (in_channel // 2))</span></span><br><span class="line">        self.Transition3 = _Transition(num_input_features, num_input_features // <span class="number">2</span>)</span><br><span class="line">        num_input_features = num_input_features // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 7 * 7 * in_channel) to (batch_size * 7 * 7 * (in_channel + num_layers[3] * growth_rate))</span></span><br><span class="line">        self.DenseBlock4 = self._make_DenseBlock(growth_rate, num_layers[<span class="number">3</span>], num_input_features, drop_rate)</span><br><span class="line">        num_input_features += num_layers[<span class="number">3</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * 7 * 7 * in_channel) to (batch_size * 1 * 1 * in_channel)</span></span><br><span class="line">        self.GlobleAvgPool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># transforming (batch_size * 1 * 1 * in_channel) to (batch_size * in_channel)</span></span><br><span class="line">        self.FC = nn.Linear(num_input_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_DenseBlock</span>(<span class="params">self, growth_rate: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span>, num_input_features: <span class="built_in">int</span>, drop_rate: <span class="built_in">int</span></span>) -&gt; nn.Sequential:</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(num_layers)):</span><br><span class="line">            layers.append(_DenseLayer(num_input_features, growth_rate, drop_rate=drop_rate))</span><br><span class="line">            num_input_features += growth_rate</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        output = self.Conv(x)</span><br><span class="line">        output = self.MaxPool(output)</span><br><span class="line">        output = self.Transition1(self.DenseBlock1(output))</span><br><span class="line">        output = self.Transition2(self.DenseBlock2(output))</span><br><span class="line">        output = self.Transition3(self.DenseBlock3(output))</span><br><span class="line">        output = self.DenseBlock4(output)</span><br><span class="line">        output = self.GlobleAvgPool(output)</span><br><span class="line">        output = torch.flatten(output, <span class="number">1</span>)</span><br><span class="line">        output = self.FC(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></tbody></table></figure>
<h4 id="构造网络">构造网络</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet121</span>() -&gt; DenseNet:</span><br><span class="line">    <span class="keyword">return</span> DenseNet(<span class="number">32</span>, [<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet169</span>() -&gt; DenseNet:</span><br><span class="line">    <span class="keyword">return</span> DenseNet(<span class="number">32</span>, [<span class="number">6</span>, <span class="number">12</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet201</span>() -&gt; DenseNet:</span><br><span class="line">    <span class="keyword">return</span> DenseNet(<span class="number">32</span>, [<span class="number">6</span>, <span class="number">12</span>, <span class="number">48</span>, <span class="number">32</span>])</span><br></pre></td></tr></tbody></table></figure>
<h4 id="参考网页">参考网页</h4>
<p>PyTorch - <a href="https://pytorch.org/vision/stable/_modules/torchvision/models/densenet.html#densenet121">SOURCE CODE FOR TORCHVISION.MODELS.DENSENET</a></p>
<p>Mayurji - <a href="https://github.com/Mayurji/Image-Classification-PyTorch/blob/main/DenseNet.py">Image-Classification-PyTorch/DenseNet.py</a></p>
<p>wmpscc - <a href="https://github.com/wmpscc/CNN-Series-Getting-Started-and-PyTorch-Implementation/blob/master/DenseNet/DenseNet-Torch.py">CNN-Series-Getting-Started-and-PyTorch-Implementation/DenseNet/DenseNet-Torch.py</a></p>
<p>pytorch - <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py">vision/torchvision/models/densenet.py</a></p>
<h2 id="模型训练">模型训练</h2>
<p>模型训练内容与 AlexNet_CIFAR10 项目相似，相同之处不再赘述</p>
<h2 id="总结">总结</h2>
<p>DenseNet 和 ResNet 很像， ResNet 是使用了 short cut，而 DenseNet 可以理解为将所有输出都进行 short cut 连接到了其后面的所有输出</p>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch 搭建 ResNet 模型对 CIFAR10 数据集分类总结</title>
    <url>/posts/cfb33f60.html</url>
    <content><![CDATA[<h2 id="resnet-模型搭建">ResNet 模型搭建</h2>
<h3 id="结构分析">结构分析</h3>
<h4 id="通用框架">通用框架</h4>
<p>根据 <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep Residual Learning for Image Recognition</a> 论文中的信息，可以得到常规 ResNet 模型的通用框架如下</p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">(18/34 layers) out_channel</th>
<th style="text-align: center;">(50/101/152 layers) out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Input</td>
<td style="text-align: center;">224*224</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">112*112</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Maxpool</td>
<td style="text-align: center;">56*56</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2_x</td>
<td style="text-align: center;">56*56</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64*4=256</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv3_x</td>
<td style="text-align: center;">28*28</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128*4=512</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv4_x</td>
<td style="text-align: center;">14*14</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256*4=1024</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv5_x</td>
<td style="text-align: center;">7*7</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512*4=2048</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Avgpool</td>
<td style="text-align: center;">1*1</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Flatten</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: center;">FC</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
</tbody>
</table>
<p>其中 Conv2_x 、 Conv3_x 、Conv4_x 、 Conv5_x 层可由 BasicBlock 和 Bottleneck 两种基本模型组合而成</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">ResNet18</th>
<th style="text-align: center;">ResNet34</th>
<th style="text-align: center;">ResNet50</th>
<th style="text-align: center;">ResNet101</th>
<th style="text-align: center;">ResNet152</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv2_x</td>
<td style="text-align: center;">BasicBlock*2</td>
<td style="text-align: center;">BasicBlock*3</td>
<td style="text-align: center;">Bottleneck*3</td>
<td style="text-align: center;">Bottleneck*3</td>
<td style="text-align: center;">Bottleneck*3</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv3_x</td>
<td style="text-align: center;">BasicBlock*2</td>
<td style="text-align: center;">BasicBlock*4</td>
<td style="text-align: center;">Bottleneck*4</td>
<td style="text-align: center;">Bottleneck*4</td>
<td style="text-align: center;">Bottleneck*8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv4_x</td>
<td style="text-align: center;">BasicBlock*2</td>
<td style="text-align: center;">BasicBlock*6</td>
<td style="text-align: center;">Bottleneck*6</td>
<td style="text-align: center;">Bottleneck*23</td>
<td style="text-align: center;">Bottleneck*36</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv5_x</td>
<td style="text-align: center;">BasicBlock*2</td>
<td style="text-align: center;">BasicBlock*3</td>
<td style="text-align: center;">Bottleneck*3</td>
<td style="text-align: center;">Bottleneck*3</td>
<td style="text-align: center;">Bottleneck*3</td>
</tr>
</tbody>
</table>
<h4 id="基础结构">基础结构</h4>
<p>ResNet 18/34 使用的 BasicBlock 结构如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">in_size</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">x*x</td>
<td style="text-align: center;">(x/stride)*(x/stride)</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">stride</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">identity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>ResNet 50/101/152 使用的 Bottleneck 结构如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">in_size</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">x*x</td>
<td style="text-align: center;">(x/stride)*(x/stride)</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">stride</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv3</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">identity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h4 id="stride-和-identity">stride 和 identity</h4>
<p>当基础结构是 Conv3_x 、Conv4_x 、 Conv5_x 的第一层时， <code>stride=2</code> 且 identity 为下采样后的输入</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">nn.Sequential(</span><br><span class="line">    nn.Conv2d(self.in_channel, out_channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">    nn.BatchNorm2d(out_channel * block.expansion),</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<p>当基础结构是 Conv3_x 、Conv4_x 、 Conv5_x 的其他层或在 Conv2_x 层时， <code>stride=1</code> 且 identity 为输入本身</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">nn.Sequential()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="网络实现">网络实现</h3>
<h4 id="basicblock">BasicBlock</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        downsample: <span class="type">Optional</span>[nn.Module] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># transforming (batch_size * x * x * input_channel) to (batch_size * x * x * output_channel)</span></span><br><span class="line">        <span class="comment">#                                                   or (batch_size * x/2 * x/2 * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x - 3 + 2 * 1) / stride) + 1) =&gt; floor(x) stride = 1</span></span><br><span class="line">        <span class="comment">#                                       =&gt; floor(x/2) stride = 2</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># transforming (batch_size * x' * x' * output_channel) to (batch_size * x' * x' * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x' - 3 + 2 * 1) / 1) + 1) =&gt; floor(x')</span></span><br><span class="line">        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure>
<h4 id="bottleneck">Bottleneck</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        downsample: <span class="type">Optional</span>[nn.Module] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transforming (batch_size * x * x * output_channel) to (batch_size * x * x * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x - 3 + 2 * 1) / 1) + 1) =&gt; floor(x)</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># transforming (batch_size * x * x * output_channel) to (batch_size * x * x * output_channel)</span></span><br><span class="line">        <span class="comment">#                                                    or (batch_size * x/2 * x/2 * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x - 3 + 2 * 1) / stride) + 1) =&gt; floor(x) stride = 1</span></span><br><span class="line">        <span class="comment">#                                       =&gt; floor(x/2) stride = 2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># transforming (batch_size * x' * x' * output_channel) to (batch_size * x' * x' * (output_channel* expansion))</span></span><br><span class="line">        <span class="comment"># floor(((x' - 3 + 2 * 1) / 1) + 1) =&gt; floor(x')</span></span><br><span class="line">        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure>
<h4 id="通用框架-1">通用框架</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block: <span class="type">Type</span>[<span class="type">Union</span>[BasicBlock, Bottleneck]], num_block: <span class="type">List</span>[<span class="built_in">int</span>], num_classes: <span class="built_in">int</span> = <span class="number">1000</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line">        <span class="comment"># transforming (batch_size * 224 * 224 * input_channel) to (batch_size * 112 * 112 * 64)</span></span><br><span class="line">        <span class="comment"># floor(((224 - 7 + 2 * 3) / 2) + 1) =&gt; floor(112.5) =&gt; floor(112)</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(self.in_channel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># transforming (batch_size * 112 * 112 * 64) to (batch_size * 56 * 56 * 64)</span></span><br><span class="line">        <span class="comment"># floor(((112 - 3 + 2 * 1) / 2) + 1) =&gt; floor(56.5) =&gt; floor(56)</span></span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 56 * 56 * 64) to (batch_size * 56 * 56 * (64 * block.expansion))</span></span><br><span class="line">        self.conv2_x = self._make_layer(block, <span class="number">64</span>, num_block[<span class="number">0</span>], stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 56 * 56 * (64 * block.expansion)) to (batch_size * 28 * 28 * (128 * block.expansion))</span></span><br><span class="line">        self.conv3_x = self._make_layer(block, <span class="number">128</span>, num_block[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 28 * 28 * (128 * block.expansion)) to (batch_size * 14 * 14 * (256 * block.expansion))</span></span><br><span class="line">        self.conv4_x = self._make_layer(block, <span class="number">256</span>, num_block[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 14 * 14 * (256 * block.expansion)) to (batch_size * 7 * 7 * (512 * block.expansion))</span></span><br><span class="line">        self.conv5_x = self._make_layer(block, <span class="number">512</span>, num_block[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 7 * 7 * (512 * block.expansion)) to (batch_size * 1 * 1 * (512 * block.expansion))</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># transforming (batch_size * 2048) to (batch_size * num_classes)</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, block: <span class="type">Type</span>[<span class="type">Union</span>[BasicBlock, Bottleneck]], out_channel: <span class="built_in">int</span>, num_blocks: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; nn.Sequential:</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span>:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, out_channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">                nn.BatchNorm2d(out_channel * block.expansion),</span><br><span class="line">            )</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel, out_channel, stride, downsample))</span><br><span class="line">        self.in_channel = out_channel * block.expansion</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_blocks):</span><br><span class="line">            layers.append(block(self.in_channel, out_channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        x = self.conv2_x(x)</span><br><span class="line">        x = self.conv3_x(x)</span><br><span class="line">        x = self.conv4_x(x)</span><br><span class="line">        x = self.conv5_x(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h4 id="构造网络">构造网络</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet18</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet34</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet50</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet101</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet152</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure>
<h4 id="参考网页">参考网页</h4>
<p>PyTorch - <a href="https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#resnet101">SOURCE CODE FOR TORCHVISION.MODELS.RESNET</a></p>
<p>明素 - <a href="https://blog.csdn.net/weixin_43406381/article/details/118404612">ResNet详解</a></p>
<h3 id="回顾">回顾</h3>
<h4 id="nn.conv2d-函数-bias-参数的设置"><code>nn.Conv2d()</code> 函数 <code>bias</code> 参数的设置</h4>
<p>当 <code>nn.Conv2d()</code> 后接 <code>nn.BatchNorm2d()</code> 时，可以把 <code>bias</code> 参数设置为 <code>False</code></p>
<p>因为在 BN 层中，输入是否存在偏置不影响输出结果</p>
<p>不添加偏置还可以减少显卡内存的占用</p>
<h5 id="参考网页-1">参考网页</h5>
<p>7s记忆的鱼 - <a href="https://blog.csdn.net/qq_38230414/article/details/125977540">【pytorch】Conv2d()里面的参数bias什么时候加，什么时候不加？</a></p>
<h4 id="nn.adaptiveavgpool2d-函数"><code>nn.AdaptiveAvgPool2d</code> 函数</h4>
<h5 id="参考网页-2">参考网页</h5>
<h4 id="参数-的作用"><code>*参数</code> 的作用</h4>
<p><code>*参数</code> 可以解压参数</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">a = (<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>)</span><br><span class="line">b = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line"><span class="built_in">print</span>(*a)</span><br><span class="line"><span class="built_in">print</span>(*b)</span><br></pre></td></tr></tbody></table></figure>
<p>将 List 和 Tuple 中的元素逐一解压出来</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="参考网页-3">参考网页</h5>
<p>TEDxPY - <a href="https://blog.csdn.net/weixin_40796925/article/details/107574267">Python *args 用法笔记</a></p>
<h4 id="pip-install-默认安装在-base-环境"><code>pip install</code> 默认安装在 <code>base</code> 环境</h4>
<p>使用 <code>pip install</code> 时改用如下指令即可安装到当前虚拟环境中</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">python -m pip install **</span><br></pre></td></tr></tbody></table></figure>
<h5 id="参考网页-4">参考网页</h5>
<p>timertimer - <a href="https://blog.csdn.net/timertimer/article/details/122808662">在conda虚拟环境中用pip安装包总是在base环境中的解决办法</a></p>
<h3 id="cifar10-特化模型">CIFAR10 特化模型</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Input</td>
<td style="text-align: center;">32*32</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">32*32</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv2_x</td>
<td style="text-align: center;">32*32</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">64*4=256</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv3_x</td>
<td style="text-align: center;">16*16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128*4=512</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv4_x</td>
<td style="text-align: center;">8*8</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">256*4=1024</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">Avgpool</td>
<td style="text-align: center;">1*1</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Flatten</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">FC</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>其中 Conv2_x 、 Conv3_x 、Conv4_x 层由 Block 组成</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">ResNet_CIFAR10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv2_x</td>
<td style="text-align: center;">Block*n</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv3_x</td>
<td style="text-align: center;">Block*n</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Conv4_x</td>
<td style="text-align: center;">Block*n</td>
</tr>
</tbody>
</table>
<p>Block 结构如下</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">layer_name</th>
<th style="text-align: center;">in_size</th>
<th style="text-align: center;">out_size</th>
<th style="text-align: center;">out_channel</th>
<th style="text-align: center;">kernel_size</th>
<th style="text-align: center;">stride</th>
<th style="text-align: center;">padding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Conv1</td>
<td style="text-align: center;">x*x</td>
<td style="text-align: center;">(x/stride)*(x/stride)</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">stride</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Conv2</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">x'*x'</td>
<td style="text-align: center;">out_channel</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">identity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>当基础结构是 Conv3_x 、Conv4_x 、 Conv5_x 的第一层时， <code>stride=2</code> 且 identity 为下采样后的输入</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">nn.Sequential(</span><br><span class="line">    nn.Conv2d(self.in_channel, out_channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">    nn.BatchNorm2d(out_channel * block.expansion),</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<p>当基础结构是 Conv3_x 、Conv4_x 、 Conv5_x 的其他层或在 Conv2_x 层时， <code>stride=1</code> 且 identity 为输入本身</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">nn.Sequential()</span><br></pre></td></tr></tbody></table></figure>
<h4 id="模型实现">模型实现</h4>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    expansion: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        in_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        out_channel: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        stride: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        downsample: <span class="type">Optional</span>[nn.Module] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># transforming (batch_size * x * x * input_channel) to (batch_size * x * x * output_channel)</span></span><br><span class="line">        <span class="comment">#                                                   or (batch_size * x/2 * x/2 * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x - 3 + 2 * 1) / stride) + 1) =&gt; floor(x) stride = 1</span></span><br><span class="line">        <span class="comment">#                                       =&gt; floor(x/2) stride = 2</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="comment"># transforming (batch_size * x' * x' * output_channel) to (batch_size * x' * x' * output_channel)</span></span><br><span class="line">        <span class="comment"># floor(((x' - 3 + 2 * 1) / 1) + 1) =&gt; floor(x')</span></span><br><span class="line">        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block: Block, num_block: <span class="type">List</span>[<span class="built_in">int</span>], num_classes: <span class="built_in">int</span> = <span class="number">10</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_channel = <span class="number">16</span></span><br><span class="line">        <span class="comment"># transforming (batch_size * 32 * 32 * input_channel) to (batch_size * 32 * 32 * 16)</span></span><br><span class="line">        <span class="comment"># floor(((32 - 3 + 2 * 1) / 1) + 1) =&gt; floor(112.5) =&gt; floor(112)</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(self.in_channel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># transforming (batch_size * 32 * 32 * 16) to (batch_size * 32 * 32 * 16)</span></span><br><span class="line">        self.conv2_x = self._make_layer(block, <span class="number">16</span>, num_block, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 32 * 32 * 16) to (batch_size * 16 * 16 * 32)</span></span><br><span class="line">        self.conv3_x = self._make_layer(block, <span class="number">32</span>, num_block, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 16 * 16 * 16) to (batch_size * 8 * 8 * 64)</span></span><br><span class="line">        self.conv4_x = self._make_layer(block, <span class="number">64</span>, num_block, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># transforming (batch_size * 8 * 8 * 64) to (batch_size * 1 * 1 * 64)</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># transforming (batch_size * 64) to (batch_size * num_classes)</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">64</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">"fan_out"</span>, nonlinearity=<span class="string">"relu"</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block: Block, out_channel: <span class="built_in">int</span>, num_blocks: <span class="built_in">int</span>, stride: <span class="built_in">int</span> = <span class="number">1</span></span>) -&gt; nn.Sequential:</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span>:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, out_channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">                nn.BatchNorm2d(out_channel * block.expansion),</span><br><span class="line">            )</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel, out_channel, stride, downsample))</span><br><span class="line">        self.in_channel = out_channel * block.expansion</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_blocks):</span><br><span class="line">            layers.append(block(self.in_channel, out_channel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2_x(x)</span><br><span class="line">        x = self.conv3_x(x)</span><br><span class="line">        x = self.conv4_x(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet20</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet32</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet44</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet56</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet110</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">18</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet1202</span>() -&gt; ResNet:</span><br><span class="line">    <span class="keyword">return</span> ResNet(Block, <span class="number">200</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="模型训练">模型训练</h2>
<p>模型训练内容与 AlexNet_CIFAR10 项目相似，相同之处不再赘述</p>
<h3 id="封装自定义-python-库">封装自定义 Python 库</h3>
<p>此次实验中将 AlexNet_CIFAR10 项目中计算数据集均值和方差封装在 <code>utils</code> 文件夹下</p>
<p>需要在 <code>utils</code> 文件夹下生成空的 <code>__init__.py</code> 文件，声明 <code>utils</code> 文件夹为封装好的 Python 库</p>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>WSL 安装总结</title>
    <url>/posts/f1f0ebac.html</url>
    <content><![CDATA[<p>考虑到虚拟机不能使用 GPU 加速，现在打算配置 WSL 在 Ubuntu 上进行训练</p>
<h2 id="环境要求">环境要求</h2>
<p>Windows 10 版本 2004 及更高版本</p>
<h2 id="wsl-安装步骤">WSL 安装步骤</h2>
<ol type="1">
<li><p>启用系统功能</p>
<p>控制面板 =⇒ 程序 =⇒ 程序和功能 =⇒ 启用或关闭 windows 功能，勾选下列选项</p>
<ul>
<li>Hyper-V</li>
<li>适用于 Linux 的 Windows 子系统</li>
<li>虚拟机平台</li>
</ul>
<p>然后重新启动计算机</p></li>
<li><p>下载并安装<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">适用于 x64 计算机的 WSL2 Linux 内核更新包</a></p></li>
<li><p>将 WSL 2 设置为默认版本</p>
<p>打开 PowerShell ，输入如下指令</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">wsl <span class="literal">--set-default-version</span> <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>安装 Linux 分发</p>
<p>在 PowerShell 中输入如下指令可以查看可获取的 Linux 分发版本</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">wsl <span class="literal">--list</span> <span class="literal">--online</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>推荐选择 Ubuntu-20.04.3 LTS</p>
<p>输入如下指令安装 Linux 分发</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">wsl <span class="literal">--install</span> <span class="literal">-d</span> &lt;Distribution Name&gt;</span><br></pre></td></tr></tbody></table></figure><p></p>
<blockquote>
<p>WSL --install 默认安装固定版本的 Ubuntu -d 参数可以选择任意其他 Linux 分发版本 &lt;Distribution Name&gt; 替换为 Linux 分发版本的名称</p>
</blockquote></li>
<li><p>安装后设置用户账号和密码</p></li>
</ol>
<h3 id="wsl-安装参考网页">WSL 安装参考网页</h3>
<p>Microsoft - <a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-manual">旧版 WSL 的手动安装步骤</a></p>
<p>憨憨不敢_ - <a href="https://blog.csdn.net/weixin_45191709/article/details/125871102">2022 window下安装ubuntu22.04（wsl升级 包含 podman &amp; docker ）</a></p>
<h2 id="wsl-配置">WSL 配置</h2>
<ol type="1">
<li><p>WSL2 连接 Windows 防火墙</p>
<ol type="1">
<li><p>检查 WSL 与 Windows 连接</p>
<p>在 PowerShell 中输入如下指令获取 Windows 本机 IP 和 WSL IP</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">ipconfig</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Windows 本机 IP 地址如下</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">无线局域网适配器 WLAN:</span><br><span class="line"></span><br><span class="line">    IPv4 地址 . . . . . . . . . . . . :</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>WSL IP 地址如下</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">以太网适配器 vEthernet (WSL):</span><br><span class="line"></span><br><span class="line">    IPv4 地址 . . . . . . . . . . . . :</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>分别在 PowerShell 和 WSL 中输入下列指令检查连接</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line">ping &lt;IPv4 Address&gt;</span><br></pre></td></tr></tbody></table></figure><p></p>
<blockquote>
<p>Linux 系统中 Ping 指令不会自行停止，需要使用<code>Ctrl + C</code>停止指令</p>
</blockquote></li>
<li><p>若不能 ping 通，则检查 Windows 防火墙：</p>
<p>控制面板 =⇒ 系统和安全 =⇒ Windows Defender 防火墙 =⇒ 高级设置</p>
<ul>
<li>入站规则</li>
<li>出站规则</li>
</ul>
<p>启用其中所有 WSL 规则</p></li>
<li><p>若不存在该规则则需要自行创建：在 PowerShell 中以管理员身份输入如下指令</p>
<p></p><figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">New-NetFirewallRule</span> <span class="literal">-DisplayName</span> <span class="string">"WSL"</span> <span class="literal">-Direction</span> Inbound  <span class="literal">-InterfaceAlias</span> <span class="string">"vEthernet (WSL)"</span>  <span class="literal">-Action</span> Allow</span><br><span class="line"><span class="built_in">New-NetFirewallRule</span> <span class="literal">-DisplayName</span> <span class="string">"WSL"</span> <span class="literal">-Direction</span> Outbound  <span class="literal">-InterfaceAlias</span> <span class="string">"vEthernet (WSL)"</span>  <span class="literal">-Action</span> Allow</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol></li>
<li><p>配置 Ubuntu apt 源</p>
<p>备份原 apt 源</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">sudo <span class="built_in">mv</span> /etc/apt/sources.list /etc/apt/sources.list.bak</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>编辑 apt 源</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">sudo vi /etc/apt/sources.list</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>添加国内源</p>
<ul>
<li><p>清华源</p>
<figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt; main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt; main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-security main restricted universe multiverse</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>中科大源</p>
<figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt; main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt; main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-security main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-updates main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-backports main restricted universe multiverse</span><br><span class="line">deb-src https://mirrors.ustc.edu.cn/ubuntu/ &lt;Ubuntu Distribution Name&gt;-backports main restricted universe multiverse</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<p>更新源</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>更新软件</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></tbody></table></figure><p></p></li>
</ol>
<h3 id="wsl-ubuntu-配置问题">WSL / Ubuntu 配置问题</h3>
<ol type="1">
<li><p>Ubuntu 切换阿里云源后提示缺少公钥</p>
<p>解决方法：</p>
<p>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32</p></li>
</ol>
<h3 id="wsl-ubuntu-配置参考网页">WSL / Ubuntu 配置参考网页</h3>
<p>清华大学开源软件镜像站 - <a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/">Ubuntu 镜像使用帮助</a> 中国科技大学开源软件镜像站 - <a href="https://mirrors.ustc.edu.cn/help/ubuntu.html">Ubuntu 源使用帮助</a> weixin_43858295 - <a href="https://blog.csdn.net/weixin_43858295/article/details/123959824">Ubuntu 换阿里云源后更新提示： GPG error 缺少公钥解决方法</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>WSL</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>WSL 深度学习总结</title>
    <url>/posts/e5bb69fc.html</url>
    <content><![CDATA[<h2 id="环境安装">环境安装</h2>
<ol type="1">
<li><p>下载 Anaconda for Linux</p></li>
<li><p>使用 <code>cd</code> 指令转到下载文件夹，输入如下指令安装 Anaconda</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">bash Anaconda3-&lt;版本&gt;-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><p></p>
<blockquote>
<p>在 WSL 中， Windows 系统文件保存在 <code>/mnt</code> 文件夹下， C 盘对应文件夹 <code>/mnt/C</code></p>
</blockquote></li>
<li><p>配置环境变量</p>
<p>输入如下指令，编辑 profile</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>在文件的最底端，添加如下代码</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=&lt;Anaconda Adress&gt;/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>输入如下指令，重新加载环境变量</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>构建 Anaconda 环境</p>
<p>输入以下指令创建新 conda 环境</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n &lt;Environment Name&gt; pip python=&lt;Python Vertion&gt;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>输入以下指令激活 conda 环境</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">conda activate &lt;Environment Name&gt;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>使用 <code>pip install</code> 指令或 <code>conda install</code> 指令安装环境所需模块</p></li>
</ol>
<h3 id="环境安装参考网页">环境安装参考网页</h3>
<p>萝北村的枫子 - <a href="https://blog.csdn.net/thy0000/article/details/122878599">Ubuntu 安装 Anaconda 详细步骤</a></p>
<p>文艺圈不知名刘先生 - <a href="https://blog.csdn.net/huiruwei1020/article/details/107630269">WSL 安装 Anaconda</a></p>
<p>阿柴 - <a href="https://zhuanlan.zhihu.com/p/510556215">从零开始的 WSL 深度学习环境配置</a></p>
<p>Lyle Chen -<a href="https://zhuanlan.zhihu.com/p/453778081">基于 WSL 搭建深度学习开发环境</a></p>
<h2 id="point-net-在-wsl-上复现">Point Net 在 WSL 上复现</h2>
<p>根据作者在 GitHub 上的信息构建环境</p>
<blockquote>
<p>Installation Install TensorFlow. You may also need to install h5py. The code has been tested with Python 2.7, TensorFlow 1.0.1, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04.</p>
</blockquote>
<ol type="1">
<li><p>创建 python 2.7 conda 环境</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n pointnet pip python=<span class="number">2.7</span></span><br><span class="line">conda activate pointnet</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>在 conda 环境中安装下列模块</p>
<ul>
<li>TensorFlow</li>
<li>Matplotlib</li>
<li>Pillow</li>
<li>SciPy</li>
<li>h5py</li>
<li>PIL</li>
</ul>
<p>即输入如下指令</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">sudo pip install h5py</span><br><span class="line">conda install h5py</span><br><span class="line">conda install tensorflow-gpu=<span class="number">1.0</span><span class="number">.1</span></span><br><span class="line">conda install matplotlib</span><br><span class="line">conda install pillow</span><br><span class="line">conda install scipy</span><br><span class="line">conda install PIL</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>下载 <a href="https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip">modelnet40_ply_hdf5_2048</a> 解压到 <code>pointnet-master/data</code> 文件夹下</p></li>
<li><p>输入如下指令，执行 train.py</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">python train.py</span><br></pre></td></tr></tbody></table></figure><p></p>
<blockquote>
<p>train.py 中默认训练 250 epochs ， GTX 1060 训练 110 epochs 需要 40 小时，而在 80 epoch 时，训练结果已收敛，若只是复现可以略微减少训练轮次 输入 <code>python train.py --epoch &lt;epoch&gt;</code> 即可调整最大训练轮次</p>
</blockquote></li>
<li><p>train.py 运行结果片段</p>
<p></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">**** EPOCH <span class="number">100</span> ****</span><br><span class="line">----<span class="number">0</span>-----</span><br><span class="line">mean loss: <span class="number">0.147063</span></span><br><span class="line">accuracy: <span class="number">0.951593</span></span><br><span class="line">----<span class="number">1</span>-----</span><br><span class="line">mean loss: <span class="number">0.169319</span></span><br><span class="line">accuracy: <span class="number">0.939941</span></span><br><span class="line">----<span class="number">2</span>-----</span><br><span class="line">mean loss: <span class="number">0.155273</span></span><br><span class="line">accuracy: <span class="number">0.940430</span></span><br><span class="line">----<span class="number">3</span>-----</span><br><span class="line">mean loss: <span class="number">0.176028</span></span><br><span class="line">accuracy: <span class="number">0.940918</span></span><br><span class="line">----<span class="number">4</span>-----</span><br><span class="line">mean loss: <span class="number">0.156226</span></span><br><span class="line">accuracy: <span class="number">0.941406</span></span><br><span class="line">----<span class="number">0</span>-----</span><br><span class="line">----<span class="number">1</span>-----</span><br><span class="line"><span class="built_in">eval</span> mean loss: <span class="number">0.564006</span></span><br><span class="line"><span class="built_in">eval</span> accuracy: <span class="number">0.874188</span></span><br><span class="line"><span class="built_in">eval</span> avg <span class="keyword">class</span> <span class="title class_">acc</span>: <span class="number">0.844451</span></span><br><span class="line">Model saved <span class="keyword">in</span> file: log/model.ckpt</span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>输入如下指令，进行测试</p>
<p></p><figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">python evaluate.py --visu</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>测试结果</p>
<p></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">eval</span> mean loss: <span class="number">0.535309</span></span><br><span class="line"><span class="built_in">eval</span> accuracy: <span class="number">0.880875</span></span><br><span class="line"><span class="built_in">eval</span> avg <span class="keyword">class</span> <span class="title class_">acc</span>: <span class="number">0.852448</span></span><br></pre></td></tr></tbody></table></figure><p></p></li>
<li><p>输入如下指令，在 TensorBoard 中查看训练历史</p>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/5KaobVEOAci4W7v.png" alt=""><figcaption>accuracy</figcaption>
</figure>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/oeDEZXPg13I8Txz.png" alt=""><figcaption>bn_decay</figcaption>
</figure>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/qEgKTBavbFm41zS.png" alt=""><figcaption>classify_loss</figcaption>
</figure>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/sc1w7rvyXxDOH93.png" alt=""><figcaption>loss</figcaption>
</figure>
<figure>
<img data-src="https://s2.loli.net/2022/08/19/ygeWncQrGSH1K3I.png" alt=""><figcaption>mat_loss</figcaption>
</figure></li>
</ol>
<h3 id="point-net-在-wsl-上复现参考网页">Point Net 在 WSL 上复现参考网页</h3>
<p>YQ8023family - <a href="https://blog.csdn.net/qq_40234695/article/details/86223577">PointNet 复现</a></p>
<h3 id="point-net-在-wsl-上复现遇到问题">Point Net 在 WSL 上复现遇到问题</h3>
<ol type="1">
<li><p>在进行训练的时候，检查 Windows 系统任务管理器，发现 GPU 使用率基本为 0%，即没有使用 GPU 加速，且运行速度较慢</p>
<p>可能的解决方法：</p>
<ul>
<li>在检查环境的时候发现没有安装 cudnn ，怀疑可能是缺乏该模块影响了训练速度，稍后安装好后再进行尝试</li>
<li>改用服务器进行训练，服务器的显卡比本机好，运行速度较快(GTX1060)</li>
</ul>
<p>后续解决：</p>
<ul>
<li>在输入指令 <code>conda install cudnn</code> 安装 cudnn 后，再次执行 <code>train.py</code> 文件，发现已经可以调用核显(Intel® UHD Graphics 630)的 GPU ，但是独显的 GPU 使用率依旧为 0%</li>
</ul>
<p>后续可能的解决方法：</p>
<ul>
<li><del>在检查 <code>train.py</code> 文件默认参数后发现，程序默认使用 GPU 0，即本机的核显，使用指令 <code>python train.py --gpu 1</code> 执行可使用本机独显</del></li>
<li>经咨询后使用 <code>nvidia-smi</code> 指令查看显卡，发现只有独显 GTX 1060，即不存在 GPU 1</li>
</ul>
<p>最终解决方法：</p>
<ul>
<li>输入 <code>conda remove tensorflow</code> 指令移除 TensorFlow 库，输入 <code>conda install tensorflow-gpu==1.1.0</code> 安装 TensorFlow-GPU 库</li>
</ul>
<p>事后追因：</p>
<ol type="1">
<li>安装环境时输入指令 <code>nvidia-smi</code> 检查驱动，发现 <code>CUDA Version: 11.7</code> 且 <code>Driver Version: 516.94</code> 就没有去安装 cudnn</li>
<li>安装 TensorFlow 时，一开始有考虑安装 TensorFlow-GPU 版本，但是想着尽量和作者要求的版本一致，然而并不存在 TensorFlow-GPU 1.0.1 版本，最终安装了 TensorFlow 1.0.1 版本</li>
</ol>
<p>总结：</p>
<ul>
<li>安装环境时优先安装 cuda 、 cudnn ，若需要安装 TensorFlow 库时，优先安装先进版本的 TensorFlow-GPU 库</li>
</ul></li>
<li><p>WSL 上安装的 Ubuntu-22.04 LTS 因无法安装图形化界面，无法显示分类失败的图片，也没法通过输入 <code>tensorboard --logdir log</code> 后打开 <a href="http://localhost:6006" class="uri">http://localhost:6006</a> 查看训练过程</p>
<p>可能的解决方法：</p>
<ul>
<li>将训练后整个文件夹复制到 Windows 硬盘上，在 Windows 系统下查看</li>
<li>后续实验发现，在 Ubuntu-20.04 LTS 系统上可以实现图形化界面，也可尝试在该系统上进行论文复现</li>
</ul></li>
<li><p>在执行 train.py 文件时，弹出错误信息</p>
<p></p><figure class="highlight text"><table><tbody><tr><td class="code"><pre><span class="line">Could not identify NUMA node of /job:localhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA support.</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>翻找 TensorFlow 源码后找到弹出错误信息的原因</p>
<blockquote>
<p>For some reason the StreamExecutor couldn't get the NUMA affinity of the GPU. If this is not a multi-socket mobo with GPUs local to different buses, it doesn't matter. If it is, we may run into trouble later with data transfer operations. The trouble may manifest as slower than expected performance, or outright failures.</p>
</blockquote>
<p>最终处理是忽略该错误信息</p></li>
</ol>
<h3 id="解决问题参考网页">解决问题参考网页</h3>
<p>小白的2015 - <a href="https://blog.csdn.net/baixiaozhe/article/details/54598346">NUMA support</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>点云</tag>
        <tag>PointNet</tag>
        <tag>WSL</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>基于机器学习的相邻时序植物点云配准总结</title>
    <url>/posts/5f7835f6.html</url>
    <content><![CDATA[<h2 id="总结">总结</h2>
<h3 id="代码部分">代码部分</h3>
<h4 id="循环部分">循环部分</h4>
<p>在循环时需要注意利用 <code>if</code> 判断跳过的部分</p>
<p>例如通过某些特征值进行筛选，对特定的类别赋值时，不应忘记没有通过筛选的部分，要对这些部分也进行赋值，否则会导致赋值顺序与实际顺序无法对应</p>
<h4 id="应用变换部分">应用变换部分</h4>
<p>在将点云应用变换矩阵的时候应该注意，该函数是否会改变原点云，若会改变则需要深复制点云进行保护</p>
<h3 id="策略部分">策略部分</h3>
<h4 id="归一化部分">归一化部分</h4>
<p>数据集的归一化具有必要性</p>
<p>但是针对点云中坐标的归一化不能简单的使用最大最小值归一化，因为这可能会导致点云的压缩</p>
<p>针对点云坐标的归一化应该使用均值方差归一化，即 <code>PointNet</code> 和 <code>PointNet++</code> 所提出的归一化方式</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">xyz = np.asarray(pcd.points)                    <span class="comment"># pcd 为 open3D 的 PointCloud 类型</span></span><br><span class="line">centroid = np.mean(xyz, axis=<span class="number">0</span>)                 <span class="comment"># 求取点云的中心</span></span><br><span class="line">xyz = xyz - centroid                            <span class="comment"># 将点云中心置于原点 (0, 0, 0)</span></span><br><span class="line">m = np.<span class="built_in">max</span>(np.sqrt(np.<span class="built_in">sum</span>(xyz**<span class="number">2</span>, axis=<span class="number">1</span>)))     <span class="comment"># 求取长轴的的长度</span></span><br><span class="line">xyz_norm = xyz / m                              <span class="comment"># 将点云进行缩放</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="配准策略部分">配准策略部分</h4>
<p>直接对整个点云进行配准可能会面临叶片增加或减少情况，导致配准效果不佳</p>
<p>针对此情况，采取选取植株中最大的三片叶片，对这三片叶片进行计算特征后进行配准</p>
<p>暂时使用的配准方法是使用 open3D 库提供的配准方法</p>
<p>即 计算点云的 FPFH 特征，然后进行粗配准，再进行 ICP 精配准</p>
<p>叶片生长过程中叶片位置、形态可能会发生较大变化</p>
<p>导致相邻时序中选取的叶片并不对应，配准时陷入局部最优（叶片翻转或叶片垂直或叶片偏移）</p>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>点云</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>点云补全论文阅读总结</title>
    <url>/posts/781c276d.html</url>
    <content><![CDATA[<h2 id="beyond-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds1">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2>
<p>论文提出了一种针对 LiDAR 雷达获得的存在运动物体场景的目标跟踪方法</p>
<p>论文并不是纯粹点云补全，而是针对运动中的物体（即在场景中发生刚性变换的物体）进行补全，略过</p>
<h2 id="monocular-3d-object-reconstruction-with-gan-inversion2">Monocular 3D Object Reconstruction with GAN Inversion<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h2>
<p>论文提出了一种基于 GAN 网络的由图像生成具有相似纹理特征的 Mesh 方法</p>
<p>略过</p>
<h2 id="d-pl-domain-adaptive-depth-estimation-with-3d-aware-pseudo-labeling3">3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h2>
<p>论文提出了一种利用二维图像和三维点云进行标注深度图像数据集的单目图像深度估计方法</p>
<p>论文并不是关于点云补全，而是针对深度图进行补全，略过</p>
<h2 id="rignet-repetitive-image-guided-network-for-depth-completion4">RigNet: Repetitive Image Guided Network for Depth Completion<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h2>
<p>论文提出了一种基于多尺度的稀疏深度图补全方法</p>
<p>论文并不是关于点云补全，而是针对深度图进行补全，略过</p>
<h2 id="multi-modal-masked-pre-training-for-monocular-panoramic-depth-completion5">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></h2>
<p>论文提出了一种使用随机 Mask 训练的单目 360° 全景图片的稀疏深度图补全方法</p>
<p>论文并不是纯粹点云补全，针对全景图片的点云进行补全，略过</p>
<h2 id="shapeformer-transformer-based-shape-completion-via-sparse-representation6">ShapeFormer: Transformer-based Shape Completion via Sparse Representation<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></h2>
<p>论文提出了一种基于注意力机制和点云体素化表示编码器解码器 (VQDIF) 的补全 / GAN 方法，输入为缺失的点云，转换成 DIF 表示后进行补全 / 生成，最后再转换为 Mesh 输出</p>
<ul>
<li>深度隐函数 DIF</li>
</ul>
<blockquote>
<p>与图像补全不同，在 3D 形状补全中，输入也可能含有噪声，若是完整保留输入必然会产生嘈杂的结果</p>
</blockquote>
<p>潜在的研究方向，可以放着，但是并不是纯点云研究，优先级并不高</p>
<h2 id="fast-algorithm-for-low-rank-tensor-completion-in-delay-embedded-space7">Fast Algorithm for Low-rank Tensor Completion in Delay-embedded Space<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></h2>
<p>论文提出了一种利用多路延迟嵌入变换实现张量 / 图像补全的方法</p>
<p>论文并不是纯粹点云补全，略过</p>
<h2 id="sparse-fuse-dense-towards-high-quality-3d-detection-with-depth-completion8">Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></h2>
<p>论文提出了一种利用 LiDAR 稀疏点云和二维彩色图像的多模态融合实现目标检测的方法，并没有涉及到补全部分</p>
<p>论文并不是纯粹点云补全，而是尝试使用多模态进行补全，略过</p>
<h2 id="d-shape-reconstruction-from-2d-images-with-disentangled-attribute-flow9">3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></h2>
<p>论文提出了一种基于 Attribute Flow 模型实现从单张二维 RGB 图像重建为 3D 点云的方法</p>
<p>但在论文中也提到该方法可以用在点云补全，但是文中并没有详细说明如何修改网络，而是说参照 VRCNet<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> 的设置，VRCNet 在待阅读论文中，将详细查看具体方法</p>
<p>暂留</p>
<p>[ ] VRCNet 补充引用</p>
<h2 id="learning-local-displacements-for-point-cloud-completion10">Learning Local Displacements for Point Cloud Completion<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></h2>
<h3 id="introduction-conclusion">Introduction &amp; Conclusion</h3>
<p>论文提出了以下内容</p>
<ul>
<li>基于编码器-解码器架构的对象点云补全和场景点云补全算法模型
<ul>
<li>针对局部点云的特征提取方法</li>
<li>能够提取点云的特征的邻域池化算法</li>
<li>创新的上采样方法</li>
</ul></li>
<li>结合上述处理方法并结合 Transformer 架构的算法模型</li>
</ul>
<h3 id="related-works">Related works</h3>
<ul>
<li><p>论文中提到的点云补全的研究现状</p>
<blockquote>
<p>FoldingNet 和 AtlasNet 是最早提出利用 PointNet 提取的特征进行点云补全的两个网络，方法是将一个或多个二维网格变形为所需的形状。 PCN 在上述两个模型的基础上提出使用更小的 2D 网格进行变换以重建更精细的结构。 通过编码器-解码器架构，ASFM-Net 和 VRCNet 将编码的潜在特征与先验完成形状相匹配，从而产生良好的粗略完成结果。为了从部分扫描中保留观察到的几何形状以进行精细重建，MSN 和 VRCNet 通过使用最小密度采样 (MDS) 或距观察表面的最远点采样 (FPS) 绕过观察到的几何形状，并且建立跳过连接。通过嵌入体积子架构，GRNet 保留离散化输入几何体与体积 U 型连接，无需在点云空间中采样。 在最近提出的网络中，PMP-Net 从观察到最近的遮挡区域逐渐重建整个对象。 PoinTr 还专注于仅预测被遮挡的几何形状，它是通过将部分扫描代理转换为一组被遮挡代理以进一步细化重建来针对点云补全的前几个 transformer 方法之一。</p>
</blockquote></li>
<li><p>论文中提到的点云特征提取的研究现状</p>
<blockquote>
<p>对象点云补全中的大量工作都依赖于 PointNet 提取的特征。 PointNet 的主要优点是它能够通过最大池化实现排列不变。 然而，最大池化操作忽略了 3D 空间中的局部特征。这促使 SoftPoolNet 通过基于激活对特征向量进行排序而不是为每个元素取最大值来解决这个问题。实际上，他们能够连接特征以形成二维矩阵，以便可以应用 2D-CNN 。 除了通过池化操作构建特征表示之外，PointNet++ 对具有最远点采样 (FPS) 的点的局部子集进行采样，然后将其馈送到 PointNet。 基于 PointNet++ 的特征提取方法，SA-Net 提出将不同分辨率的特征与 KNN 分组以进行进一步处理，而 PMP-Net 使用 PointNet++ 特征来识别应该重建对象的方向。 PoinTr 还通过将输入点的位置编码添加到转换器中来解决置换不变问题而无需池化。</p>
</blockquote></li>
<li><p>总结</p>
<ol type="1">
<li>在当前的学术界，大多数基于深度学习的点云补全网络都是基于编码器-解码器架构</li>
<li>大多数点云补全网络基于 PointNet 或 PointNet++ 提取的特征进行补全</li>
<li>VRCNet 被多次提及，应提高阅读优先级，改为下一篇阅读的论文</li>
</ol></li>
</ul>
<h3 id="operators">Operators</h3>
<blockquote>
<p>编码器将输入的点云迭代下采样为其潜在特征。然后，解码器对潜伏特征进行迭代上采样以重建物体或场景。</p>
</blockquote>
<h4 id="down-sampling-operation">Down-sampling operation</h4>
<ul>
<li><p>Feature Extraction</p>
<p>首先定义输入点集为 $ _{in} $</p>
<p>给予可训练的向量集 $ $ 和针对每个输入点的权重集 $ $ ，其中向量集 $ $ 的个数为 $ s $ 个， $ s $ 为超参数</p>
<p>将输入点集中任一点 $ f $ 移动 $ _i $ ，得到新的一个点 $ f + _i $</p>
<p>在输入点集中寻找距离 $ f + _i $ 最近的点 $  $</p>
<p>计算点 $ f + _i $ 与点 $  $ 的距离并记为 $ d(f, _i) $</p>
<p>对于向量集 $ $ 中的每一个向量均有一个对应的权重 $ $ （并没有体积这些权重是否可以训练，不过根据推测，应该是可训练的）</p>
<p>然后将点 $ f $ 的 $ s $ 个 $ d(f, _i) $ 利用如下函数聚合起来，得到一个标量</p>
<p><span class="math display">\[ g(f) = \sum^{s}_{i=0} \sigma_i \tanh{\frac{\alpha}{d(f,\delta_i)+ \beta}} \]</span></p>
<p>其中的 $ $ 和 $ $ 是固定的常数，根据推测应该不是可训练的，是经验值，是超参数</p>
<p>最终，对于输入点集中的每一个点，若想输出 $ D_{out} $ 维特征，则需要有 $ D_{out} $ 组向量集 $ $</p>
<p>最终得到整个输入点集的特征</p>
<p><span class="math display">\[ \mathcal{F}_{out} = \left\{ [g_b(f_a) + h(f_a)]^{D_{out}}_{b=1} \right\} ^{|\mathcal{F}_{in}|}_{a=1} \]</span></p>
<p>其中， $ |_{in}| $ 是输入点集的点数</p>
<p>该方法受 ICP 算法的启发</p>
<p>将 ICP 算法 和 FPS 算法作为阅读完该论文的下一个学习目标</p>
<p>理解是否存在偏差，需要结合作者的代码印证</p></li>
<li><p>Neighbor pooling</p>
<p>该操作是将上一步计算的每个点激活后的 $ g(f) $ 聚合起来，再进行激活</p>
<p>目的是去除那些平均 $ g(f) $ 较大的点，使得点数减少至输入点数的 $  $</p></li>
</ul>
<h4 id="up-sampling-operation">Up-sampling operation</h4>
<p>当下采样进行到最后会得到仅剩一个点的特征</p>
<p>上采样的操作实际上是将 $ N_{up} $ 个 $ _{out} $ 的集合</p>
<h3 id="encoder-decoder-architectures">Encoder-decoder architectures</h3>
<p>论文提出了两种架构，一种架构是基于编码器-解码器，另一种架构是基于 PoinTr 派生的 Transformer</p>
<p>邻域池化相比最远点采样（ FPS ），其得到的结果更能勾勒出输入点云的轮廓</p>
<h4 id="direct-application">Direct application</h4>
<p>基于编码器-解码器的架构中</p>
<p>编码器部分实际上就是下采样操作，即特征提取和邻域池化</p>
<p>解码器部分实际上就是上采样操作</p>
<h4 id="transformers">Transformers</h4>
<p>基于 PoinTr 派生的 Transformer 的架构中</p>
<p>利用特征提取和邻域池化替换了编码器之前的下采样操作，该部分被称为 Points-to-Tokens</p>
<p>利用特征提取和上采样替换了译码器之后的上采样操作，该部分被称为 Coarse-to-Fine</p>
<h3 id="总结">总结</h3>
<p>论文主要的创新点是提出新的下采样和上采样方法</p>
<p>了解到了当前点云补全网络的主流架构和常见的组件和算法</p>
<h2 id="variational-relational-point-completion-network11">Variational Relational Point Completion Network<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></h2>
<p>论文提出了一种由两个连续的编码器-解码器子网络组成的点云补全网络模型，两个自网络分别用于“概率建模”（PMNet）和“关系增强”（RENet）。</p>
<p>第一个子网络 PMNet 采用双流网络结构，从不完整的点云中嵌入全局特征和潜在分布，并预测用作 3D 自适应锚点的整体骨架。</p>
<p>第二个子网络 RENet 努力通过学习多尺度局部点特征来增强结构关系。论文提出了点自注意内核（PSA）和点选择内核模块（PSK），以利用关系点特征，在粗略完成的条件下细化局部形状细节。</p>
<p>论文还创建了一个大型多视图的局部点云数据集，其中包含超过 100,000 个高质量扫描局部和完整点云。对于从 ShapeNet 中选择的每个完整 3D CAD 模型，我们从单位球体上均匀分布的摄像机视图中随机渲染 26 个部分点云，从而提高了数据多样性。该数据集可以作为实验的测试数据集。</p>
<h3 id="pmnet-网络">PMNet 网络</h3>
<p>PMNet 网络的两条路径输入分别为完整点云和不完整点云。输入为完整点云的路径被称为建设路径（construction path），输入为不完整点云的路径被称为补全路径（completion）。两条路径具有相似的结构，除了分布推理层之外，都共享其编码器和解码器的权重。</p>
<p>存在的问题：该子网络需要完整点云作为输入进行训练，以带动不完整点云的重建训练，但是在植株点云采样时，基于不对植株进行破坏性采样的原则，很难得到完整的被遮挡叶片的点云</p>
<h3 id="renet-网络">RENet 网络</h3>
<p>点自注意内核（PSA）能够自适应地聚合局部相邻点特征与相邻点中的学习关系。</p>
<p>点选择内核模块（PSK）能够通过利用选择性内核单元来自适应地调整它们的感受野大小。</p>
<h3 id="总结-1">总结</h3>
<p>该模型主要使用了被遮挡的点云目标的完整点云进行训练，但是在实际工作中，被遮挡点云目标的完整点云可能比较难采集到</p>
<p>因此点云补全新模型的应尽量不依赖完整点云的输入，仅用单次扫描的所有结果作为输入</p>
<h2 id="morphing-and-sampling-network-for-dense-point-cloud-completion12">Morphing and sampling network for dense point cloud completion<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></h2>
<p>论文提出了一种分两个阶段完成部分点云的新方法。具体来说，在第一阶段，该方法预测一个完整但粗粒度的点云，其中包含一组参数化表面元素。然后，在第二阶段，它通过一种新颖的采样算法将粗粒度预测与输入点云合并。</p>
<h3 id="morphing-based-prediction">Morphing-Based Prediction</h3>
<p>在粗补全的过程中，编码器借鉴了 PointNet 网络结构，然后将提取的特征输入到基于变形的解码器中，以预测连续和平滑的形状。</p>
<p>即使用多个点构成复杂平面，然后再平均采样生成平面上的点。</p>
<h3 id="merging-and-refining">Merging and Refining</h3>
<p>在细补全过程中，利用完整的点云进行残差计算，提高补全的准确率。</p>
<h2 id="grnet-gridding-residual-network-for-dense-point-cloud-completion13">GRNet: Gridding Residual Network for Dense Point Cloud Completion<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></h2>
<p>论文设计了两个新颖的可微分层，名为 Gridding 和 Gridding Reverse，以在点云和体素之间转换而不会丢失结构信息。</p>
<p>论文还提出了可区分的立方特征采样层来提取相邻点的特征，从而保留了上下文信息。</p>
<p>此外，论文设计了一种新的损失函数，即 Gridding Loss，用于计算预测点云和 ground truth 点云的 3D 网格之间的 L1 距离，这有助于恢复细节。</p>
<h3 id="gridding">Gridding</h3>
<p>在 Gridding 中，对于点云的每个点，该点所在的体素的八个顶点首先使用插值函数进行加权，该插值函数显式测量点云的几何关系。</p>
<p>然后，采用具有跳跃连接的 3D 卷积神经网络 (3D CNN) 来学习上下文感知和空间感知的特征，从而使网络能够补全不完整点云的缺失部分。</p>
<h3 id="gridding-reverse">Gridding Reverse</h3>
<p>在 Gridding Reverse 中，通过将每个体素替换为一个新点，将输出的 3D 网格转换为粗点云，该新点的坐标是体素的八个顶点的加权和。</p>
<p>下面的 Cubic Feature Sampling 通过连接点所在的体素的相应八个顶点的特征来为粗点云中的每个点提取特征。</p>
<p>粗点云和特征被转发到 MLP 以获得最终补全的点云。</p>
<h2 id="pointr-diverse-point-cloud-completion-with-geometry-aware-transformers14"><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yu_PoinTr_Diverse_Point_Cloud_Completion_With_Geometry-Aware_Transformers_ICCV_2021_paper.html">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</a><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></h2>
<p>这篇论文主要讨论无监督的点云补全任务</p>
<h3 id="创新点">创新点</h3>
<ol type="1">
<li>论文提出了一种新的点云转换方法（与 PointNet 的 T-Net 不同），将点云表示为一组具有位置嵌入的无序点组</li>
<li>论文提出了一个几何感知块，它可以显式地模拟局部几何关系</li>
<li>论文提出了基于编码器解码器架构的 PoinTr 模型</li>
</ol>
<h3 id="point-proxies">Point Proxies</h3>
<p>点代理其实是先对点云进行下采样得到中心点云集，然后利用具有分层下采样的轻量级 DGCNN 从输入点云中提取中心点云集的特征，然后将中心点云集的特征和位置信息结合起来</p>
<h3 id="geometry-aware-transformer-block">Geometry-aware Transformer Block</h3>
<p>几何感知块使用 knn 模型来捕获点云中的几何关系。</p>
<p>给定查询坐标 <span class="math inline">\(p_Q\)</span> ，我们根据键坐标 <span class="math inline">\(p_k\)</span> 查询最近键的特征。</p>
<h2 id="pcn-point-completion-network15"><a href="https://ieeexplore.ieee.org/abstract/document/8491026/">PCN: Point Completion Network</a><a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></h2>
<p>这是点云补全的第一篇工作，在此之前的3D形状不全都是以voxel的形式，而点云作为常见3D初始数据，直接在点云上做形状补全具有更大意义。</p>
<p>PCN 的基本架构也是编码器-解码器架构</p>
<h3 id="enocder">enocder</h3>
<p>编码器采用了两层叠加的 PointNet，第二层 PointNet 的 Input 是 Point Feature 和 Global Feature 的拼接</p>
<h3 id="decoder">decoder</h3>
<p>解码器部分则分为粗生成和折叠补充两个部分，粗生成就是直接经过一个 MLP，折叠补充则是在粗生成的每个点上都用多个点构成立方体替代，然后再经过 MLP 将形状进行转换</p>
<h3 id="kpi">KPI</h3>
<h4 id="chamfer-distance">Chamfer Distance</h4>
<p><span class="math display">\[CD(S_1, S_2) = \frac{1}{\lvert S_1 \rvert} \sum_{x \in S_1}{\min_{y \in S_2} \lVert x-y \rVert _2} + \frac{1}{\lvert S_2 \rvert} \sum_{y \in S_2}{\min_{x \in S_1} \lVert y-x \rVert _2}\]</span></p>
<p>倒角距离主要是计算两个点云之间各点的平均最短距离</p>
<p>计算公式的前半部分是为了让输出点云更加靠近 G.T</p>
<p>计算公式的后半部分是为了让输出点云尽可能覆盖 G.T</p>
<h4 id="earth-movers-distance">Earth Mover’s Distance</h4>
<p><span class="math display">\[EMD(S_1, S_2) =  \min_{\phi:S_1 \rightarrow S_2} \frac{1}{\lvert S_1 \rvert} \sum_{x \in S_1}{\lVert x - \phi(x) \rVert_2}\]</span></p>
<p>推土机距离则是找到一组映射关系，然后计算对应点之间的最小平均距离</p>
<h3 id="loss-函数">Loss 函数</h3>
<p><span class="math display">\[ L(Y_{coarse}, Y_{detail}, T_{gt}) = d_1(Y_{coarse}, T_{gt}) + \alpha d_2(Y_{detail}, T_{gt})\]</span></p>
<p>Loss 函数的第一部分是粗生成的点云与下采样的 G.T 的 CD 和 EMD</p>
<p>Loss 函数的第二部分是最终补全点云与 G.T 的 CD</p>
<p><span class="math inline">\(\alpha\)</span> 是超参数</p>
<h2 id="unpaired-point-cloud-completion-on-real-scans-using-adversarial-training16"><a href="https://arxiv.org/abs/1904.00069">UNPAIRED POINT CLOUD COMPLETION ON REAL SCANS USING ADVERSARIAL TRAINING</a><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h2>
<p>这篇文章首次讨论无监督的点云补全任务，很大程度上借鉴了首个点云的对抗生成网络。</p>
<p>论文提出了一种不成对的基于点云的补全方法，无需缺失点云和完整点云之间的显式对应即可进行训练。</p>
<p>在没有 G.T 的情况下，使用 plausibility scores 进行比较。</p>
<p>论文将给定噪声和缺失点云作为模型的输入</p>
<p>由于生成器借鉴了 GAN 网络，可以从噪声生成与缺失点云完全不像的完整点云，为了加以约束，还在 Loss 中添加重建损失项</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang Cui, and Zhen Li. 2022. Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 8101–8110. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00794" class="uri">https://doi.org/10.1109/CVPR52688.2022.00794</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Junzhe Zhang, Daxuan Ren, Zhongang Cai, Chai Kiat Yeo, Bo Dai, and Chen Change Loy. 2022. Monocular 3D Object Reconstruction with&nbsp;GAN Inversion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 673–689. DOI:<a href="https://doi.org/10.1007/978-3-031-19769-7_39" class="uri">https://doi.org/10.1007/978-3-031-19769-7_39</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Yu-Ting Yen, Chia-Ni Lu, Wei-Chen Chiu, and Yi-Hsuan Tsai. 2022. 3D-PL: Domain Adaptive Depth Estimation with&nbsp;3D-Aware Pseudo-Labeling. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 710–728. DOI:<a href="https://doi.org/10.1007/978-3-031-19812-0_41" class="uri">https://doi.org/10.1007/978-3-031-19812-0_41</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Zhiqiang Yan, Kun Wang, Xiang Li, Zhenyu Zhang, Jun Li, and Jian Yang. 2022. RigNet: Repetitive Image Guided Network for&nbsp;Depth Completion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 214–230. DOI:<a href="https://doi.org/10.1007/978-3-031-19812-0_13" class="uri">https://doi.org/10.1007/978-3-031-19812-0_13</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Zhiqiang Yan, Xiang Li, Kun Wang, Zhenyu Zhang, Jun Li, and Jian Yang. 2022. Multi-modal Masked Pre-training for&nbsp;Monocular Panoramic Depth Completion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 378–395. DOI:<a href="https://doi.org/10.1007/978-3-031-19769-7_22" class="uri">https://doi.org/10.1007/978-3-031-19769-7_22</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2022. ShapeFormer: Transformer-based Shape Completion via Sparse Representation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 6229–6239. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00614" class="uri">https://doi.org/10.1109/CVPR52688.2022.00614</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Ryuki Yamamoto, Hidekata Hontani, Akira Imakura, and Tatsuya Yokota. 2022. Fast Algorithm for Low-rank Tensor Completion in Delay-embedded Space. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 2048–2056. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00210" class="uri">https://doi.org/10.1109/CVPR52688.2022.00210</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng Liu, and Deng Cai. 2022. Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 5408–5417. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00534" class="uri">https://doi.org/10.1109/CVPR52688.2022.00534</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Xin Wen, Junsheng Zhou, Yu-Shen Liu, Hua Su, Zhen Dong, and Zhizhong Han. 2022. 3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 3793–3803. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00378" class="uri">https://doi.org/10.1109/CVPR52688.2022.00378</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, and Ziwei Liu. 2021. Variational Relational Point Completion Network. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Nashville, TN, USA, 8520–8529. DOI:<a href="https://doi.org/10.1109/CVPR46437.2021.00842" class="uri">https://doi.org/10.1109/CVPR46437.2021.00842</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Yida Wang, David Joseph Tan, Nassir Navab, and Federico Tombari. 2022. Learning Local Displacements for Point Cloud Completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 1558–1567. DOI:<a href="https://doi.org/10.1109/CVPR52688.2022.00162" class="uri">https://doi.org/10.1109/CVPR52688.2022.00162</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, and Ziwei Liu. 2021. Variational Relational Point Completion Network. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Nashville, TN, USA, 8520–8529. DOI:<a href="https://doi.org/10.1109/CVPR46437.2021.00842" class="uri">https://doi.org/10.1109/CVPR46437.2021.00842</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-Min Hu. 2020. Morphing and Sampling Network for Dense Point Cloud Completion. AAAI 34, 07 (April 2020), 11596–11603. DOI:<a href="https://doi.org/10.1609/aaai.v34i07.6827" class="uri">https://doi.org/10.1609/aaai.v34i07.6827</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang, and Wenxiu Sun. 2020. GRNet: Gridding Residual Network for Dense Point Cloud Completion. In Computer Vision – ECCV 2020 (Lecture Notes in Computer Science), Springer International Publishing, Cham, 365–381. DOI:<a href="https://doi.org/10.1007/978-3-030-58545-7_21" class="uri">https://doi.org/10.1007/978-3-030-58545-7_21</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. 2021. PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE, Montreal, QC, Canada, 12478–12487. DOI:<a href="https://doi.org/10.1109/ICCV48922.2021.01227" class="uri">https://doi.org/10.1109/ICCV48922.2021.01227</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert. 2018. PCN: Point Completion Network. In 2018 International Conference on 3D Vision (3DV), 728–737. DOI:<a href="https://doi.org/10.1109/3DV.2018.00088" class="uri">https://doi.org/10.1109/3DV.2018.00088</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>Xuelin Chen, Baoquan Chen, and Niloy J. Mitra. 2020. Unpaired Point Cloud Completion on Real Scans using Adversarial Training. DOI:<a href="https://doi.org/10.48550/arXiv.1904.00069" class="uri">https://doi.org/10.48550/arXiv.1904.00069</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
      <categories>
        <category>技术</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
</search>
