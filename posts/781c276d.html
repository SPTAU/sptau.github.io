<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic%7CLobster+Two:300,300italic,400,400italic,700,700italic%7CRoboto+Slab:300,300italic,400,400italic,700,700italic%7CNoto+Sans:300,300italic,400,400italic,700,700italic%7CJetBrains+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"sptau.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="阅读 2017-2023 年的 ICCV 、 CVPR 、 ECCV 等会议论文集中以补全 (Completion) 为关键词的论文"><meta property="og:type" content="article"><meta property="og:title" content="点云补全论文阅读总结"><meta property="og:url" content="http://sptau.github.io/posts/781c276d.html"><meta property="og:site_name" content="巽离阁"><meta property="og:description" content="阅读 2017-2023 年的 ICCV 、 CVPR 、 ECCV 等会议论文集中以补全 (Completion) 为关键词的论文"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-03-06T14:35:16.000Z"><meta property="article:modified_time" content="2023-11-24T14:50:07.163Z"><meta property="article:author" content="SPTAU"><meta property="article:tag" content="论文"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://sptau.github.io/posts/781c276d.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://sptau.github.io/posts/781c276d.html","path":"posts/781c276d.html","title":"点云补全论文阅读总结"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>点云补全论文阅读总结 | 巽离阁</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="巽离阁" type="application/atom+xml"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">巽离阁</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">非生来富贵 是天道酬勤</p></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds1"><span class="nav-text">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#monocular-3d-object-reconstruction-with-gan-inversion2"><span class="nav-text">Monocular 3D Object Reconstruction with GAN Inversion2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-pl-domain-adaptive-depth-estimation-with-3d-aware-pseudo-labeling3"><span class="nav-text">3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rignet-repetitive-image-guided-network-for-depth-completion4"><span class="nav-text">RigNet: Repetitive Image Guided Network for Depth Completion4</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multi-modal-masked-pre-training-for-monocular-panoramic-depth-completion5"><span class="nav-text">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shapeformer-transformer-based-shape-completion-via-sparse-representation6"><span class="nav-text">ShapeFormer: Transformer-based Shape Completion via Sparse Representation6</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fast-algorithm-for-low-rank-tensor-completion-in-delay-embedded-space7"><span class="nav-text">Fast Algorithm for Low-rank Tensor Completion in Delay-embedded Space7</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sparse-fuse-dense-towards-high-quality-3d-detection-with-depth-completion8"><span class="nav-text">Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion8</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-shape-reconstruction-from-2d-images-with-disentangled-attribute-flow9"><span class="nav-text">3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow9</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-local-displacements-for-point-cloud-completion10"><span class="nav-text">Learning Local Displacements for Point Cloud Completion11</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#introduction-conclusion"><span class="nav-text">Introduction &amp; Conclusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#related-works"><span class="nav-text">Related works</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#operators"><span class="nav-text">Operators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#down-sampling-operation"><span class="nav-text">Down-sampling operation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#up-sampling-operation"><span class="nav-text">Up-sampling operation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-decoder-architectures"><span class="nav-text">Encoder-decoder architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#direct-application"><span class="nav-text">Direct application</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transformers"><span class="nav-text">Transformers</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#variational-relational-point-completion-network11"><span class="nav-text">Variational Relational Point Completion Network12</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pmnet-%E7%BD%91%E7%BB%9C"><span class="nav-text">PMNet 网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#renet-%E7%BD%91%E7%BB%9C"><span class="nav-text">RENet 网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#morphing-and-sampling-network-for-dense-point-cloud-completion12"><span class="nav-text">Morphing and sampling network for dense point cloud completion13</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#morphing-based-prediction"><span class="nav-text">Morphing-Based Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#merging-and-refining"><span class="nav-text">Merging and Refining</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#grnet-gridding-residual-network-for-dense-point-cloud-completion13"><span class="nav-text">GRNet: Gridding Residual Network for Dense Point Cloud Completion14</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gridding"><span class="nav-text">Gridding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gridding-reverse"><span class="nav-text">Gridding Reverse</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pointr-diverse-point-cloud-completion-with-geometry-aware-transformers14"><span class="nav-text">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers15</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="nav-text">创新点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#point-proxies"><span class="nav-text">Point Proxies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#geometry-aware-transformer-block"><span class="nav-text">Geometry-aware Transformer Block</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pcn-point-completion-network15"><span class="nav-text">PCN: Point Completion Network16</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#enocder"><span class="nav-text">enocder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder"><span class="nav-text">decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kpi"><span class="nav-text">KPI</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#chamfer-distance"><span class="nav-text">Chamfer Distance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#earth-movers-distance"><span class="nav-text">Earth Mover’s Distance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-%E5%87%BD%E6%95%B0"><span class="nav-text">Loss 函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unpaired-point-cloud-completion-on-real-scans-using-adversarial-training16"><span class="nav-text">UNPAIRED POINT CLOUD COMPLETION ON REAL SCANS USING ADVERSARIAL TRAINING17</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="SPTAU" src="/images/avatar.png"><p class="site-author-name" itemprop="name">SPTAU</p><div class="site-description" itemprop="description">江湖行走 初心依旧 来年还乡 一梦千秋</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/SPTAU" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SPTAU" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:sptau@qq.com" title="E-Mail → mailto:sptau@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/1601578" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;1601578" rel="noopener me" target="_blank"><i class="fa-brands fa-bilibili fa-fw"></i>Bilibili</a></span></div></div></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> 链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://blinkfox.github.io/" title="https:&#x2F;&#x2F;blinkfox.github.io" rel="noopener" target="_blank">闪烁之狐 - blinkfox</a></li><li class="links-of-blogroll-item"><a href="https://www.haomwei.com/" title="https:&#x2F;&#x2F;www.haomwei.com&#x2F;" rel="noopener" target="_blank">屠·城 - 屠夫9441</a></li><li class="links-of-blogroll-item"><a href="http://yearito.cn/" title="http:&#x2F;&#x2F;yearito.cn&#x2F;" rel="noopener" target="_blank">Yearito's Blog - yearito</a></li><li class="links-of-blogroll-item"><a href="https://io-oi.me/" title="https:&#x2F;&#x2F;io-oi.me&#x2F;" rel="noopener" target="_blank">reuixiy's Blog - reuixiy</a></li><li class="links-of-blogroll-item"><a href="https://www.yuque.com/cbyd/chengbao" title="https:&#x2F;&#x2F;www.yuque.com&#x2F;cbyd&#x2F;chengbao" rel="noopener" target="_blank">城堡阅读杂志 - 南百城</a></li></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://sptau.github.io/posts/781c276d.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="SPTAU"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="巽离阁"><meta itemprop="description" content="江湖行走 初心依旧 来年还乡 一梦千秋"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="点云补全论文阅读总结 | 巽离阁"><meta itemprop="description" content="阅读 2017-2023 年的 ICCV 、 CVPR 、 ECCV 等会议论文集中以补全 (Completion) 为关键词的论文"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">点云补全论文阅读总结</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-03-06 22:35:16" itemprop="dateCreated datePublished" datetime="2023-03-06T22:35:16+08:00">2023-03-06</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-11-24 22:50:07" itemprop="dateModified" datetime="2023-11-24T22:50:07+08:00">2023-11-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%8A%80%E6%9C%AF/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>11k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>10 分钟</span></span></div><div class="post-description">阅读 2017-2023 年的 ICCV 、 CVPR 、 ECCV 等会议论文集中以补全 (Completion) 为关键词的论文</div></div></header><div class="post-body" itemprop="articleBody"><h2 id="beyond-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds1">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h2><p>论文提出了一种针对 LiDAR 雷达获得的存在运动物体场景的目标跟踪方法</p><p>论文并不是纯粹点云补全，而是针对运动中的物体（即在场景中发生刚性变换的物体）进行补全，略过</p><h2 id="monocular-3d-object-reconstruction-with-gan-inversion2">Monocular 3D Object Reconstruction with GAN Inversion<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h2><p>论文提出了一种基于 GAN 网络的由图像生成具有相似纹理特征的 Mesh 方法</p><p>略过</p><h2 id="d-pl-domain-adaptive-depth-estimation-with-3d-aware-pseudo-labeling3">3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h2><p>论文提出了一种利用二维图像和三维点云进行标注深度图像数据集的单目图像深度估计方法</p><p>论文并不是关于点云补全，而是针对深度图进行补全，略过</p><h2 id="rignet-repetitive-image-guided-network-for-depth-completion4">RigNet: Repetitive Image Guided Network for Depth Completion<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h2><p>论文提出了一种基于多尺度的稀疏深度图补全方法</p><p>论文并不是关于点云补全，而是针对深度图进行补全，略过</p><h2 id="multi-modal-masked-pre-training-for-monocular-panoramic-depth-completion5">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></h2><p>论文提出了一种使用随机 Mask 训练的单目 360° 全景图片的稀疏深度图补全方法</p><p>论文并不是纯粹点云补全，针对全景图片的点云进行补全，略过</p><h2 id="shapeformer-transformer-based-shape-completion-via-sparse-representation6">ShapeFormer: Transformer-based Shape Completion via Sparse Representation<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></h2><p>论文提出了一种基于注意力机制和点云体素化表示编码器解码器 (VQDIF) 的补全 / GAN 方法，输入为缺失的点云，转换成 DIF 表示后进行补全 / 生成，最后再转换为 Mesh 输出</p><ul><li>深度隐函数 DIF</li></ul><blockquote><p>与图像补全不同，在 3D 形状补全中，输入也可能含有噪声，若是完整保留输入必然会产生嘈杂的结果</p></blockquote><p>潜在的研究方向，可以放着，但是并不是纯点云研究，优先级并不高</p><h2 id="fast-algorithm-for-low-rank-tensor-completion-in-delay-embedded-space7">Fast Algorithm for Low-rank Tensor Completion in Delay-embedded Space<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></h2><p>论文提出了一种利用多路延迟嵌入变换实现张量 / 图像补全的方法</p><p>论文并不是纯粹点云补全，略过</p><h2 id="sparse-fuse-dense-towards-high-quality-3d-detection-with-depth-completion8">Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></h2><p>论文提出了一种利用 LiDAR 稀疏点云和二维彩色图像的多模态融合实现目标检测的方法，并没有涉及到补全部分</p><p>论文并不是纯粹点云补全，而是尝试使用多模态进行补全，略过</p><h2 id="d-shape-reconstruction-from-2d-images-with-disentangled-attribute-flow9">3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></h2><p>论文提出了一种基于 Attribute Flow 模型实现从单张二维 RGB 图像重建为 3D 点云的方法</p><p>但在论文中也提到该方法可以用在点云补全，但是文中并没有详细说明如何修改网络，而是说参照 VRCNet<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> 的设置，VRCNet 在待阅读论文中，将详细查看具体方法</p><p>暂留</p><p>[ ] VRCNet 补充引用</p><h2 id="learning-local-displacements-for-point-cloud-completion10">Learning Local Displacements for Point Cloud Completion<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></h2><h3 id="introduction-conclusion">Introduction &amp; Conclusion</h3><p>论文提出了以下内容</p><ul><li>基于编码器-解码器架构的对象点云补全和场景点云补全算法模型<ul><li>针对局部点云的特征提取方法</li><li>能够提取点云的特征的邻域池化算法</li><li>创新的上采样方法</li></ul></li><li>结合上述处理方法并结合 Transformer 架构的算法模型</li></ul><h3 id="related-works">Related works</h3><ul><li><p>论文中提到的点云补全的研究现状</p><blockquote><p>FoldingNet 和 AtlasNet 是最早提出利用 PointNet 提取的特征进行点云补全的两个网络，方法是将一个或多个二维网格变形为所需的形状。 PCN 在上述两个模型的基础上提出使用更小的 2D 网格进行变换以重建更精细的结构。 通过编码器-解码器架构，ASFM-Net 和 VRCNet 将编码的潜在特征与先验完成形状相匹配，从而产生良好的粗略完成结果。为了从部分扫描中保留观察到的几何形状以进行精细重建，MSN 和 VRCNet 通过使用最小密度采样 (MDS) 或距观察表面的最远点采样 (FPS) 绕过观察到的几何形状，并且建立跳过连接。通过嵌入体积子架构，GRNet 保留离散化输入几何体与体积 U 型连接，无需在点云空间中采样。 在最近提出的网络中，PMP-Net 从观察到最近的遮挡区域逐渐重建整个对象。 PoinTr 还专注于仅预测被遮挡的几何形状，它是通过将部分扫描代理转换为一组被遮挡代理以进一步细化重建来针对点云补全的前几个 transformer 方法之一。</p></blockquote></li><li><p>论文中提到的点云特征提取的研究现状</p><blockquote><p>对象点云补全中的大量工作都依赖于 PointNet 提取的特征。 PointNet 的主要优点是它能够通过最大池化实现排列不变。 然而，最大池化操作忽略了 3D 空间中的局部特征。这促使 SoftPoolNet 通过基于激活对特征向量进行排序而不是为每个元素取最大值来解决这个问题。实际上，他们能够连接特征以形成二维矩阵，以便可以应用 2D-CNN 。 除了通过池化操作构建特征表示之外，PointNet++ 对具有最远点采样 (FPS) 的点的局部子集进行采样，然后将其馈送到 PointNet。 基于 PointNet++ 的特征提取方法，SA-Net 提出将不同分辨率的特征与 KNN 分组以进行进一步处理，而 PMP-Net 使用 PointNet++ 特征来识别应该重建对象的方向。 PoinTr 还通过将输入点的位置编码添加到转换器中来解决置换不变问题而无需池化。</p></blockquote></li><li><p>总结</p><ol type="1"><li>在当前的学术界，大多数基于深度学习的点云补全网络都是基于编码器-解码器架构</li><li>大多数点云补全网络基于 PointNet 或 PointNet++ 提取的特征进行补全</li><li>VRCNet 被多次提及，应提高阅读优先级，改为下一篇阅读的论文</li></ol></li></ul><h3 id="operators">Operators</h3><blockquote><p>编码器将输入的点云迭代下采样为其潜在特征。然后，解码器对潜伏特征进行迭代上采样以重建物体或场景。</p></blockquote><h4 id="down-sampling-operation">Down-sampling operation</h4><ul><li><p>Feature Extraction</p><p>首先定义输入点集为 $ _{in} $</p><p>给予可训练的向量集 $ $ 和针对每个输入点的权重集 $ $ ，其中向量集 $ $ 的个数为 $ s $ 个， $ s $ 为超参数</p><p>将输入点集中任一点 $ f $ 移动 $ _i $ ，得到新的一个点 $ f + _i $</p><p>在输入点集中寻找距离 $ f + _i $ 最近的点 $ $</p><p>计算点 $ f + _i $ 与点 $ $ 的距离并记为 $ d(f, _i) $</p><p>对于向量集 $ $ 中的每一个向量均有一个对应的权重 $ $ （并没有体积这些权重是否可以训练，不过根据推测，应该是可训练的）</p><p>然后将点 $ f $ 的 $ s $ 个 $ d(f, _i) $ 利用如下函数聚合起来，得到一个标量</p><p><span class="math display">\[ g(f) = \sum^{s}_{i=0} \sigma_i \tanh{\frac{\alpha}{d(f,\delta_i)+ \beta}} \]</span></p><p>其中的 $ $ 和 $ $ 是固定的常数，根据推测应该不是可训练的，是经验值，是超参数</p><p>最终，对于输入点集中的每一个点，若想输出 $ D_{out} $ 维特征，则需要有 $ D_{out} $ 组向量集 $ $</p><p>最终得到整个输入点集的特征</p><p><span class="math display">\[ \mathcal{F}_{out} = \left\{ [g_b(f_a) + h(f_a)]^{D_{out}}_{b=1} \right\} ^{|\mathcal{F}_{in}|}_{a=1} \]</span></p><p>其中， $ |_{in}| $ 是输入点集的点数</p><p>该方法受 ICP 算法的启发</p><p>将 ICP 算法 和 FPS 算法作为阅读完该论文的下一个学习目标</p><p>理解是否存在偏差，需要结合作者的代码印证</p></li><li><p>Neighbor pooling</p><p>该操作是将上一步计算的每个点激活后的 $ g(f) $ 聚合起来，再进行激活</p><p>目的是去除那些平均 $ g(f) $ 较大的点，使得点数减少至输入点数的 $ $</p></li></ul><h4 id="up-sampling-operation">Up-sampling operation</h4><p>当下采样进行到最后会得到仅剩一个点的特征</p><p>上采样的操作实际上是将 $ N_{up} $ 个 $ _{out} $ 的集合</p><h3 id="encoder-decoder-architectures">Encoder-decoder architectures</h3><p>论文提出了两种架构，一种架构是基于编码器-解码器，另一种架构是基于 PoinTr 派生的 Transformer</p><p>邻域池化相比最远点采样（ FPS ），其得到的结果更能勾勒出输入点云的轮廓</p><h4 id="direct-application">Direct application</h4><p>基于编码器-解码器的架构中</p><p>编码器部分实际上就是下采样操作，即特征提取和邻域池化</p><p>解码器部分实际上就是上采样操作</p><h4 id="transformers">Transformers</h4><p>基于 PoinTr 派生的 Transformer 的架构中</p><p>利用特征提取和邻域池化替换了编码器之前的下采样操作，该部分被称为 Points-to-Tokens</p><p>利用特征提取和上采样替换了译码器之后的上采样操作，该部分被称为 Coarse-to-Fine</p><h3 id="总结">总结</h3><p>论文主要的创新点是提出新的下采样和上采样方法</p><p>了解到了当前点云补全网络的主流架构和常见的组件和算法</p><h2 id="variational-relational-point-completion-network11">Variational Relational Point Completion Network<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></h2><p>论文提出了一种由两个连续的编码器-解码器子网络组成的点云补全网络模型，两个自网络分别用于“概率建模”（PMNet）和“关系增强”（RENet）。</p><p>第一个子网络 PMNet 采用双流网络结构，从不完整的点云中嵌入全局特征和潜在分布，并预测用作 3D 自适应锚点的整体骨架。</p><p>第二个子网络 RENet 努力通过学习多尺度局部点特征来增强结构关系。论文提出了点自注意内核（PSA）和点选择内核模块（PSK），以利用关系点特征，在粗略完成的条件下细化局部形状细节。</p><p>论文还创建了一个大型多视图的局部点云数据集，其中包含超过 100,000 个高质量扫描局部和完整点云。对于从 ShapeNet 中选择的每个完整 3D CAD 模型，我们从单位球体上均匀分布的摄像机视图中随机渲染 26 个部分点云，从而提高了数据多样性。该数据集可以作为实验的测试数据集。</p><h3 id="pmnet-网络">PMNet 网络</h3><p>PMNet 网络的两条路径输入分别为完整点云和不完整点云。输入为完整点云的路径被称为建设路径（construction path），输入为不完整点云的路径被称为补全路径（completion）。两条路径具有相似的结构，除了分布推理层之外，都共享其编码器和解码器的权重。</p><p>存在的问题：该子网络需要完整点云作为输入进行训练，以带动不完整点云的重建训练，但是在植株点云采样时，基于不对植株进行破坏性采样的原则，很难得到完整的被遮挡叶片的点云</p><h3 id="renet-网络">RENet 网络</h3><p>点自注意内核（PSA）能够自适应地聚合局部相邻点特征与相邻点中的学习关系。</p><p>点选择内核模块（PSK）能够通过利用选择性内核单元来自适应地调整它们的感受野大小。</p><h3 id="总结-1">总结</h3><p>该模型主要使用了被遮挡的点云目标的完整点云进行训练，但是在实际工作中，被遮挡点云目标的完整点云可能比较难采集到</p><p>因此点云补全新模型的应尽量不依赖完整点云的输入，仅用单次扫描的所有结果作为输入</p><h2 id="morphing-and-sampling-network-for-dense-point-cloud-completion12">Morphing and sampling network for dense point cloud completion<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></h2><p>论文提出了一种分两个阶段完成部分点云的新方法。具体来说，在第一阶段，该方法预测一个完整但粗粒度的点云，其中包含一组参数化表面元素。然后，在第二阶段，它通过一种新颖的采样算法将粗粒度预测与输入点云合并。</p><h3 id="morphing-based-prediction">Morphing-Based Prediction</h3><p>在粗补全的过程中，编码器借鉴了 PointNet 网络结构，然后将提取的特征输入到基于变形的解码器中，以预测连续和平滑的形状。</p><p>即使用多个点构成复杂平面，然后再平均采样生成平面上的点。</p><h3 id="merging-and-refining">Merging and Refining</h3><p>在细补全过程中，利用完整的点云进行残差计算，提高补全的准确率。</p><h2 id="grnet-gridding-residual-network-for-dense-point-cloud-completion13">GRNet: Gridding Residual Network for Dense Point Cloud Completion<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></h2><p>论文设计了两个新颖的可微分层，名为 Gridding 和 Gridding Reverse，以在点云和体素之间转换而不会丢失结构信息。</p><p>论文还提出了可区分的立方特征采样层来提取相邻点的特征，从而保留了上下文信息。</p><p>此外，论文设计了一种新的损失函数，即 Gridding Loss，用于计算预测点云和 ground truth 点云的 3D 网格之间的 L1 距离，这有助于恢复细节。</p><h3 id="gridding">Gridding</h3><p>在 Gridding 中，对于点云的每个点，该点所在的体素的八个顶点首先使用插值函数进行加权，该插值函数显式测量点云的几何关系。</p><p>然后，采用具有跳跃连接的 3D 卷积神经网络 (3D CNN) 来学习上下文感知和空间感知的特征，从而使网络能够补全不完整点云的缺失部分。</p><h3 id="gridding-reverse">Gridding Reverse</h3><p>在 Gridding Reverse 中，通过将每个体素替换为一个新点，将输出的 3D 网格转换为粗点云，该新点的坐标是体素的八个顶点的加权和。</p><p>下面的 Cubic Feature Sampling 通过连接点所在的体素的相应八个顶点的特征来为粗点云中的每个点提取特征。</p><p>粗点云和特征被转发到 MLP 以获得最终补全的点云。</p><h2 id="pointr-diverse-point-cloud-completion-with-geometry-aware-transformers14"><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Yu_PoinTr_Diverse_Point_Cloud_Completion_With_Geometry-Aware_Transformers_ICCV_2021_paper.html">PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</a><a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></h2><p>这篇论文主要讨论无监督的点云补全任务</p><h3 id="创新点">创新点</h3><ol type="1"><li>论文提出了一种新的点云转换方法（与 PointNet 的 T-Net 不同），将点云表示为一组具有位置嵌入的无序点组</li><li>论文提出了一个几何感知块，它可以显式地模拟局部几何关系</li><li>论文提出了基于编码器解码器架构的 PoinTr 模型</li></ol><h3 id="point-proxies">Point Proxies</h3><p>点代理其实是先对点云进行下采样得到中心点云集，然后利用具有分层下采样的轻量级 DGCNN 从输入点云中提取中心点云集的特征，然后将中心点云集的特征和位置信息结合起来</p><h3 id="geometry-aware-transformer-block">Geometry-aware Transformer Block</h3><p>几何感知块使用 knn 模型来捕获点云中的几何关系。</p><p>给定查询坐标 <span class="math inline">\(p_Q\)</span> ，我们根据键坐标 <span class="math inline">\(p_k\)</span> 查询最近键的特征。</p><h2 id="pcn-point-completion-network15"><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8491026/">PCN: Point Completion Network</a><a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></h2><p>这是点云补全的第一篇工作，在此之前的3D形状不全都是以voxel的形式，而点云作为常见3D初始数据，直接在点云上做形状补全具有更大意义。</p><p>PCN 的基本架构也是编码器-解码器架构</p><h3 id="enocder">enocder</h3><p>编码器采用了两层叠加的 PointNet，第二层 PointNet 的 Input 是 Point Feature 和 Global Feature 的拼接</p><h3 id="decoder">decoder</h3><p>解码器部分则分为粗生成和折叠补充两个部分，粗生成就是直接经过一个 MLP，折叠补充则是在粗生成的每个点上都用多个点构成立方体替代，然后再经过 MLP 将形状进行转换</p><h3 id="kpi">KPI</h3><h4 id="chamfer-distance">Chamfer Distance</h4><p><span class="math display">\[CD(S_1, S_2) = \frac{1}{\lvert S_1 \rvert} \sum_{x \in S_1}{\min_{y \in S_2} \lVert x-y \rVert _2} + \frac{1}{\lvert S_2 \rvert} \sum_{y \in S_2}{\min_{x \in S_1} \lVert y-x \rVert _2}\]</span></p><p>倒角距离主要是计算两个点云之间各点的平均最短距离</p><p>计算公式的前半部分是为了让输出点云更加靠近 G.T</p><p>计算公式的后半部分是为了让输出点云尽可能覆盖 G.T</p><h4 id="earth-movers-distance">Earth Mover’s Distance</h4><p><span class="math display">\[EMD(S_1, S_2) = \min_{\phi:S_1 \rightarrow S_2} \frac{1}{\lvert S_1 \rvert} \sum_{x \in S_1}{\lVert x - \phi(x) \rVert_2}\]</span></p><p>推土机距离则是找到一组映射关系，然后计算对应点之间的最小平均距离</p><h3 id="loss-函数">Loss 函数</h3><p><span class="math display">\[ L(Y_{coarse}, Y_{detail}, T_{gt}) = d_1(Y_{coarse}, T_{gt}) + \alpha d_2(Y_{detail}, T_{gt})\]</span></p><p>Loss 函数的第一部分是粗生成的点云与下采样的 G.T 的 CD 和 EMD</p><p>Loss 函数的第二部分是最终补全点云与 G.T 的 CD</p><p><span class="math inline">\(\alpha\)</span> 是超参数</p><h2 id="unpaired-point-cloud-completion-on-real-scans-using-adversarial-training16"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00069">UNPAIRED POINT CLOUD COMPLETION ON REAL SCANS USING ADVERSARIAL TRAINING</a><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h2><p>这篇文章首次讨论无监督的点云补全任务，很大程度上借鉴了首个点云的对抗生成网络。</p><p>论文提出了一种不成对的基于点云的补全方法，无需缺失点云和完整点云之间的显式对应即可进行训练。</p><p>在没有 G.T 的情况下，使用 plausibility scores 进行比较。</p><p>论文将给定噪声和缺失点云作为模型的输入</p><p>由于生成器借鉴了 GAN 网络，可以从噪声生成与缺失点云完全不像的完整点云，为了加以约束，还在 Loss 中添加重建损失项</p><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang Cui, and Zhen Li. 2022. Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 8101–8110. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00794" class="uri">https://doi.org/10.1109/CVPR52688.2022.00794</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2" role="doc-endnote"><p>Junzhe Zhang, Daxuan Ren, Zhongang Cai, Chai Kiat Yeo, Bo Dai, and Chen Change Loy. 2022. Monocular 3D Object Reconstruction with&nbsp;GAN Inversion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 673–689. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-19769-7_39" class="uri">https://doi.org/10.1007/978-3-031-19769-7_39</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3" role="doc-endnote"><p>Yu-Ting Yen, Chia-Ni Lu, Wei-Chen Chiu, and Yi-Hsuan Tsai. 2022. 3D-PL: Domain Adaptive Depth Estimation with&nbsp;3D-Aware Pseudo-Labeling. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 710–728. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-19812-0_41" class="uri">https://doi.org/10.1007/978-3-031-19812-0_41</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4" role="doc-endnote"><p>Zhiqiang Yan, Kun Wang, Xiang Li, Zhenyu Zhang, Jun Li, and Jian Yang. 2022. RigNet: Repetitive Image Guided Network for&nbsp;Depth Completion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 214–230. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-19812-0_13" class="uri">https://doi.org/10.1007/978-3-031-19812-0_13</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5" role="doc-endnote"><p>Zhiqiang Yan, Xiang Li, Kun Wang, Zhenyu Zhang, Jun Li, and Jian Yang. 2022. Multi-modal Masked Pre-training for&nbsp;Monocular Panoramic Depth Completion. In Computer Vision – ECCV 2022 (Lecture Notes in Computer Science), Springer Nature Switzerland, Cham, 378–395. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-031-19769-7_22" class="uri">https://doi.org/10.1007/978-3-031-19769-7_22</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn6" role="doc-endnote"><p>Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2022. ShapeFormer: Transformer-based Shape Completion via Sparse Representation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 6229–6239. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00614" class="uri">https://doi.org/10.1109/CVPR52688.2022.00614</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn7" role="doc-endnote"><p>Ryuki Yamamoto, Hidekata Hontani, Akira Imakura, and Tatsuya Yokota. 2022. Fast Algorithm for Low-rank Tensor Completion in Delay-embedded Space. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 2048–2056. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00210" class="uri">https://doi.org/10.1109/CVPR52688.2022.00210</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn8" role="doc-endnote"><p>Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng, Haifeng Liu, and Deng Cai. 2022. Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 5408–5417. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00534" class="uri">https://doi.org/10.1109/CVPR52688.2022.00534</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn9" role="doc-endnote"><p>Xin Wen, Junsheng Zhou, Yu-Shen Liu, Hua Su, Zhen Dong, and Zhizhong Han. 2022. 3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 3793–3803. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00378" class="uri">https://doi.org/10.1109/CVPR52688.2022.00378</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn10" role="doc-endnote"><p>Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, and Ziwei Liu. 2021. Variational Relational Point Completion Network. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Nashville, TN, USA, 8520–8529. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR46437.2021.00842" class="uri">https://doi.org/10.1109/CVPR46437.2021.00842</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn11" role="doc-endnote"><p>Yida Wang, David Joseph Tan, Nassir Navab, and Federico Tombari. 2022. Learning Local Displacements for Point Cloud Completion. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New Orleans, LA, USA, 1558–1567. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR52688.2022.00162" class="uri">https://doi.org/10.1109/CVPR52688.2022.00162</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn12" role="doc-endnote"><p>Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai Yi, and Ziwei Liu. 2021. Variational Relational Point Completion Network. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Nashville, TN, USA, 8520–8529. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/CVPR46437.2021.00842" class="uri">https://doi.org/10.1109/CVPR46437.2021.00842</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn13" role="doc-endnote"><p>Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-Min Hu. 2020. Morphing and Sampling Network for Dense Point Cloud Completion. AAAI 34, 07 (April 2020), 11596–11603. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1609/aaai.v34i07.6827" class="uri">https://doi.org/10.1609/aaai.v34i07.6827</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn14" role="doc-endnote"><p>Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang, and Wenxiu Sun. 2020. GRNet: Gridding Residual Network for Dense Point Cloud Completion. In Computer Vision – ECCV 2020 (Lecture Notes in Computer Science), Springer International Publishing, Cham, 365–381. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-3-030-58545-7_21" class="uri">https://doi.org/10.1007/978-3-030-58545-7_21</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn15" role="doc-endnote"><p>Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. 2021. PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE, Montreal, QC, Canada, 12478–12487. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV48922.2021.01227" class="uri">https://doi.org/10.1109/ICCV48922.2021.01227</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn16" role="doc-endnote"><p>Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert. 2018. PCN: Point Completion Network. In 2018 International Conference on 3D Vision (3DV), 728–737. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/3DV.2018.00088" class="uri">https://doi.org/10.1109/3DV.2018.00088</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn17" role="doc-endnote"><p>Xuelin Chen, Baoquan Chen, and Niloy J. Mitra. 2020. Unpaired Point Cloud Completion on Real Scans using Adversarial Training. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.1904.00069" class="uri">https://doi.org/10.48550/arXiv.1904.00069</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section></div><footer class="post-footer"><script src="//sdk.jinrishici.com/v2/browser/jinrishici.js"></script><script>jinrishici.load((result) => {
    let jrsc = document.getElementById('jrsc');
    const data = result.data;
    let author = data.origin.author;
    let title = '《' + data.origin.title + '》';
    let content = data.content.substr(0, data.content.length - 1);
    let dynasty = data.origin.dynasty.substr(0, data.origin.dynasty.length - 1);
    jrsc.innerText = content + ' @ ' + dynasty + '·' + author + title;
  });</script><div style="text-align:center"><span id="jrsc">正在加载今日诗词....</span></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>SPTAU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://sptau.github.io/posts/781c276d.html" title="点云补全论文阅读总结">http://sptau.github.io/posts/781c276d.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"><i class="fa fa-tag"></i> 论文</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/c254b0.html" rel="prev" title="ICCV 2017 扫读总结"><i class="fa fa-angle-left"></i> ICCV 2017 扫读总结</a></div><div class="post-nav-item"><a href="/posts/98f1e749.html" rel="next" title="CNN 参数更新">CNN 参数更新 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments gitalk-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2022 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">SPTAU</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span title="站点总字数">86k</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script><script src="/js/third-party/tags/mermaid.js"></script><script src="/js/third-party/fancybox.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script><script>var options = {
  bottom: '32px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous"><script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"SPTAU","repo":"SPTAU.github.io","client_id":"88f04708a8ad279abe57","client_secret":"f9f4099eb59d8ab8522197e56437b52ad9048c0e","admin_user":"SPTAU","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"5c32eca61ee3856e1ef2c555fdcb135e"}</script><script src="/js/third-party/comments/gitalk.js"></script><script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script><script>const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'SPTAU.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+href+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });</script></body></html>