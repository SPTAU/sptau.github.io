<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic%7CLobster+Two:300,300italic,400,400italic,700,700italic%7CRoboto+Slab:300,300italic,400,400italic,700,700italic%7CNoto+Sans:300,300italic,400,400italic,700,700italic%7CJetBrains+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.css" integrity="sha256-vQkngPS8jiHHH0I6ABTZroZk8NPZ7b+MUReOFE9UsXQ=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"sptau.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="扫读 ICCV 2017 论文集中感兴趣的论文，将根据 略读 - 细读 - 精读 的方式快速过一遍"><meta property="og:type" content="article"><meta property="og:title" content="ICCV 2017 扫读总结"><meta property="og:url" content="http://sptau.github.io/posts/c254b0.html"><meta property="og:site_name" content="巽离阁"><meta property="og:description" content="扫读 ICCV 2017 论文集中感兴趣的论文，将根据 略读 - 细读 - 精读 的方式快速过一遍"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-03-06T14:24:59.000Z"><meta property="article:modified_time" content="2023-11-24T14:50:07.159Z"><meta property="article:author" content="SPTAU"><meta property="article:tag" content="论文"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://sptau.github.io/posts/c254b0.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://sptau.github.io/posts/c254b0.html","path":"posts/c254b0.html","title":"ICCV 2017 扫读总结"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>ICCV 2017 扫读总结 | 巽离阁</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="巽离阁" type="application/atom+xml"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">巽离阁</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">非生来富贵 是天道酬勤</p></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E6%96%B9%E5%BC%8F"><span class="nav-text">阅读方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%95%A5%E8%AF%BB"><span class="nav-text">略读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%86%E8%AF%BB"><span class="nav-text">细读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B2%BE%E8%AF%BB"><span class="nav-text">精读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AF%87%E8%AE%BA%E6%96%87"><span class="nav-text">18 篇论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#d-graph-neural-networks-for-rgbd-semantic-segmentation1"><span class="nav-text">3D Graph Neural Networks for RGBD Semantic Segmentation1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-driven-3d-object-detection-in-rgb-d-images2"><span class="nav-text">2D-Driven 3D Object Detection in RGB-D Images2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-surface-detail-enhancement-from-a-single-normal-map3"><span class="nav-text">3D Surface Detail Enhancement from A Single Normal Map3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-prnn-generating-shape-primitives-with-recurrent-neural-networks4"><span class="nav-text">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dcnn-dqn-rnn-a-deep-reinforcement-learning-framework-for-semantic-parsing-of-large-scale-3d-point-clouds5"><span class="nav-text">3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a-read-write-memory-network-for-movie-story-understanding6"><span class="nav-text">A Read-Write Memory Network for Movie Story Understanding*6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adversarial-examples-for-semantic-segmentation-and-object-detection7"><span class="nav-text">Adversarial Examples for Semantic Segmentation and Object Detection*7</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-empirical-study-of-language-cnn-for-image-captioning8"><span class="nav-text">An Empirical Study of Language CNN for Image Captioning*8</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bb8-a-scalable-accurate-robust-to-partial-occlusion-method-for-predicting-the-3d-poses-of-challenging-objects-without-using-depth9"><span class="nav-text">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth9</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#channel-pruning-for-accelerating-very-deep-neural-networks10"><span class="nav-text">Channel Pruning for Accelerating Very Deep Neural Networks10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#colored-point-cloud-registration-revisited11"><span class="nav-text">Colored Point Cloud Registration Revisited11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deepcontext-context-encoding-neural-pathways-for-3d-holistic-scene-understanding12"><span class="nav-text">DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding12</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#directionally-convolutional-networks-for-3d-shape-segmentation13"><span class="nav-text">Directionally Convolutional Networks for 3D Shape Segmentation13</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#editable-parametric-dense-foliage-from-3d-capture14"><span class="nav-text">Editable Parametric Dense Foliage from 3D Capture14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#efficient-global-2d-3d-matching-for-camera-localization-in-a-large-scale-3d-map15"><span class="nav-text">Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#egocentric-gesture-recognition-using-recurrent-3d-convolutional-neural-networks-with-spatiotemporal-transformer-modules16"><span class="nav-text">Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules16</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-3d-geometric-features-for-rigid-object-part-segmentation17"><span class="nav-text">Embedding 3D Geometric Features for Rigid Object Part Segmentation17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#end-to-end-learning-of-geometry-and-context-for-deep-stereo-regression18"><span class="nav-text">End-to-End Learning of Geometry and Context for Deep Stereo Regression18</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="SPTAU" src="/images/avatar.png"><p class="site-author-name" itemprop="name">SPTAU</p><div class="site-description" itemprop="description">江湖行走 初心依旧 来年还乡 一梦千秋</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/SPTAU" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SPTAU" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:sptau@qq.com" title="E-Mail → mailto:sptau@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/1601578" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;1601578" rel="noopener me" target="_blank"><i class="fa-brands fa-bilibili fa-fw"></i>Bilibili</a></span></div></div></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> 链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://blinkfox.github.io/" title="https:&#x2F;&#x2F;blinkfox.github.io" rel="noopener" target="_blank">闪烁之狐 - blinkfox</a></li><li class="links-of-blogroll-item"><a href="https://www.haomwei.com/" title="https:&#x2F;&#x2F;www.haomwei.com&#x2F;" rel="noopener" target="_blank">屠·城 - 屠夫9441</a></li><li class="links-of-blogroll-item"><a href="http://yearito.cn/" title="http:&#x2F;&#x2F;yearito.cn&#x2F;" rel="noopener" target="_blank">Yearito's Blog - yearito</a></li><li class="links-of-blogroll-item"><a href="https://io-oi.me/" title="https:&#x2F;&#x2F;io-oi.me&#x2F;" rel="noopener" target="_blank">reuixiy's Blog - reuixiy</a></li><li class="links-of-blogroll-item"><a href="https://www.yuque.com/cbyd/chengbao" title="https:&#x2F;&#x2F;www.yuque.com&#x2F;cbyd&#x2F;chengbao" rel="noopener" target="_blank">城堡阅读杂志 - 南百城</a></li></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://sptau.github.io/posts/c254b0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="SPTAU"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="巽离阁"><meta itemprop="description" content="江湖行走 初心依旧 来年还乡 一梦千秋"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="ICCV 2017 扫读总结 | 巽离阁"><meta itemprop="description" content="扫读 ICCV 2017 论文集中感兴趣的论文，将根据 略读 - 细读 - 精读 的方式快速过一遍"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ICCV 2017 扫读总结</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-03-06 22:24:59" itemprop="dateCreated datePublished" datetime="2023-03-06T22:24:59+08:00">2023-03-06</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-11-24 22:50:07" itemprop="dateModified" datetime="2023-11-24T22:50:07+08:00">2023-11-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%8A%80%E6%9C%AF/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>7.5k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div><div class="post-description">扫读 ICCV 2017 论文集中感兴趣的论文，将根据 略读 - 细读 - 精读 的方式快速过一遍</div></div></header><div class="post-body" itemprop="articleBody"><p>本次扫读从 ICCV 2017 论文集 600 篇中根据论文名称寻找了感兴趣的 50 篇，将根据 略读 - 细读 - 精读 的方式将这 50 篇论文快速过一遍</p><blockquote><p>因时间关系最后只读了其中的 18 篇</p></blockquote><h2 id="阅读方式">阅读方式</h2><h3 id="略读">略读</h3><p>所有论文都先过一遍 Abstract 、Introduction 、Conclusion ，简要了解该论文的主要贡献</p><p>若该论文的应用场景、研究方向不合预期，则立刻略过</p><h3 id="细读">细读</h3><p>将全文过一遍，但不细究其中的技术，主要把流程捋顺</p><h3 id="精读">精读</h3><p>对于十分契合研究方向论文，将会仔细学习其中的算法流程和思路</p><h2 id="篇论文">18 篇论文</h2><h3 id="d-graph-neural-networks-for-rgbd-semantic-segmentation1">3D Graph Neural Networks for RGBD Semantic Segmentation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h3><p>此前利用 RGB 图像和深度图像进行语义分割的方法是分别对这两个图像输入进 CNN 神经网络，这样做的计算成本和内存成本很大</p><p>该论文提出了一种 3D 图神经网络（ 3DGNN ），在点云上构建 k 最临近图，分别输入 RGB 图像、深度图像和点云，输出 RGB 图像的语义分类结果</p><p>考虑到适用场景不同以及 GNN 的复杂性，略过该论文</p><h3 id="d-driven-3d-object-detection-in-rgb-d-images2">2D-Driven 3D Object Detection in RGB-D Images<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h3><p>在 3D 目标检测中，一般使用 3D 边界框标注目标，但是 3D 标注框的计算成本较大且难以利用到点云中的局部特性</p><p>该论文提出了利用 RGB 图像和深度图像的 2D 目标检测驱动 3D 目标检测，先进行二维图像的目标检测，然后利用得到的 2D 边界框在 3D 点云上重新配准的到 3D 边界框</p><p>考虑到适用场景不同，略过该论文</p><h3 id="d-surface-detail-enhancement-from-a-single-normal-map3">3D Surface Detail Enhancement from A Single Normal Map<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h3><p>现有的三维重建算法重建的表面质量受到输入图像质量的影响，如何利用现有质量的图像重建更优质表面是需要解决的问题</p><p>该论文提出了一种不需要训练数据集、不依赖硬件的三维重建方式，提高了表面的细腻度</p><p>该论文不是基于深度学习，略过</p><h3 id="d-prnn-generating-shape-primitives-with-recurrent-neural-networks4">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h3><p>该论文提出了一种基于 RNN 输入深度图像、能够生成三维模型的 3D-PRNN 模型和一种基于高斯场和能量最小化的点云拟合基元的有效方法，能够有效应用于三维重建</p><p>考虑到应该不会使用深度图像，略过</p><h3 id="dcnn-dqn-rnn-a-deep-reinforcement-learning-framework-for-semantic-parsing-of-large-scale-3d-point-clouds5">3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></h3><p>该论文提出了一种自动解析大规模三维点云的 3DCNN-DQN-RNN 模型，利用 3DCNN 提取点云的特征，利用 RNN 融合 3DCNN 得到的各种特征，利用 DQN 进行类滑动窗口的操作，实现定位、检测、分类的集成</p><p>略感兴趣，可以后期再看看</p><p>DQN 是未见过的一类神经网络架构，后面也可以再留意</p><h3 id="a-read-write-memory-network-for-movie-story-understanding6">A Read-Write Memory Network for Movie Story Understanding*<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></h3><p>论文提出了一种基于 ResNet 和 Word2Vec 网络和 CNN 结构的电影故事理解模型，输入画面和台词，可以很好地回答针对故事的提问</p><p>只是对这方面略感兴趣，快速略过</p><h3 id="adversarial-examples-for-semantic-segmentation-and-object-detection7">Adversarial Examples for Semantic Segmentation and Object Detection*<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></h3><p>对对抗样本感兴趣，于是看到这篇论文的时候挑出来了</p><p>为了应对输入中的微小扰动带来的论文提出了一种密集对抗生成的新算法</p><p>只是看摘要、总结和引言看不出什么，快速略过</p><h3 id="an-empirical-study-of-language-cnn-for-image-captioning8">An Empirical Study of Language CNN for Image Captioning*<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></h3><p>论文提出的网络是非典型的神经网络，使用了图像和文字作为输入</p><p>论文提出的模型首先将图片输入进 CNN 里面得到特征，然后再将特征作为 RNN 的基础状态，和文字一起进行 RNN ，最终实现 Image To Text</p><p>快速略过</p><h3 id="bb8-a-scalable-accurate-robust-to-partial-occlusion-method-for-predicting-the-3d-poses-of-challenging-objects-without-using-depth9">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></h3><p>论文提出了一种基于二维图像进行物体的三维姿势检测的方法，实现了在图像上预测物体的三维边界框</p><p>但是论文中没有提出具体的模型，感觉不是很可靠，或者说因此觉得他不希望被复现</p><p>可以暂存</p><h3 id="channel-pruning-for-accelerating-very-deep-neural-networks10">Channel Pruning for Accelerating Very Deep Neural Networks<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></h3><p>论文提出了一种修剪卷积神经网络通道的方法，通过减少卷积神经网络中的通道数加速网络计算并将因此带来的误差减少到一定程度，且该方法具有很强的拓展性，可以在 VGG 、 ResNet 上使用该方法</p><p>可以暂存</p><h3 id="colored-point-cloud-registration-revisited11">Colored Point Cloud Registration Revisited<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></h3><p>论文提出了一种针对 RGB-D 图像和 XYZRGB 点云结合的点云配准方法，效果优于基于机器学习的 ICP 配准和 FGR 配准方法</p><p>但是看起来好像不是基于深度学习，且需要用到深度图像</p><p>可以留意有没有作者公布的代码实现</p><h3 id="deepcontext-context-encoding-neural-pathways-for-3d-holistic-scene-understanding12">DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></h3><p>论文提出了一种能够学习场景信息的三维卷积神经网络，通过对齐数据集学习到的基本场景模板和卷积神经网络，实现场景中物体的实例分割</p><p>可以暂存，但是如果应用在植物上的话，可能很难总结出不同的模板</p><h3 id="directionally-convolutional-networks-for-3d-shape-segmentation13">Directionally Convolutional Networks for 3D Shape Segmentation<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></h3><p>论文提出了一种基于 Mesh 网格数据集的定向卷积神经网络 DCN 和普通神经网络结合的双流分割网络</p><p>双流指的是，同一输入分别送入不同网络中学习到不同的特征后融合在一起，本质上还是一种将多种方法结合在一起的作弊行为，虽然很好用</p><p>由于该网络是针对 Mesh 网格的，快速略过</p><h3 id="editable-parametric-dense-foliage-from-3d-capture14">Editable Parametric Dense Foliage from 3D Capture<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></h3><p>论文提出了一种基于 RGB-D 图像的分割单个叶片的算法和利用 B ́ezier patches 模拟叶片表面进行重建叶片点云的方法</p><p>虽然很切合植物点云方向，但是由于该算法并不是基于深度学习且使用 C++ 实现，快速略过</p><h3 id="efficient-global-2d-3d-matching-for-camera-localization-in-a-large-scale-3d-map15">Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></h3><p>论文提出了一种基于 3D 点云的马尔可夫网络，实现了在大型 3D 点云中进行检索特定场景。</p><p>由于本文研究场景是在大型场景的 3D 点云与当前研究方向不太符合，略过</p><h3 id="egocentric-gesture-recognition-using-recurrent-3d-convolutional-neural-networks-with-spatiotemporal-transformer-modules16">Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></h3><p>论文提出了一种基于带有 RSTTM 的 3DCNN 模型，用于实现手势识别。此处的 3D 是指二维图像加上时间维度，即时序图像。</p><p>以为本文的 3D 是指三维立体物体，略过</p><h3 id="embedding-3d-geometric-features-for-rigid-object-part-segmentation17">Embedding 3D Geometric Features for Rigid Object Part Segmentation<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></h3><p>论文提出了一种基于 FCN 框架的双流 CNN 网络模型，其中一个名为 AppNet 的流可以从输入图像中提取 2D 外观，另一个名为 GeoNet 的流能够提取 3D 集合特征，将双流输出的特征结合起来实现对图像中物体的零件分割。由于 GeoNet 流的输入为二维图像，输出为 3D 集合特征，因此需要利用名为 VolNet 的基于 2D-CNN 的网络从 3D 体素中提取 3D 集合特征，并以此训练 GeoNet</p><p>该论文主要是利用图像和体素，与研究方向不符，略过</p><h3 id="end-to-end-learning-of-geometry-and-context-for-deep-stereo-regression18">End-to-End Learning of Geometry and Context for Deep Stereo Regression<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></h3><p>论文提出了一种基于 2D-CNN 和 3D-CNN 的实现双目摄像机图像生成视差图像的卷积神经网络</p><p>该论文方向不属于三维重建，与研究方向不符，略过</p><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Xiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 2017. 3D Graph Neural Networks for RGBD Semantic Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5209–5218. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.556" class="uri">https://doi.org/10.1109/ICCV.2017.556</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn2" role="doc-endnote"><p>Jean Lahoud and Bernard Ghanem. 2017. 2D-Driven 3D Object Detection in RGB-D Images. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 4632–4640. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.495" class="uri">https://doi.org/10.1109/ICCV.2017.495</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3" role="doc-endnote"><p>Wuyuan Xie, Miaohui Wang, Xianbiao Qi, and Lei Zhang. 2017. 3D Surface Detail Enhancement from a Single Normal Map. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2344–2352. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.255" class="uri">https://doi.org/10.1109/ICCV.2017.255</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4" role="doc-endnote"><p>Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, and Derek Hoiem. 2017. 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 900–909. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.103" class="uri">https://doi.org/10.1109/ICCV.2017.103</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5" role="doc-endnote"><p>Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, and Jiwen Lu. 2017. 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5679–5688. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.605" class="uri">https://doi.org/10.1109/ICCV.2017.605</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn6" role="doc-endnote"><p>Seil Na, Sangho Lee, Jisung Kim, and Gunhee Kim. 2017. A Read-Write Memory Network for Movie Story Understanding. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 677–685. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.80" class="uri">https://doi.org/10.1109/ICCV.2017.80</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn7" role="doc-endnote"><p>Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. 2017. Adversarial Examples for Semantic Segmentation and Object Detection. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1378–1387. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.153" class="uri">https://doi.org/10.1109/ICCV.2017.153</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn8" role="doc-endnote"><p>Jiuxiang Gu, Gang Wang, Jianfei Cai, and Tsuhan Chen. 2017. An Empirical Study of Language CNN for Image Captioning. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1231–1240. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.138" class="uri">https://doi.org/10.1109/ICCV.2017.138</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn9" role="doc-endnote"><p>Mahdi Rad and Vincent Lepetit. 2017. BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 3848–3856. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.413" class="uri">https://doi.org/10.1109/ICCV.2017.413</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn10" role="doc-endnote"><p>Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel Pruning for Accelerating Very Deep Neural Networks. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1398–1406. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.155" class="uri">https://doi.org/10.1109/ICCV.2017.155</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn11" role="doc-endnote"><p>Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Colored Point Cloud Registration Revisited. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 143–152. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.25" class="uri">https://doi.org/10.1109/ICCV.2017.25</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn12" role="doc-endnote"><p>Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, and Jianxiong Xiao. 2017. DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 1201–1210. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.135" class="uri">https://doi.org/10.1109/ICCV.2017.135</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn13" role="doc-endnote"><p>Haotian Xu, Ming Dong, and Zichun Zhong. 2017. Directionally Convolutional Networks for 3D Shape Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2717–2726. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.294" class="uri">https://doi.org/10.1109/ICCV.2017.294</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn14" role="doc-endnote"><p>Paul Beardsley and Gaurav Chaurasia. 2017. Editable Parametric Dense Foliage from 3D Capture. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 5315–5324. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.567" class="uri">https://doi.org/10.1109/ICCV.2017.567</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn15" role="doc-endnote"><p>Liu Liu, Hongdong Li, and Yuchao Dai. 2017. Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 2391–2400. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.260" class="uri">https://doi.org/10.1109/ICCV.2017.260</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn16" role="doc-endnote"><p>Congqi Cao, Yifan Zhang, Yi Wu, Hanqing Lu, and Jian Cheng. 2017. Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks with Spatiotemporal Transformer Modules. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 3783–3791. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.406" class="uri">https://doi.org/10.1109/ICCV.2017.406</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn17" role="doc-endnote"><p>Yafei Song, Xiaowu Chen, Jia Li, and Qinping Zhao. 2017. Embedding 3D Geometric Features for Rigid Object Part Segmentation. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 580–588. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.70" class="uri">https://doi.org/10.1109/ICCV.2017.70</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn18" role="doc-endnote"><p>Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. 2017. End-to-End Learning of Geometry and Context for Deep Stereo Regression. In 2017 IEEE International Conference on Computer Vision (ICCV), IEEE, Venice, 66–75. DOI:<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICCV.2017.17" class="uri">https://doi.org/10.1109/ICCV.2017.17</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section></div><footer class="post-footer"><script src="//sdk.jinrishici.com/v2/browser/jinrishici.js"></script><script>jinrishici.load((result) => {
    let jrsc = document.getElementById('jrsc');
    const data = result.data;
    let author = data.origin.author;
    let title = '《' + data.origin.title + '》';
    let content = data.content.substr(0, data.content.length - 1);
    let dynasty = data.origin.dynasty.substr(0, data.origin.dynasty.length - 1);
    jrsc.innerText = content + ' @ ' + dynasty + '·' + author + title;
  });</script><div style="text-align:center"><span id="jrsc">正在加载今日诗词....</span></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>SPTAU</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://sptau.github.io/posts/c254b0.html" title="ICCV 2017 扫读总结">http://sptau.github.io/posts/c254b0.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"><i class="fa fa-tag"></i> 论文</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/5f7835f6.html" rel="prev" title="基于机器学习的相邻时序植物点云配准总结"><i class="fa fa-angle-left"></i> 基于机器学习的相邻时序植物点云配准总结</a></div><div class="post-nav-item"><a href="/posts/781c276d.html" rel="next" title="点云补全论文阅读总结">点云补全论文阅读总结 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments gitalk-container"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2022 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">SPTAU</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span title="站点总字数">86k</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><a role="button" class="book-mark-link book-mark-link-fixed"></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.24/fancybox/fancybox.umd.js" integrity="sha256-oyhjPiYRWGXaAt+ny/mTMWOnN1GBoZDUQnzzgC7FRI4=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.5.0/mermaid.min.js","integrity":"sha256-K7oJiQlDulzl24ZUFOywuYme1JqBBvQzK6m8qHjt9Gk="}}</script><script src="/js/third-party/tags/mermaid.js"></script><script src="/js/third-party/fancybox.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script><script>var options = {
  bottom: '32px',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous"><script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"SPTAU","repo":"SPTAU.github.io","client_id":"88f04708a8ad279abe57","client_secret":"f9f4099eb59d8ab8522197e56437b52ad9048c0e","admin_user":"SPTAU","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"6b7b49746d4be77d9c8d67fa1501619d"}</script><script src="/js/third-party/comments/gitalk.js"></script><script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script><script>const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);
        $('a').each(function() {
          const $this = $(this);
          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;
          const href = $this.attr('href');
          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {
            const strs = href.split('/');
            if (strs.length >= 3) {
                const host = strs[2];
                if (host !== 'SPTAU.github.io' || window.location.host) {
                    $this.attr('href', '/go.html?u='+href+'').attr('rel', 'external nofollow noopener noreferrer');
                    if (true) {
                        $this.attr('target', '_blank');
                    }
                }
            }
          }
        });</script></body></html>